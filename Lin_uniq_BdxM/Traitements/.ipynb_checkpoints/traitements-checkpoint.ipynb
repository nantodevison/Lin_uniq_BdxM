{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LINEARISATION UNIQUE DES TRAFICS SUR BORDEAUX METROPOLE**\n",
    "> Traitements à mettre en oeuvre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys #c'est pas propre mais pour le moment pour importer mes modules perso dans le notebook je ne sais pas faire\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Lin_uniq_BdxM\\Lin_uniq_BdxM\\Traitements')\n",
    "import Connexion_Transfert as ct\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint, Polygon, box\n",
    "import re\n",
    "from difflib import get_close_matches,SequenceMatcher\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from Outils import creer_graph, plus_proche_voisin,nb_noeud_unique_troncon_continu\n",
    "\n",
    "from Appariement import (corresp_noeud_mmm,corresp_noeud_rhv,appariement_noeud_mmm_fv,estim_mmm_jointure_voies,isoler_trafic_inconnu,\n",
    "                         trafic_mmm,calcul_trafic_rhv_depuisMMM,PasCorrespondanceError)\n",
    "from Repartition_trafic import (noeud_fv_ligne_ss_trafic,df_noeud_troncon,verif_double_sens,verif_carrefour_2_chaussees,\n",
    "                                separer_troncon_a_estimer,calcul_trafic_manquant_3troncons,PasDeTraficError,maj_carrefour_3_troncons,trouver_noeud_3tronc_1NaN)\n",
    "from Simplifier_Rdpt import creer_dico_noeud_rdpt,simplifier_noeud_rdpt,maj_graph_rdpt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "from Base_BdTopo.Import_outils import *\n",
    "from Base_BdTopo.Rond_points import *\n",
    "from Base_BdTopo.Regroupement_correspondance import *\n",
    "from Base_BdTopo.Troncon_elementaire import *\n",
    "from Base_BdTopo.Troncon_base import *\n",
    "from Base_BdTopo.Gestion_2_chaussee import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Regrouper les troncons du filaire de voie\n",
    ">Les troncons du filaire de voie sont regroupés d'abord uniquement avec les troncons de cat_rhv égal à 1,2,3,61,62,63, pour pouvoir réaliser des troncon de trafic homogènes tous reliés entre eux.\n",
    "<br> Ensuite, un deuxième regroupement comprenant l'ensemble des troncons de cat_rhv permetd'affiner le diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1.1 Import du fichier final\n",
    "le fichier shape est vérifié à la main car l'affectation automatique n'est pas 100% fiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour info : lire le graph issu de la methode decrite dessous\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    graph_filaire_123 = gp.read_postgis('select * from linearisation_bm.graph_rhv_123', c.connexionPsy)\n",
    "    graph_filaire_123_vertex = gp.read_postgis('select id,cnt,chk,ein,eout,the_geom as geom from linearisation_bm.graph_rhv_123_vertices_pgr', c.connexionPsy)\n",
    "    graph_filaire = gp.read_postgis('select * from linearisation_bm.graph_rhv_complet', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_groupe_123=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_groupe_123_v2.shp').merge(\n",
    "    graph_filaire_123[['ident','source','target']], on='ident')\n",
    "gdf_rhv_groupe=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\rhv_grp.shp').merge(\n",
    "    graph_filaire[['ident','source','target']], on='ident')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 1.2 Méthode\n",
    " pour information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import du fichier de filaire de base\n",
    "gdf_rhv=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\Filaire_voie\\FV_TRONC_L.shp')\n",
    "# 2. mise en forme\n",
    "gdf_rhv.columns=[a.lower() for a in gdf_rhv.columns] #nom de colonne en minuscule\n",
    "gdf_rhv.rename(columns={'gid':'id','nom_voie' : 'numero'},inplace=True)\n",
    "gdf_rhv=gdf_rhv.loc[~gdf_rhv.id.isna()].copy()\n",
    "gdf_rhv['id_ign']=gdf_rhv.ident.apply(lambda x : 'TRONROUT'+str(x))\n",
    "gdf_rhv=gdf_rhv.loc[~gdf_rhv.cat_rhv.isin(['98','99'])].copy()\n",
    "gdf_rhv['nature']=gdf_rhv.apply(lambda x : 'Route à 2 chaussées' if x['rgraph_dbl']==0 else 'Route à 1 chaussée', axis=1)\n",
    "gdf_rhv['nature']=gdf_rhv.apply(lambda x : 'Bretelle' if x['cat_dig']=='8' else x['nature'], axis=1)\n",
    "gdf_rhv['numero']=gdf_rhv.apply(lambda x : 'Bretelle '+x['numero'] if x['cat_dig']=='8' else x['numero'], axis=1)\n",
    "gdf_rhv['sens']=gdf_rhv.apply(lambda x : 'Direct' if x['rgraph_dbl']==0 else 'Double', axis=1)\n",
    "gdf_rhv['codevoie_d']='NR'\n",
    "gdf_rhv['importance']=gdf_rhv['cat_dig']\n",
    "gdf_rhv['id']=gdf_rhv.id.apply(lambda x : int(x))\n",
    "#gdf_rhv.loc[gdf_rhv.nom_voie.isna()] #verif des nom_voie null : 2 lignes, cf traitements plus loins\n",
    "#filtrer les voies cylcables et autres\n",
    "gdf_rhv_filtre=gdf_rhv.loc[~gdf_rhv.cat_dig.isin(['6','7','9','10'])].copy()\n",
    "\n",
    "# 3. creer le graph en bdd\n",
    "#creer_graph(gdf_rhv_filtre.loc[gdf_rhv_filtre.cat_rhv.isin(['1','2','3','33',61','62','63'])],\n",
    "            #'local_otv',schema='linearisation_bm',table='graph_rhv_123',table_vertex='graph_rhv_123_vertices_pgr')\n",
    "creer_graph(gdf_rhv_filtre,'local_otv',schema='linearisation_bm',table='graph_rhv_complet',table_vertex='graph_rhv_complet_vertices_pgr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "# 4. initialiser les paramètre de regroupement\n",
    "df=import_donnes_base('local_otv','linearisation_bm', 'graph_rhv_complet','graph_rhv_complet_vertices_pgr')\n",
    "df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=identifier_rd_pt(df)\n",
    "df_lignes=df_avec_rd_pt.set_index('id_ign')#mettre l'id_ign en index\n",
    "bretelle=df_avec_rd_pt.loc[df_avec_rd_pt['nature']=='Bretelle'].copy()\n",
    "bretelle_tri=bretelle.loc[bretelle.length.sort_values(ascending=False).index.tolist()].id_ign.tolist()\n",
    "sans_bretelle=df_avec_rd_pt.loc[df_avec_rd_pt['nature']!='Bretelle'].copy()\n",
    "list_sans_bretelle=sans_bretelle.id_ign.tolist()\n",
    "list_tri_longueur=sans_bretelle.loc[sans_bretelle.length.sort_values(ascending=False).index.tolist()].id_ign.tolist()\n",
    "list_id_ign=bretelle_tri+list_tri_longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. appel fonction de grouepement pour creer des idtronc pour uniquement voies de 1 à 3 et d'un autre coté pour toute les voies\n",
    "df_affectation, dico_erreur, lignes_traitees, lignes_non_traitees=regrouper_troncon(\n",
    "    list_id_ign, df_avec_rd_pt, carac_rd_pt,df2_chaussees,[])\n",
    "\n",
    "# 6. mise en forme et export\n",
    "gdf_rhv_groupe=gdf_rhv_filtre.merge(df_affectation, left_on='id_ign', right_on='id', how='left')\n",
    "gdf_rhv_groupe.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\rhv_grp_v2.shp')\n",
    "#gdf_rhv_groupe_123=gdf_rhv_filtre.merge(df_affectation, left_on='id_ign', right_on='id', how='left')\n",
    "#gdf_rhv_groupe_123.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_groupe_123.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2019-12-12 11:48:54.415530 nb lignes traitees : 0\n",
      "pas de parrallele trouvee pour les troncons ['TRONROUT9919']\n"
     ]
    }
   ],
   "source": [
    "#pour test sur une ligne\n",
    "df_affectation2, dico_erreur2, lignes_traitees2, lignes_non_traitees2=regrouper_troncon(\n",
    "    ['TRONROUT9919'], df_avec_rd_pt, carac_rd_pt,df2_chaussees,[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MISE EN FORME DES COMPTAGES PONCTUELS\n",
    "Pour ces comptages l'idée c'est de réussir à regrouper les comptages par un idtronc issu de la detection des troncon elementaire. ça a du sens car pour un comptage ponctuel normalement les 2 sens de cicru sont sur un troncon unique. DE plus, les ens circu, noms de voie et autres attributs permettant de regrouper les comptages sont de qualité trop variable pour s'appuyer uniquement dessus. et enfin, peu de 2*2 vois sont meusrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des données\n",
    "cpt_pct=pd.read_excel(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\base_comptages_routiers_v4.xlsx')\n",
    "gdf_rhv_groupe['geom_src']=gdf_rhv_groupe.geometry\n",
    "cpt_pct.columns=[a.lower() for a in cpt_pct.columns]\n",
    "cpt_pct = gp.GeoDataFrame(cpt_pct, geometry=gp.points_from_xy(cpt_pct.longitude, cpt_pct.latitude))\n",
    "cpt_pct.crs = {'init' :'epsg:4326'}\n",
    "cpt_pct_l93=cpt_pct.to_crs({'init': 'epsg:2154'})\n",
    "cpt_pct_l93['x_l93']=cpt_pct_l93.geometry.apply(lambda x : x.x)\n",
    "cpt_pct_l93['y_l93']=cpt_pct_l93.geometry.apply(lambda x : x.y)\n",
    "\n",
    "#mise en forme\n",
    "cpt_pct_l93['nom_voie']=cpt_pct_l93.nom_voie.apply(lambda x : re.sub(('é|è|ê'),'e',x.lower().strip()))\n",
    "cpt_pct_l93['sens_unique']=cpt_pct_l93.sens_circulation.apply(lambda x : True if SequenceMatcher(None,' '.join(x.split(' ')[:2]).lower(),'sens unique').ratio()>0.8 else False)\n",
    "cpt_pct_l93['type_voie']=cpt_pct_l93.nom_voie.apply(lambda x : x.split(' ')[0])\n",
    "cpt_pct_l93['suffix_nom_voie']=cpt_pct_l93.nom_voie.apply(lambda x : ' '.join(x.split(' ')[1:]).lower())\n",
    "cpt_pct_l93['date_max_cptg']=cpt_pct_l93.observation.apply(lambda x : pd.to_datetime(x.split(' au ')[1],dayfirst=True))\n",
    "cpt_pct_l93['sens_circulation']=cpt_pct_l93.sens_circulation.apply(lambda x : re.sub(('é|è|ê'),'e',x.strip().lower()))\n",
    "cpt_pct_l93['observation']=cpt_pct_l93.observation.apply(lambda x : x.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster à 200m\n",
    "data_test_clust=[[x, y] for x, y in zip(cpt_pct_l93.x_l93.tolist(), cpt_pct_l93.y_l93.tolist())]\n",
    "db = DBSCAN(eps=200, min_samples=2).fit(data_test_clust)\n",
    "labels = db.labels_\n",
    "cpt_pct_l93['n_cluster']=labels\n",
    "\n",
    "#voies avec nom_proches\n",
    "cross_join_ncluster=cpt_pct_l93[['ident','nom_voie','type_voie','suffix_nom_voie', 'sens_circulation','annee','n_cluster']].merge(\n",
    "   cpt_pct_l93[['ident','nom_voie','type_voie','suffix_nom_voie', 'sens_circulation','annee','n_cluster']], on='n_cluster') #avoir toutes les relations internoms possibles\n",
    "cross_join_ncluster['comp_nom_voie']=cross_join_ncluster.apply(lambda x : SequenceMatcher(None,x['suffix_nom_voie_x'], x['suffix_nom_voie_y']).ratio(), axis=1)#affecter une note a cahque relation\n",
    "voie_nom_proches=cross_join_ncluster.loc[(cross_join_ncluster['comp_nom_voie']>0.85) & (cross_join_ncluster['type_voie_x']==cross_join_ncluster['type_voie_y'])\n",
    "                        ].sort_values(['n_cluster','ident_x'])#ne conserver qque les relations bien notees\n",
    "voie_nom_proches['id_grp_nom_voie']=voie_nom_proches.ident_x.rank(method='dense')#ajouter un id \n",
    "corresp_voies=voie_nom_proches.drop_duplicates('ident_y')\n",
    "grp_nom_voie=cpt_pct_l93.merge(corresp_voies[['ident_y','id_grp_nom_voie']].rename(columns={'ident_y':'ident'}), on='ident')#jointure sur id de depart\n",
    "\n",
    "#grouper par periode\n",
    "grp_period=grp_nom_voie.copy()\n",
    "grp_period['date_max_cptg']=grp_period.observation.apply(lambda x : pd.to_datetime(x.split(' au ')[1],dayfirst=True))\n",
    "grp_period['id_period']=grp_period.date_max_cptg.rank(method='dense')\n",
    "grp_period['indicefinal']=grp_period.n_cluster+(grp_period.id_grp_nom_voie*(1000))+(grp_period.id_period*10000)\n",
    "grp_period[[a for a in grp_period.columns if a!='date_max_cptg']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cluster_comptg_pctl_grp_period.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#regarder le nb de ligne relative à chaque comptage : siune ligne est relative à 2 comptage qui ont le mm indice final\n",
    "#i.e relatif à la mme rue, à la mm periode, sur des sens différents, on fait la somme\n",
    "#si il y a plus de 2 comptages : il faut récupérer les plus recents : si 2 on fait la somme, si 1 on prends la valeur\n",
    "\n",
    "#rappatrier le numero d'id_tronc pour chaque comptage\n",
    "#trouver la distance min à chaque objet ligne\"du rhv groupe\n",
    "grp_troncon_temp=cpt_pct_l93.copy()\n",
    "grp_troncon_temp.geometry=grp_troncon_temp.buffer(20)#passer la geom en buffer\n",
    "intersct_buff_20=gp.sjoin(grp_troncon_temp,gdf_rhv_groupe,how='left',op='intersects')\n",
    "intersct_buff_20.geometry=grp_period.geometry#repasser la geom en point\n",
    "intersct_buff_20=intersct_buff_20.merge(gdf_rhv_groupe[['ident','geometry']], left_on='ident_right', right_on='ident')\n",
    "intersct_buff_20['dist_pt_ligne']=intersct_buff_20.apply(lambda x : x['geometry_x'].distance(x['geometry_y']), axis=1) #définir la disance entre les points et ligne\n",
    "joint_dist_min=intersct_buff_20.loc[intersct_buff_20.groupby('ident_left')['dist_pt_ligne'].transform(min)==intersct_buff_20['dist_pt_ligne']][['ident_left','idtronc','numero']].rename(\n",
    "    columns={'ident_left':'ident'}).copy()#ne garder que la ligne la plus proche\n",
    "grp_troncon=grp_period.merge(joint_dist_min, on='ident',how='left')#df finale\n",
    "grp_troncon[[a for a in grp_troncon.columns if a!='date_max_cptg']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cluster_comptg_pctl_grp_troncon.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annee la plus recente\n",
    "anne_recente=grp_troncon.loc[grp_troncon.groupby('idtronc')['date_max_cptg'].transform(max)==grp_troncon['date_max_cptg']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idtronc ok (avec 1 ou 2 pt de comptage)\n",
    "idtronc_grp=anne_recente.groupby('idtronc').ident.count()\n",
    "idtroncOkTmjo=anne_recente.loc[anne_recente.idtronc.isin(idtronc_grp.loc[idtronc_grp<3].index.tolist())][['ident','idtronc','sens_circulation','tmjo_tv']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isoler les idtronc supportant + de 2 pt de comptages\n",
    "idtronc_sup2=idtronc_grp.loc[idtronc_grp>2].copy()\n",
    "#trouver les points correspondants\n",
    "pt_sup2=anne_recente.loc[anne_recente['idtronc'].isin(idtronc_sup2.index.tolist())].copy()\n",
    "#ajouter attribut qui traduit le nb de valeurs différentes de sens circulation\n",
    "def nb_sens_circu(idtronc) :\n",
    "    return len(pt_sup2.loc[pt_sup2['idtronc']==idtronc].sens_circulation.unique())  \n",
    "pt_sup2['nb_sens_circu']=pt_sup2.apply(lambda x : nb_sens_circu(x['idtronc']), axis=1)\n",
    "#pour les points a 2 sens de circu : on prend la valeur max des 2 : \n",
    "ptSup2SensCircu2=pt_sup2.loc[pt_sup2['nb_sens_circu']==2].copy()\n",
    "ptSup2SensCircu2.drop_duplicates(['nom_voie','sens_circulation','tmjo_tv','observation'],inplace=True)#qq points ont des ident différents mais sont les mêmes\n",
    "ptSup2SensCircu2.drop_duplicates(['sens_circulation','tmjo_tv','observation'],inplace=True)#qla mm que la précédente, mais je ne sais pas pourquoi l'ajout de nom_voie fait bugger le drop duplicates pour les ident  716,717,975,976\n",
    "ptSup2SensCircu2OkTmjo=ptSup2SensCircu2.groupby(['idtronc','sens_circulation'])['tmjo_tv'].max().reset_index().merge(\n",
    "    ptSup2SensCircu2[['idtronc','sens_circulation','tmjo_tv','ident']], on=['idtronc','sens_circulation','tmjo_tv'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour les autres point\n",
    "ptSup2SensCircuSup2=pt_sup2.loc[pt_sup2['nb_sens_circu']>2][['ident','idtronc','sens_circulation','tmjo_tv','indicefinal']].sort_values(['idtronc','indicefinal','sens_circulation']).copy()\n",
    "\n",
    "#filtre des points dont le sens circul est le mm ou qui sont isole\n",
    "list_ident, list_idtronc, list_indicefinal, list_senscircu=(ptSup2SensCircuSup2.ident.tolist(),ptSup2SensCircuSup2.idtronc.tolist(),\n",
    "                                                            ptSup2SensCircuSup2.indicefinal.tolist(),ptSup2SensCircuSup2.sens_circulation.tolist())\n",
    "list_new_ident=list_ident\n",
    "for i in range(len(list_ident)) : \n",
    "    if i<len(list_ident)-1 :\n",
    "        if list_idtronc[i+1]==list_idtronc[i] : \n",
    "            if list_indicefinal[i+1]==list_indicefinal[i] :\n",
    "                if SequenceMatcher(None,list_senscircu[i+1], list_senscircu[i]).ratio()>0.75 : \n",
    "                    list_new_ident[i+1]=list_ident[i]\n",
    "    else : \n",
    "        if list_idtronc[i]==list_idtronc[i-1] : \n",
    "            if list_indicefinal[i]==list_indicefinal[i-1] :\n",
    "                if SequenceMatcher(None,list_senscircu[i], list_senscircu[i-1]).ratio()>0.75 : \n",
    "                    list_new_ident[i]=list_ident[i-1]\n",
    "ptSup2SensCircuSup2['ident_final']=np.array(list_ident)\n",
    "#filtrer les points isoles\n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2.groupby(['idtronc','indicefinal']).nunique()[['ident']].reset_index()\n",
    "ptSup2SensCircuSup2=ptSup2SensCircuSup2.loc[ptSup2SensCircuSup2.indicefinal.isin(ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre['ident']>1].indicefinal.\n",
    "                                                                             tolist())].copy()\n",
    "#filtrer les points qui ont des nom des sens circu proches\n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2.loc[ptSup2SensCircuSup2['tmjo_tv']==\n",
    "                                                          ptSup2SensCircuSup2.groupby('ident_final')['tmjo_tv'].transform(max)].copy()\n",
    "#filtrer les points qui sont égaux \n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre['ident']==ptSup2SensCircuSup2_filtre['ident_final']].copy()\n",
    "#filtrer les points qui présente toujours plus de 2 identifiant (i.e pb denomination ou pb référentiel ou pb tronc_elementaire)\n",
    "grp=ptSup2SensCircuSup2_filtre.groupby('idtronc').nunique()[['ident']].reset_index()\n",
    "pt_non_affectes=grp.loc[grp['ident']>2]\n",
    "ptSup2SensCircuSup2Oktmjo=ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre.idtronc.isin(grp.loc[grp['ident']<3].idtronc.tolist())][['ident','idtronc','sens_circulation','tmjo_tv']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_finale=pd.concat([ptSup2SensCircuSup2Oktmjo,idtroncOkTmjo,ptSup2SensCircu2OkTmjo],axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qq modifs manuelle pour grouper par paire les comptages ponctuels: \n",
    "affect_finale.loc[affect_finale['ident']==606,'idtronc']=affect_finale.loc[affect_finale['ident']==605].idtronc.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MISE EN FORME DES COMPTAGES PERMANENTS\n",
    "Pour ces compteurs on va regrouper en séparant les siredo des boucles liées au système gertrude. Pour les siredo pas de pb, pour les autre sc'est plus compliqué car les références de localiusation varient parfois. Il y a donc une phase de regroupement manuel à la fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Import du fichier final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fichier corrige_final\n",
    "cpt_perm_final=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cpt_perm_groupe.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Méthode\n",
    "Pour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer un gdf avec la fusion des points et des données de comptages non nulles\n",
    "cpt_brut=pd.read_excel(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\comptage_trafic_2017.xlsx')\n",
    "cpt_brut.columns=[a.lower() for a in cpt_brut.columns]\n",
    "cpt_brut=cpt_brut.loc[~cpt_brut.mjo_val.isna()].copy()\n",
    "pt_cpt_perm=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\PC_CAPTE_P.shp')\n",
    "pt_cpt_perm.columns=[a.lower() for a in pt_cpt_perm.columns]\n",
    "pt_cpt_perm=pt_cpt_perm.merge(cpt_brut, on='ident')\n",
    "pt_cpt_perm['nom_voie']=pt_cpt_perm.nom_voie.apply(lambda x : re.sub(('é|è|ê'),'e',x.lower().replace('  ',' ')))#mise en forme nom de voie\n",
    "\n",
    "#regrouper les comptages\n",
    "#un compteur peut caracteriser 1 ligne, ou 2 compteur 1 ligne ou 2 compteurs 2 lignes, dc les compteurs ne vont pas toujours par deux\n",
    "\n",
    "#regrouper geographiquement\n",
    "pt_cpt_perm['x_l93']=pt_cpt_perm.geometry.apply(lambda x : x.x)\n",
    "pt_cpt_perm['y_l93']=pt_cpt_perm.geometry.apply(lambda x : x.y)\n",
    "data_test_clust=[[x, y] for x, y in zip(pt_cpt_perm.x_l93.tolist(), pt_cpt_perm.y_l93.tolist())]\n",
    "db = DBSCAN(eps=200, min_samples=2).fit(data_test_clust)\n",
    "labels = db.labels_\n",
    "pt_cpt_perm['n_cluster']=labels\n",
    "\n",
    "#correspondance nom_rue\n",
    "#analyser les noms de commune : \n",
    "#grp_nom_voie.nom_voie.apply(lambda x : x.split()[0]).unique()#liste des permiers mots\n",
    "#grp_nom_voie.loc[grp_nom_voie.nom_voie.apply(lambda x : x.split()[0]=='carbon')].nom_voie.unique() #test des valeusr\n",
    "dico_commune={'le bouscat','le haillan','bordeaux','pessac','talence', 'merignac','st medard','carbon blanc',\n",
    "       'begles', 'eysines','villenave d\\'ornon','bruges','gradignan','bassens','cenon','floirac','pessa','lormont', 'artigues'}\n",
    "dico_type_voie={'allee' : ['all.','allee', 'allees'],'avenue': ['av', 'av.', 'avenue'], 'boulevard' : ['blvd'],\n",
    "               'bretelle' : 'bretelle', 'cours' : ['crs','cours'], 'cote' : ['côte'], 'place':['pl'], 'route':['route', 'rte'],'rue':['rue'],\n",
    "               'voie':['voie']}\n",
    "def decoupe_nom_voie(nom_voie) : \n",
    "    for c in dico_commune : \n",
    "        if nom_voie[:len(c)]==c :\n",
    "            commune=nom_voie[:len(c)]\n",
    "            nom_voie=nom_voie[len(c):].strip()\n",
    "    for k, v in dico_type_voie.items() : \n",
    "        if any([a for a in nom_voie.split() if a in v])  : \n",
    "            caractere=[a for a in nom_voie.split() if a in v][0]\n",
    "            type_voie=k\n",
    "            nom_voie=' '.join([n for n in nom_voie.split() if n!=caractere])\n",
    "            break\n",
    "    else : type_voie=None\n",
    "    for test in ['avant','apres'] :\n",
    "        if test in nom_voie : \n",
    "            return nom_voie.split(test)[0].strip(), test,nom_voie.split(test)[1].strip(), commune, type_voie\n",
    "    else : return nom_voie, nom_voie,nom_voie,commune,type_voie\n",
    "\n",
    "pt_cpt_perm['nom_rue']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[0])\n",
    "pt_cpt_perm['localisant']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[1])\n",
    "pt_cpt_perm['rue_refer']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[2])\n",
    "pt_cpt_perm['commune']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[3])\n",
    "pt_cpt_perm['type_voie']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[4])\n",
    "pt_cpt_perm.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\pt_cpt_perm.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>#### 2.2.1 SIREDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siredo=pt_cpt_perm.loc[pt_cpt_perm['type']=='SIREDO'].copy()#isoler siredo\n",
    "cross_join=siredo.assign(key=1).merge(siredo.assign(key=1), on='key').drop('key',axis=1)#calcul des distances\n",
    "cross_join['distance']=cross_join.apply(lambda x : x['geometry_x'].distance(x['geometry_y']),axis=1)\n",
    "cross_join_filtre=cross_join[[c for c in cross_join.columns if c[-2:]=='_x']+['distance','ident_y']].rename(columns=\n",
    "              {c : c[:-2] for c in cross_join.columns if c[-2:]=='_x'}).copy()#filtre des attribut\n",
    "cross_join_filtre=cross_join_filtre.loc[(cross_join_filtre['ident']!=cross_join_filtre['ident_y'])].copy()\n",
    "siredo_proches=cross_join_filtre.loc[(cross_join_filtre.groupby('ident')['distance'].transform(min)==\n",
    "                                      cross_join_filtre['distance'])].copy()#plus proche voisin\n",
    "siredo_proches['id_grpsiredo']=siredo_proches.reset_index().index\n",
    "id_siredo_proches=(pd.concat([siredo_proches[['ident','ident_y', 'id_grpsiredo']],\n",
    "           siredo_proches[['ident_y','ident', 'id_grpsiredo']].rename(columns={\n",
    "    'ident_y':'ident', 'ident':'ident_y'})],sort=False).sort_values('ident').\n",
    "    reset_index().drop_duplicates('index').drop_duplicates(['ident','ident_y']).drop('index',axis=1))\n",
    "cle_assoc_siredo=pd.concat([id_siredo_proches[['ident','id_grpsiredo']],id_siredo_proches[['ident_y','id_grpsiredo']].rename(columns={\n",
    "    'ident_y':'ident'})]).sort_values('id_grpsiredo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>#### 2.2.2 Gertrude\n",
    "> au final ce n'est pas très efficace, et un renseignement des regroupement a la ain aurait été plus rapide. il sera important si de nouvelles stations sont posées de fournir de suite un identifiant de regroupement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gertrude=pt_cpt_perm.loc[pt_cpt_perm['type']=='BOUCLE'].copy()#isoler les boucles\n",
    "gertrude['x']=gertrude.geometry.apply(lambda x : x.x)\n",
    "gertrude['y']=gertrude.geometry.apply(lambda x : x.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si c'est sur le mm idtronc on regroupe\n",
    "\n",
    "#rappatrier le numero d'id_tronc pour chaque comptage\n",
    "#trouver la distance min à chaque objet ligne\"du rhv groupe\n",
    "grp_troncon_temp=gertrude.copy()\n",
    "grp_troncon_temp.geometry=grp_troncon_temp.buffer(20)#passer la geom en buffer\n",
    "intersct_buff_20=gp.sjoin(grp_troncon_temp,gdf_rhv_groupe,how='left',op='intersects')\n",
    "intersct_buff_20.geometry=gertrude.geometry#repasser la geom en point\n",
    "intersct_buff_20=intersct_buff_20.merge(gdf_rhv_groupe[['ident','geometry']], left_on='ident_right', right_on='ident')\n",
    "intersct_buff_20['dist_pt_ligne']=intersct_buff_20.apply(lambda x : x['geometry_x'].distance(x['geometry_y']), axis=1) #définir la disance entre les points et ligne\n",
    "joint_dist_min=intersct_buff_20.loc[intersct_buff_20.groupby('ident_left')['dist_pt_ligne'].transform(min)==intersct_buff_20['dist_pt_ligne']][['ident_left','idtronc','numero']].rename(\n",
    "    columns={'ident_left':'ident'}).copy()#ne garder que la ligne la plus proche\n",
    "grp_troncon=gertrude.merge(joint_dist_min, on='ident',how='left')#df finale\n",
    "\n",
    "#regrouper\n",
    "grp_idtronc=grp_troncon.groupby('idtronc')['ident'].agg(lambda x : tuple(x))\n",
    "grp_idtronc=grp_idtronc.loc[grp_idtronc.apply(lambda x : len(x)>1)].reset_index().copy()\n",
    "grp_idtronc['id_grp']=grp_idtronc.ident.rank()\n",
    "#mettre en forme\n",
    "grp_idtronc=pd.DataFrame([(a,g) for i,g in zip(grp_idtronc.ident.tolist(),grp_idtronc.id_grp.tolist()) for a in i], columns=['ident','id_grp'])\n",
    "grp_final=grp_idtronc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trouver les points qui restent et préparer un regrouepement par commune, nom de rue, distance et se,s\n",
    "\n",
    "#points qui restent\n",
    "gertrude_etape2=gertrude.loc[~gertrude['ident'].isin(grp_idtronc.ident.tolist())].copy()\n",
    "#comparer les noms\n",
    "cross_join=gertrude_etape2[['gid','geometry','ident','nom_voie', 'sens_cir','commune', 'nom_rue', 'localisant','rue_refer','type_voie']].assign(key=1).merge(\n",
    "    gertrude_etape2[['gid','geometry','ident','nom_voie', 'sens_cir','commune', 'nom_rue', 'localisant','rue_refer','type_voie']].assign(key=1), on='key').drop('key',axis=1)#calcul des distances\n",
    "cross_join['distance']=cross_join.apply(lambda x : x['geometry_x'].distance(x['geometry_y']),axis=1)\n",
    "cross_join['comp_nom_rue']=cross_join.apply(lambda x : SequenceMatcher(None,x['nom_rue_x'], x['nom_rue_y']).ratio(), axis=1)#affecter une note a cahque relation\n",
    "\n",
    "#vérifier que les sens de cicru sont cohérents et ne garder que les lignes avec un sens ok, un nom de voie proche et une distance proche\n",
    "gertrude_join=cross_join.loc[(cross_join['comp_nom_rue']>0.6) & (cross_join['distance']<1000) & (cross_join['commune_x']==cross_join['commune_y']) & \n",
    "                             (cross_join['type_voie_x']==cross_join['type_voie_y'])].copy()\n",
    "#on ajoute un attribut de verif du sens\n",
    "def verif_sens(s1,s2) : \n",
    "    for sens in [['Sens Sortant', 'Sens Entrant'],['Sens N > S', 'Sens S > N'], ['Sens Nord > Sud', 'Sens Sud > Nord'],['Nord > Sud', 'Sud > Nord'],\n",
    "                ['Boulevards Nord > Sud', 'Boulevards Sud > Nord']] :\n",
    "        if s1 in sens and s2 in sens : \n",
    "            if s1!=s2 : \n",
    "                return True\n",
    "        else : \n",
    "            continue\n",
    "    return False\n",
    "gertrude_join['valid_sens']=gertrude_join.apply(lambda x : verif_sens(x['sens_cir_x'],x['sens_cir_y']),axis=1)  \n",
    "gertrude_join['id_grp']=gertrude_join.sort_values('ident_x').ident_x.rank(method=\"min\")\n",
    "gertrude_grp=gertrude_join.loc[gertrude_join['valid_sens']][['ident_x','ident_y','distance', 'comp_nom_rue', 'valid_sens','id_grp']].sort_values('ident_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenir une liste des ident qui se regroupent\n",
    "list_id_grp=pd.concat([gertrude_grp[['ident_x','id_grp']],gertrude_grp[['ident_y','id_grp']].rename(columns={'ident_y' : 'ident_x'})]).drop_duplicates([\n",
    "    'ident_x','id_grp']).groupby('id_grp').ident_x.agg(lambda x : set(sorted(tuple(x)))).tolist()\n",
    "dico={}\n",
    "for e,c in enumerate(gertrude_grp.ident_x.tolist()) : \n",
    "    for l in list_id_grp : \n",
    "        if c in l :\n",
    "            #print('c dans l','c : ',c,'l :',l)\n",
    "            if c not in dico.keys() :\n",
    "                dico[c]=list(l)\n",
    "            else : \n",
    "                dico[c]+=list(l)\n",
    "list_id_grp_temp=set([ v for v in {k : tuple(set(sorted(dico[k]))) for k in dico.keys()}.values()])\n",
    "list_id_grp_final=set(v for v in {c :b for c in gertrude_grp.ident_x.tolist() \n",
    " for b in [a for a in list_id_grp_temp if c in a] \n",
    " if len(b) == max([len(c) for c in [a for a in list_id_grp_temp if c in a]])}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affecter un identifiant\n",
    "def groupe_pt_gertrude_etape2(df_local) : \n",
    "    \"\"\"\n",
    "    regrouper les points à partir d'une liste d'ident selon l'ordre d'apparition \n",
    "    \"\"\"\n",
    "    limite_box=MultiPoint([(a.x,a.y) for a in df_local.itertuples()]).bounds\n",
    "    plg_long, plg_larg=limite_box[2]-limite_box[0],limite_box[3]-limite_box[1]\n",
    "    if max(plg_long,plg_larg)==plg_long : \n",
    "        df_local['next_sens']=df_local.sort_values('x').sens_cir.shift(-1,fill_value='NC')\n",
    "        df_local['prev_sens']=df_local.sort_values('x').sens_cir.shift(1,fill_value='NC')\n",
    "    else : \n",
    "        df_local['next_sens']=df_local.sort_values('y').sens_cir.shift(-1,fill_value='NC')\n",
    "        df_local['prev_sens']=df_local.sort_values('y').sens_cir.shift(1,fill_value='NC')\n",
    "    df_local['sens_comp']=df_local.apply(lambda x : x.prev_sens if x.next_sens=='NC' else x.next_sens,axis=1)\n",
    "    df_local['verif_sens']=df_local.apply(lambda x : verif_sens(x['sens_cir'],x['sens_comp']),axis=1)\n",
    "    df_local['id_grp']=-99\n",
    "    i=0\n",
    "    while i < len(df_local)-1:\n",
    "        if df_local.iloc[i].verif_sens : \n",
    "            df_local.loc[df_local.index.isin(df_local.iloc[i:i+2].index.tolist()),'id_grp']=i\n",
    "            i+=2\n",
    "        else : \n",
    "            i+=1\n",
    "    return\n",
    "\n",
    "# à partior de la liste des ident groupé : \n",
    "grp_final2=grp_final.copy()\n",
    "# on selectionne ces points dans la df\n",
    "increment=grp_final2.id_grp.max()\n",
    "for l in list_id_grp_final : \n",
    "    #if 'Z8CT13' not in l : continue\n",
    "    df_local=gertrude_etape2.loc[gertrude_etape2.ident.isin(l)].copy()\n",
    "    if len(df_local)==1:\n",
    "        continue\n",
    "    if len(df_local)==2 : \n",
    "        if (gertrude_join.loc[(gertrude_join.ident_x.isin(l)) & (gertrude_join['ident_x'] != gertrude_join['ident_y'])].valid_sens).all() : \n",
    "            increment+=1\n",
    "            df_local['id_grp']=increment\n",
    "            grp_final2=pd.concat([grp_final2,df_local[['ident','id_grp']]],sort=False)\n",
    "    else :\n",
    "        groupe_pt_gertrude_etape2(df_local)\n",
    "        increment+=grp_final2.id_grp.max()\n",
    "        df_local.loc[df_local['id_grp']!=-99,'id_grp']=df_local['id_grp']+increment\n",
    "        grp_final2=pd.concat([df_local[['ident','id_grp']],grp_final2],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rappatrier l'id_grp sur les données de base et export pour modif manuelle\n",
    "gertrude.merge(grp_final2, on='ident',how='left').to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude.shp')\n",
    "\n",
    "#import des modifs manuelles et mise en forme finale\n",
    "gertrude_manu=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude.shp')\n",
    "gertrude_manu.loc[(gertrude_manu.id_grp.isna()) | (gertrude_manu.id_grp==-99),'id_grp']=gertrude_manu.loc[(gertrude_manu.id_grp.isna()) | (gertrude_manu.id_grp==-99),'id_grp'].reset_index().index + gertrude_manu.id_grp.max()\n",
    "gertrude_manu.loc[gertrude_manu.type_grp.isnull(),'type_grp']='auto'\n",
    "#export pour dernière modif manuelle\n",
    "gertrude_manu.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude_final.shp')\n",
    "\n",
    "#fichier final\n",
    "gertrude_fin=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude_final.shp')\n",
    "#concat avec les siredo\n",
    "cle_assoc_siredo['id_grpsiredo']=cle_assoc_siredo['id_grpsiredo']+gertrude_fin.id_grp.max()\n",
    "cpt_perm_final=pd.concat([gertrude_fin,siredo.merge(cle_assoc_siredo, on='ident').rename(columns={'id_grpsiredo':'id_grp'})],axis=0, sort=False)\n",
    "cpt_perm_final.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cpt_perm_groupe.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Affecter du trafic au filaire de voie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### On commence par affecter le trafic aux troncons des catégories 1,2,3 à partirdes comptages permanents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#trouver la ligne la plus proche\n",
    "\n",
    "lgn_proche_perm=cpt_perm_final.merge(plus_proche_voisin(cpt_perm_final,gdf_rhv_groupe,10,'ident','ident'),left_on='ident', right_on='ident_left',how='left').merge(\n",
    "    gdf_rhv_groupe[['ident','cat_rhv','rgraph_dbl','idtronc']], left_on='ident_right', right_on='ident', how='left').rename(columns={'ident_right':'ident_lgn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer le trafic total par point de comptage permanent\n",
    "def calcul_tmjo_2sens_perm(tmjo, rgraph_dbl, nb_cpt, df,id_grp,nom_attr_trafic,nom_attr_id_grp,id_cpt,nom_attr_id_cpt) :\n",
    "    \"\"\"calculer le tmja 2 sens en fonction du nb de compteur et du sens unique ou non\"\"\"\n",
    "    if nb_cpt==2 : \n",
    "        return df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].sum(), df.loc[df[nom_attr_id_grp]==id_grp].groupby(nom_attr_id_grp)[nom_attr_id_cpt].agg(\n",
    "            lambda x : tuple(x)).values[0]\n",
    "    else : \n",
    "        if rgraph_dbl==1 : \n",
    "            return tmjo*2, (id_cpt,)\n",
    "        else : \n",
    "            return tmjo, (id_cpt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_attr_sup=lgn_proche_perm.merge(lgn_proche_perm.groupby('id_grp')['gid'].nunique(), left_on='id_grp',\n",
    "                    right_index=True).rename(columns={'gid_y':'nb_cpt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_attr_sup['tmjo_2_sens']=lgn_proche_attr_sup.apply(lambda x : calcul_tmjo_2sens_perm(x['mjo_val'],x['rgraph_dbl'],\n",
    "                                                                                          x['nb_cpt'], lgn_proche_attr_sup,x['id_grp'],\n",
    "                                                                                          'mjo_val','id_grp',x['ident_x'],'ident_x')[0],axis=1)\n",
    "lgn_proche_attr_sup['id_cpt_2_sens']=lgn_proche_attr_sup.apply(lambda x : calcul_tmjo_2sens_perm(x['mjo_val'],x['rgraph_dbl'],\n",
    "                                                                                          x['nb_cpt'], lgn_proche_attr_sup,x['id_grp'],\n",
    "                                                                                          'mjo_val','id_grp',x['ident_x'],'ident_x')[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour les lignes de cat rhv 1 ou 2 ou 3 : basculer ce trafic vers l'ensemble des lignes du même idtronc issu du cat_rhv_123\n",
    "mjo_tronc_cat123=lgn_proche_attr_sup.loc[lgn_proche_attr_sup['cat_rhv'].isin(['1','2','3','61','62','63'])][['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].merge(\n",
    "    gdf_rhv_groupe_123[['ident','idtronc']].rename(columns={'ident':'ident_lgn'}),how='left')#[['idtronc','tmjo_2_sens']]\n",
    "mjo_tronc_cat123=mjo_tronc_cat123.drop_duplicates(['idtronc','tmjo_2_sens'])\n",
    "\n",
    "#il rest des données dupliquée, notamment car certains troncons supportent plueiusr points de comptages. dans ce cas on ne garde que la valeur max (vérifié)\n",
    "traf_max=mjo_tronc_cat123.loc[mjo_tronc_cat123.duplicated('idtronc',keep=False)].groupby('idtronc').tmjo_2_sens.max().reset_index().merge(\n",
    "mjo_tronc_cat123, on='idtronc')\n",
    "traf_max=traf_max.loc[traf_max['tmjo_2_sens_x']==traf_max['tmjo_2_sens_y']].drop('tmjo_2_sens_y',axis=1).rename(columns={'tmjo_2_sens_x':'tmjo_2_sens'}).set_index('idtronc').copy()\n",
    "mjo_tronc_cat123.set_index('idtronc',inplace=True)\n",
    "mjo_tronc_cat123.update(traf_max.reset_index().set_index('idtronc'))\n",
    "mjo_tronc_cat123.reset_index(inplace=True)\n",
    "mjo_tronc_cat123_v2=mjo_tronc_cat123.drop_duplicates(['idtronc','tmjo_2_sens'])\n",
    "#jointure entre l'idtronc issu des cat 1,2,3 et les lignes qui y sont affectées puis jointure avec la df de base des lignes\n",
    "gdf_rhv_cpt_perm_123=gdf_rhv_groupe_123.merge(mjo_tronc_cat123_v2, on='idtronc', how='left')\n",
    "gdf_rhv_cpt_perm_123.loc[~gdf_rhv_cpt_perm_123.tmjo_2_sens.isna(),'type_cpt']='permanent'\n",
    "gdf_rhv_cpt_perm_123.drop('ident_lgn',axis=1, inplace=True)\n",
    "#export\n",
    "gdf_rhv_cpt_perm_123[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens', 'type_cpt']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_v0.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ensuite les comptages ponctuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#à partir de affect final, chercher les points qui supporte pas déjà un tafic d'un comptage permanent (attention, on se base sur les troncon elementaires toute\n",
    "#catégorie rhv)\n",
    "cpt_ponct_ok=cpt_pct_l93.merge(affect_finale.loc[~affect_finale.idtronc.isin(lgn_proche_perm.idtronc.tolist())][['ident','idtronc']], on='ident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#trouver la ligne la plus proche\n",
    "lgn_proche_ponct=cpt_ponct_ok.merge(plus_proche_voisin(cpt_ponct_ok,gdf_rhv_groupe,10,'ident','ident'),left_on='ident', right_on='ident_left',how='left').merge(\n",
    "    gdf_rhv_groupe[['ident','cat_rhv','rgraph_dbl','idtronc']], left_on='ident_right', right_on='ident', how='left').rename(\n",
    "    columns={'ident_right':'ident_lgn','idtronc_x':'idtronc_tt_rhv','ident_x':'ident'}).drop(['ident_left','ident_y','idtronc_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer le trafic total par point de comptage permanent\n",
    "def calcul_tmjo_2sens_ponct(tmjo, rgraph_dbl, nb_cpt,sens_uniq, df,id_grp,nom_attr_trafic,nom_attr_id_grp,id_cpt,nom_attr_id_cpt) :\n",
    "    \"\"\"calculer le tmja 2 sens en fonction du nb de compteur et du sens unique ou non\"\"\"\n",
    "    if nb_cpt==2 : \n",
    "        if (df.loc[df[nom_attr_id_grp]==id_grp].sens_unique==True).all() : #si sur unmm idtronc les 2 copt sont en sens uniq, on garde le max\n",
    "            return (df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].max(),df.loc[(df[nom_attr_id_grp]==id_grp) & (df[nom_attr_trafic]==\n",
    "                        df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].max())][nom_attr_id_cpt].values[0],)\n",
    "        else :\n",
    "            return df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].sum(), df.loc[df[nom_attr_id_grp]==id_grp].groupby(nom_attr_id_grp)[nom_attr_id_cpt].agg(\n",
    "                lambda x : tuple(x)).values[0]\n",
    "    else : \n",
    "        if rgraph_dbl==1 : \n",
    "            return tmjo*2, (id_cpt,)\n",
    "        else : \n",
    "            return tmjo, (id_cpt,)\n",
    "        \n",
    "#calcul du tmjo_2_sens\n",
    "lgn_proche_ponct_attr_sup=lgn_proche_ponct.merge(lgn_proche_ponct.groupby('idtronc_tt_rhv')['ident'].nunique(), on='idtronc_tt_rhv').rename(\n",
    "    columns={'ident_y':'nb_cpt','ident_x':'ident'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_ponct_attr_sup['tmjo_2_sens']=lgn_proche_ponct_attr_sup.apply(lambda x : \n",
    "                        calcul_tmjo_2sens_ponct(x['tmjo_tv'],x['rgraph_dbl'],x['nb_cpt'],x['sens_unique'], lgn_proche_ponct_attr_sup,\n",
    "                                          x['idtronc_tt_rhv'],'tmjo_tv','idtronc_tt_rhv',x['ident'],'ident')[0],axis=1)\n",
    "lgn_proche_ponct_attr_sup['id_cpt_2_sens']=lgn_proche_ponct_attr_sup.apply(lambda x : \n",
    "                        calcul_tmjo_2sens_ponct(x['tmjo_tv'],x['rgraph_dbl'],x['nb_cpt'],x['sens_unique'], lgn_proche_ponct_attr_sup,\n",
    "                                          x['idtronc_tt_rhv'],'tmjo_tv','idtronc_tt_rhv',x['ident'],'ident')[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verif doublon de compteur --si besoin\n",
    "lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup.duplicated('ident')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 cas possibles : soit le comptage ponctuel est sur un troncon dejà concerné issu des troncon elementaire categorie 1,2,3 ou non\n",
    "\n",
    "#séparer les points restant dans les 2 catégories : \n",
    "ponct_sur_perm_123=lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident_lgn'].isin(\n",
    "    gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['type_cpt']=='permanent'].ident.tolist())].copy()\n",
    "ponct_libre_123=lgn_proche_ponct_attr_sup.loc[~lgn_proche_ponct_attr_sup['ident_lgn'].isin(\n",
    "    gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['type_cpt']=='permanent'].ident.tolist())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rappatriement des compteurs sur les idtronc des cat 1,2,3 : il y a des doublons car les troncons sont long, donc on separe, et on va traiter les ponct_libr sans doublons\n",
    "ponct_libre_123_tot=gdf_rhv_groupe_123[['ident','idtronc']].merge(ponct_libre_123[['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].rename(columns={'ident_lgn':'ident'}), \n",
    "    on='ident')[['idtronc','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates(['idtronc','tmjo_2_sens','id_cpt_2_sens'])\n",
    "list_pct_libre_dbl=ponct_libre_123_tot.loc[ponct_libre_123_tot.duplicated('idtronc',keep=False)].sort_values('idtronc').idtronc.unique()\n",
    "#affectation des ponctuels libre sans doublons\n",
    "pct_libre_123_ss_dbl=ponct_libre_123_tot.loc[~ponct_libre_123_tot['idtronc'].isin(list_pct_libre_dbl)].copy()\n",
    "pct_libre_123_ss_dbl['type_cpt']='ponctuel'\n",
    "gdf_rhv_cpt_perm_123.set_index('idtronc', inplace=True)\n",
    "gdf_rhv_cpt_perm_123.update(pct_libre_123_ss_dbl.set_index('idtronc'))\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl=gdf_rhv_groupe_123.merge(pct_libre_123_ss_dbl, on='idtronc')\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl.loc[~gdf_rhv_groupe_123_pctLibreSsDbl.tmjo_2_sens.isna(),'type_cpt']='ponctuel'\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctSsDblv0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affectation des ponctuel libre avec doublons\n",
    "#l'idee c'est pour un idtronc des cat 1,2,3 on a la liste des ident, on affecte sur les ident des idtronc elementaire \n",
    "#sur les lignes, puis on propage sur les tronc de la cat 1,2,3\n",
    "\n",
    "pct_libre_123_avec_dbl=ponct_libre_123_tot.loc[ponct_libre_123_tot['idtronc'].isin(list_pct_libre_dbl)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idtronc_test in sorted(pct_libre_123_avec_dbl.idtronc.unique()) : \n",
    "    print(idtronc_test)\n",
    "    #récupérer l'idtronc toute cat_rhv des compteurs avec doublons et faire la jointure du trafic sur les lignes concernees\n",
    "    pct_libre_123_avec_dbl_decompose=lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident'].isin(\n",
    "        [a for b in pct_libre_123_avec_dbl.loc[pct_libre_123_avec_dbl['idtronc']==idtronc_test].\n",
    "         id_cpt_2_sens.tolist() for a in b])][['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']].copy()\n",
    "\n",
    "    #initialisation des paramètres : \n",
    "    ligne_ok=gdf_rhv_groupe.merge(pct_libre_123_avec_dbl_decompose.rename(columns={'idtronc_tt_rhv':'idtronc'}), on='idtronc').merge(\n",
    "        graph_filaire_123[['ident','source','target']], on='ident').drop(['source_x', 'target_x'],axis=1).rename(\n",
    "        columns={'source_y':'source', 'target_y':'target'})#lignes avec trafic issues des troncons elemntaires et on garde les ousrces et target liées uniquement aux cat 1,2,3\n",
    "    lgn_id_tronc_123_vide=gdf_rhv_groupe_123.loc[(gdf_rhv_groupe_123['idtronc']==idtronc_test) & \n",
    "                                                 (~gdf_rhv_groupe_123.ident.isin(ligne_ok.ident.tolist()))] #l'ensemble des lignes a renseigner\n",
    "    while ~lgn_id_tronc_123_vide.empty :\n",
    "        list_noeud_trafic=ligne_ok.source.tolist()+ligne_ok.target.tolist() #les noeuds avec du trafic\n",
    "        list_noeud_trafic_null=lgn_id_tronc_123_vide.source.tolist()+lgn_id_tronc_123_vide.target.tolist() #l'inverse\n",
    "        lgn_a_renseigner=lgn_id_tronc_123_vide.loc[((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic_null))) | \n",
    "                ((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic_null)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic)))].copy()\n",
    "        if lgn_a_renseigner.empty : break #si pas de ligne a renseigner on sort\n",
    "        ligne_ok['noeud_partage']=ligne_ok.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic_null else x['target'], axis=1) #pour faire une jointure entre les lignes a trafic et les autres\n",
    "        lgn_a_renseigner['noeud_partage']=lgn_a_renseigner.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic else x['target'], axis=1)\n",
    "        lgn_a_renseigner=lgn_a_renseigner.merge(ligne_ok[['noeud_partage','tmjo_2_sens','id_cpt_2_sens']], on='noeud_partage') #recupere le tmj\n",
    "        ligne_ok=pd.concat([ligne_ok,lgn_a_renseigner], axis=0, sort=False)#on ajoute les lignes avec du trafic aux autres\n",
    "        lgn_id_tronc_123_vide=lgn_id_tronc_123_vide.loc[~lgn_id_tronc_123_vide.ident.isin(ligne_ok.ident.tolist())].copy() #on met à jour pour la boucle\n",
    "    ligne_ok.drop_duplicates('ident',inplace=True)# les ronds points crees des doublons\n",
    "    ligne_ok['type_cpt']='ponctuel'\n",
    "    #mettre à jour la donnees de base\n",
    "    gdf_rhv_cpt_perm_123.update(ligne_ok[['ident','tmjo_2_sens','id_cpt_2_sens','type_cpt']].set_index('ident'))\n",
    "#export\n",
    "#gdf_rhv_groupe_123_pctLibreAvecDbl.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctAvecDblv0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affectation des ponctuels sur permanents : de la mm façon que les ponctuels en doublons\n",
    "\n",
    "#trouver la liste des idtronc cat_rhv = 1,2,3 avec ponct sur perm\n",
    "idtronc123_pct_sur_perm=gdf_rhv_groupe_123[['ident','idtronc']].merge(ponct_sur_perm_123[['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].rename(columns={'ident_lgn':'ident'}), \n",
    "    on='ident')[['idtronc','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates()\n",
    "\n",
    "for idtronc_test in idtronc123_pct_sur_perm.idtronc.unique() : \n",
    "    print(idtronc_test)\n",
    "    #trouver les id tronc elementaire des comptages ponct et perm concerne\n",
    "    ligne_ok= gdf_rhv_groupe.merge(pd.concat([lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident'].isin(\n",
    "        [a for b in idtronc123_pct_sur_perm.loc[idtronc123_pct_sur_perm['idtronc']==idtronc_test].id_cpt_2_sens.tolist()\n",
    "         for a in b])][['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates(['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']),\n",
    "    gdf_rhv_groupe_123.loc[gdf_rhv_groupe_123['idtronc']==idtronc_test].merge(\n",
    "        lgn_proche_attr_sup, left_on='ident', right_on='ident_lgn')[['idtronc_y','tmjo_2_sens','id_cpt_2_sens']].rename(\n",
    "        columns={'idtronc_y' : 'idtronc_tt_rhv'}).drop_duplicates()],axis=0, sort=False), left_on='idtronc', right_on='idtronc_tt_rhv')\n",
    "\n",
    "    lgn_id_tronc_123_vide=gdf_rhv_groupe_123.loc[(gdf_rhv_groupe_123['idtronc']==idtronc_test) & \n",
    "                                                     (~gdf_rhv_groupe_123.ident.isin(ligne_ok.ident.tolist()))]\n",
    "\n",
    "    while ~lgn_id_tronc_123_vide.empty :\n",
    "        list_noeud_trafic=ligne_ok.source.tolist()+ligne_ok.target.tolist() #les noeuds avec du trafic\n",
    "        list_noeud_trafic_null=lgn_id_tronc_123_vide.source.tolist()+lgn_id_tronc_123_vide.target.tolist() #l'inverse\n",
    "        lgn_a_renseigner=lgn_id_tronc_123_vide.loc[((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic_null))) | \n",
    "                ((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic_null)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic)))].copy()\n",
    "        if lgn_a_renseigner.empty : break #si pas de ligne a renseigner on sort\n",
    "        ligne_ok['noeud_partage']=ligne_ok.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic_null else x['target'], axis=1) #pour faire une jointure entre les lignes a trafic et les autres\n",
    "        lgn_a_renseigner['noeud_partage']=lgn_a_renseigner.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic else x['target'], axis=1)\n",
    "        lgn_a_renseigner=lgn_a_renseigner.merge(ligne_ok[['noeud_partage','tmjo_2_sens','id_cpt_2_sens']], on='noeud_partage') #recupere le tmj\n",
    "        ligne_ok=pd.concat([ligne_ok,lgn_a_renseigner], axis=0, sort=False)#on ajoute les lignes avec du trafic aux autres\n",
    "        lgn_id_tronc_123_vide=lgn_id_tronc_123_vide.loc[~lgn_id_tronc_123_vide.ident.isin(ligne_ok.ident.tolist())].copy() #on met à jour pour la boucle\n",
    "    ligne_ok.drop_duplicates('ident',inplace=True)# les ronds points crees des doublons\n",
    "    ligne_ok['type_cpt']='ponctuel'\n",
    "    #mettre à jour la donnees de base\n",
    "    gdf_rhv_cpt_perm_123.update(ligne_ok[['ident','tmjo_2_sens','id_cpt_2_sens','type_cpt']].set_index('ident'))\n",
    "\n",
    "#gdf_rhv_groupe_123_pctLibreAvecDbl.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctAvecDblv01.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_cpt_perm_123['id_cpt_2_sens'].fillna('NC', inplace=True)\n",
    "gdf_rhv_cpt_perm_123['id_cpt_2_sens']=gdf_rhv_cpt_perm_123.apply(lambda x : ', '.join([str(a) for a in x['id_cpt_2_sens']] \n",
    "                if not isinstance(x['id_cpt_2_sens'],np.int64) else str(x['id_cpt_2_sens']) ), axis=1)\n",
    "gdf_rhv_cpt_perm_123.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_v1.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On peut essayer de trouver les lignes qui intersectent d'autres avec lignes avec du trafci pour les completer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verifier si necessaire\n",
    "gdf_rhv_cpt_perm_123.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver les noeuds du graph pour lesquels les lignes qui arrivent\n",
    "\n",
    "#simplifier la topologie en affectant une seule valuer de noeud aux voie arrivant sur un rond point\n",
    "#trouver les noeud faisant partie d'un rond point \n",
    "df=import_donnes_base('local_otv','linearisation_bm', 'graph_rhv_123','graph_rhv_123_vertices_pgr')\n",
    "df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=identifier_rd_pt(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_rdpt=df_avec_rd_pt.loc[~df_avec_rd_pt.id_rdpt.isna()].copy()\n",
    "dico_noeud=creer_dico_noeud_rdpt(lgn_rdpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# àpartir du dico on va remplacer les valeurs de source ou target par la valeur de clé du dico. \n",
    "gdf_rhv_rdpt_simple=gdf_rhv_cpt_perm_123.loc[~gdf_rhv_cpt_perm_123.index.isin(lgn_rdpt.ident.to_list())].copy()\n",
    "#remplacement des sources et targets : \n",
    "simplifier_noeud_rdpt(gdf_rhv_rdpt_simple, dico_noeud)\n",
    "#puis recalculer le count du nb de ligne par noeud (en otant les lignes qui font les rdpoints)\n",
    "cnt_maj=maj_graph_rdpt(gdf_rhv_rdpt_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_x</th>\n",
       "      <th>ident</th>\n",
       "      <th>domanial</th>\n",
       "      <th>groupe</th>\n",
       "      <th>cat_dig</th>\n",
       "      <th>cat_rhv</th>\n",
       "      <th>passage</th>\n",
       "      <th>rggraph_nd</th>\n",
       "      <th>rggraph_na</th>\n",
       "      <th>rgraph_dbl</th>\n",
       "      <th>numero</th>\n",
       "      <th>cdate</th>\n",
       "      <th>mdate</th>\n",
       "      <th>id_ign</th>\n",
       "      <th>nature</th>\n",
       "      <th>sens</th>\n",
       "      <th>codevoie_d</th>\n",
       "      <th>importance</th>\n",
       "      <th>id_y</th>\n",
       "      <th>geometry</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>tmjo_2_sens</th>\n",
       "      <th>id_cpt_2_sens</th>\n",
       "      <th>type_cpt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idtronc</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1872.0</th>\n",
       "      <td>393.0</td>\n",
       "      <td>43498</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>None</td>\n",
       "      <td>19986.0</td>\n",
       "      <td>649.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Avenue Roul</td>\n",
       "      <td>19950406145300</td>\n",
       "      <td>20140910100448</td>\n",
       "      <td>TRONROUT43498</td>\n",
       "      <td>Route Ã  1 chaussÃ©e</td>\n",
       "      <td>Double</td>\n",
       "      <td>NR</td>\n",
       "      <td>4</td>\n",
       "      <td>TRONROUT43498</td>\n",
       "      <td>LINESTRING Z (415556.350 6418346.460 0.000, 41...</td>\n",
       "      <td>1</td>\n",
       "      <td>4967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>N, C</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id_x  ident domanial  groupe cat_dig cat_rhv passage  rggraph_nd  \\\n",
       "idtronc                                                                      \n",
       "1872.0   393.0  43498        4     1.0       4       2    None     19986.0   \n",
       "\n",
       "         rggraph_na  rgraph_dbl       numero           cdate           mdate  \\\n",
       "idtronc                                                                        \n",
       "1872.0        649.0         1.0  Avenue Roul  19950406145300  20140910100448   \n",
       "\n",
       "                id_ign                nature    sens codevoie_d importance  \\\n",
       "idtronc                                                                      \n",
       "1872.0   TRONROUT43498  Route Ã  1 chaussÃ©e  Double         NR          4   \n",
       "\n",
       "                  id_y                                           geometry  \\\n",
       "idtronc                                                                     \n",
       "1872.0   TRONROUT43498  LINESTRING Z (415556.350 6418346.460 0.000, 41...   \n",
       "\n",
       "         source  target  tmjo_2_sens id_cpt_2_sens type_cpt  \n",
       "idtronc                                                      \n",
       "1872.0        1    4967          NaN          N, C      NaN  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf_rhv_rdpt_simple.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer en boucle les trafics manquants sur les carrefrour avec 3 troncon qui arrivent dont 1 seul trafic inconnu\n",
    "\n",
    "df_trafic_extrapoles=gdf_rhv_rdpt_simple.reset_index().set_index('idtronc')\n",
    "liste_noeud_traites=[]\n",
    "#initialiser les noeuds concernes\n",
    "liste_noeud,df_noeud_tot=trouver_noeud_3tronc_1NaN(df_trafic_extrapoles, liste_noeud_traites)\n",
    "while liste_noeud : \n",
    "    #calculer les trafics manquants\n",
    "    dico_trafic, dico_erreur=maj_carrefour_3_troncons(liste_noeud,gdf_rhv_cpt_perm_123,gdf_rhv_rdpt_simple,df_noeud_tot,graph_filaire_123_vertex)\n",
    "    #mettre à jour le trafic\n",
    "    df_trafic_extrapoles.update(pd.DataFrame.from_dict(dico_trafic, orient='index',columns=['tmjo_2_sens']))\n",
    "    #rechercher les nouveaux noeuds sucupetibles d'etre mis à jour\n",
    "    liste_noeud_traites+=[a for a in dico_erreur.keys()]\n",
    "    liste_noeud,df_noeud_tot=trouver_noeud_3tronc_1NaN(df_trafic_extrapoles, liste_noeud_traites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trafic_extrapoles.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens','source','target']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_extrapol.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MMM\n",
    "En premier lieu il faut mettre à jour les attributs en renomant de façon explicite puis en faisant la somme des 2 sens de circulation pour les voies doucble sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer les données et convertir les noms de champs\n",
    "fichier_src=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\MMM\\2017_Matin_PR\\2017_HPM_link.SHP')\n",
    "#définir les colonnes définitives (PLUS TARD ON FERA UN FILTRE EN AMONT)\n",
    "liste_col_def=['NO', 'FROMNODENO', 'TONODENO', 'TYPENO', 'TSYSSET', 'LENGTH', 'NUMLANES', 'CAPPRT', 'V0_TV', 'VOLVEHPR~1', \n",
    "'Q_HC_PL', 'Q_HC_TV', 'Q_HC_VL', 'Q_HPM_PL', 'Q_HPM_TV', 'Q_HPM_VL', 'Q_HPS_PL', 'Q_HPS_TV', 'Q_HPS_VL', 'Q_Jour_PL', 'Q_Jour_TV',\n",
    "'Q_Jour_VL','coef_HC_PL','coef_HC_TV','coef_HC_VL','Ocup_VL_PL','V0_PL','V0_VL','VCharg_PL','VCharg_VL','VOLPCUP~22',\n",
    "'SHAREHGV', 'VOLVEH_~23', 'VOLVEH_~24', 'VOLCAPR~25', 'VEHHOUR~26', 'SPEEDLIMIT',\n",
    "'R_NO', 'R_FROMNODENO', 'R_TONODENO', 'R_TYPENO', 'R_TSYSSET','R_LENGTH', 'R_NUMLANES', 'R_CAPPRT', 'r_V0_TV', 'R_VOLVEHPR~1', \n",
    "'R_Q_HC_PL', 'R_Q_HC_TV', 'R_Q_HC_VL', 'R_Q_HPM_PL', 'R_Q_HPM_TV', 'R_Q_HPM_VL', \n",
    "'R_Q_HPS_PL', 'R_Q_HPS_TV', 'R_Q_HPS_VL', 'R_Q_Jour_PL', 'R_Q_Jour_TV', 'R_Q_Jour_VL', 'R_coef_HC_PL', 'R_coef_HC_TV', 'R_coef_HC_VL', 'R_Ocup_VL_PL', 'R_V0_PL', \n",
    "'R_V0_VL', 'R_VCharg_PL', 'R_VCharg_VL', 'R_VOLPCUP~22', 'R_SHAREHGV', 'R_VOLVEH_~23', 'R_VOLVEH_~24', 'R_VOLCAPR~25', 'R_VEHHOUR~26', 'R_SPEEDLIMIT']\n",
    "#creer un dico de renomage et renommer\n",
    "dico_renomage={a: b for a, b in zip(fichier_src.columns,liste_col_def)}\n",
    "fichier_src.rename(columns=dico_renomage, inplace=True)\n",
    "#ne conserver dans un premier temps que les attributs relatifs au trafic sur la journée\n",
    "fichier_src_simpl=fichier_src[['geometry','NO', 'FROMNODENO', 'TONODENO', 'TYPENO', 'TSYSSET', 'LENGTH', 'NUMLANES', 'CAPPRT', 'V0_TV', \n",
    "             'VOLVEHPR~1']+[a for a in fichier_src.columns if 'Jour' in a ]].copy()\n",
    "#caculer le tmja_TV\n",
    "fichier_src_simpl['tmja_tv']=fichier_src_simpl.Q_Jour_TV+fichier_src_simpl.R_Q_Jour_TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_src_simpl.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\MMM_simplifie.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer graph et importer \n",
    "creer_graph(fichier_src_simpl, 'local_otv',id_name='NO', schema='public', table='mmm_graph', table_vertex='mmm_graph_vertices_pgr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer le graph\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    graph_mmm_filaire = gp.read_postgis('select * from linearisation_bm.mmm_graph', c.connexionPsy)\n",
    "    graph_mmm_filaire_vertex = gp.read_postgis('select * from linearisation_bm.mmm_graph_vertices_pgr', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer le fichier simplifie\n",
    "mmm_simple=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\MMM_simplifie.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## définir des relations entre les voies, à chaque point qui intersecte plus de 2 voie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#les noued s sup à 2 voies\n",
    "noeud_sup2Voies=graph_mmm_filaire_vertex.loc[graph_mmm_filaire_vertex['cnt']>2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver la correspondance entre les noeuds MMM et FV\n",
    "#depuis le MMM on cherche le noeud FV le plus proche et inversement,puis on fusionne\n",
    "appariement_noeud_mmm_fv=appariement_noeud_mmm_fv(noeud_sup2Voies, graph_filaire_123_vertex, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver la correspondance entre les lignes\n",
    "#importer les données de correspondance\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    cle_mmm_rhv = gp.read_postgis('select * from linearisation_bm.appariement_etape1_2', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver les noeuds fv concernes par des lignes à mettre à jour\n",
    "noeud_grp=noeud_fv_ligne_ss_trafic(df_trafic_extrapoles,'min 1 ok')[0]\n",
    "#ne conserver que le noeuds relatif à une certaine catégorie\n",
    "#liste_noeud_a_traiter=noeud_grp.loc[(noeud_grp['estimable']) & (noeud_grp.apply(lambda x : '1' in x['cat_rhv'],axis=1))].noeud.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CorrespondanceError??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "38\n",
      "112\n",
      "130\n",
      "168\n",
      "194\n",
      "215\n",
      "218\n",
      "317\n",
      "403\n",
      "580\n",
      "581\n",
      "687\n",
      "737\n",
      "763\n",
      "824\n",
      "916\n",
      "984\n",
      "1013\n",
      "1021\n",
      "1026\n",
      "1031\n",
      "1044\n",
      "1045\n",
      "1063\n",
      "1078\n",
      "1128\n",
      "1133\n",
      "1134\n",
      "1163\n",
      "1245\n",
      "1272\n",
      "1282\n",
      "1310\n",
      "1325\n",
      "1354\n",
      "1395\n",
      "1467\n",
      "1469\n",
      "1481\n",
      "1490\n",
      "1505\n",
      "1508\n",
      "1536\n",
      "1569\n",
      "1709\n",
      "1831\n",
      "1850\n",
      "1851\n",
      "1881\n",
      "1907\n",
      "1924\n",
      "1949\n",
      "1950\n",
      "2019\n",
      "2025\n",
      "2048\n",
      "2064\n",
      "2097\n",
      "2234\n",
      "2290\n",
      "2368\n",
      "2416\n",
      "2417\n",
      "2432\n",
      "2434\n",
      "2541\n",
      "2643\n",
      "2657\n",
      "2722\n",
      "2728\n",
      "2734\n",
      "2789\n",
      "3025\n",
      "3114\n",
      "3216\n",
      "3253\n",
      "3264\n",
      "3274\n",
      "3332\n",
      "3353\n",
      "3427\n",
      "3449\n",
      "3580\n",
      "3609\n",
      "3614\n",
      "3639\n",
      "3786\n",
      "3813\n",
      "3877\n",
      "3927\n",
      "3944\n",
      "4013\n",
      "4024\n",
      "4068\n",
      "4071\n",
      "4100\n",
      "4105\n",
      "4127\n",
      "4217\n",
      "4225\n",
      "4317\n",
      "4319\n",
      "4375\n",
      "4431\n",
      "4459\n",
      "4560\n",
      "4578\n",
      "4606\n",
      "4620\n",
      "4624\n",
      "4655\n",
      "4656\n",
      "4669\n",
      "4670\n",
      "4850\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "('float division by zero', 'occurred at index 45')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-194-611add34412d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mtrafc_rens\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrafic_mmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrafic_inconnus_prior_cat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmmm_simple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;31m#calcul des trafics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mtrafc_fin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalcul_trafic_rhv_depuisMMM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrafc_rens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgdf_rhv_cpt_perm_123\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\git\\Lin_uniq_BdxM\\Lin_uniq_BdxM\\Traitements\\Appariement.py\u001b[0m in \u001b[0;36mcalcul_trafic_rhv_depuisMMM\u001b[1;34m(trafc_rens, df_tot)\u001b[0m\n\u001b[0;32m    108\u001b[0m     trafc_rens['tmjo_2_sens_extrapol']=trafc_rens.apply(lambda x : \n\u001b[0;32m    109\u001b[0m         \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmjo_2_sens_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmjo_2_sens_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmjo_2_sens_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m                                                       axis=1)\n\u001b[0m\u001b[0;32m    111\u001b[0m     \u001b[1;31m#si plusieurs resultas possibles pour unident on garde le max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     trafc_fin=trafc_rens.loc[trafc_rens['traf_max']==trafc_rens.groupby('ident_a_rens').traf_max.transform(max)].merge(\n",
      "\u001b[1;32mc:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, result_type, args, **kwds)\u001b[0m\n\u001b[0;32m   6485\u001b[0m                          \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6486\u001b[0m                          kwds=kwds)\n\u001b[1;32m-> 6487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapplymap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    149\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 151\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_empty_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[1;31m# compute the result using the series generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;31m# wrap results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\git\\Lin_uniq_BdxM\\Lin_uniq_BdxM\\Traitements\\Appariement.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    107\u001b[0m                                                   else x['ident_y'], axis=1)\n\u001b[0;32m    108\u001b[0m     trafc_rens['tmjo_2_sens_extrapol']=trafc_rens.apply(lambda x : \n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmjo_2_sens_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmjo_2_sens_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmja_tv_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tmjo_2_sens_y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                                                       axis=1)\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m#si plusieurs resultas possibles pour unident on garde le max\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: ('float division by zero', 'occurred at index 45')"
     ]
    }
   ],
   "source": [
    "gdf_rhv_extrapol=gdf_rhv_cpt_perm_123.reset_index().set_index('idtronc')\n",
    "dico_erreur={}\n",
    "for noeud_fv in noeud_grp.loc[noeud_grp['estimable']].noeud.tolist() : \n",
    "    print(noeud_fv)\n",
    "    #noeud_fv=3580\n",
    "    #voies rhv concernees par le noeud\n",
    "    matrice_voie_rhv=corresp_noeud_rhv(gdf_rhv_rdpt_simple,noeud_fv)\n",
    "    #jointure via cle de passage avec voies du MMM pour la partie entrante. Si une des voies a pas de jointure  : pb ; si plusiuers voies MMM à joindre : pb\n",
    "    try : \n",
    "        joint_fv_mmm_e2=estim_mmm_jointure_voies(matrice_voie_rhv,cle_mmm_rhv)\n",
    "    except PasCorrespondanceError : \n",
    "        dico_erreur[noeud_fv]=('CorrespondanceError')\n",
    "        continue\n",
    "    #on ne garde que les voies qui ont un des deux tmjo avec un trafic inconnu\n",
    "    trafic_inconnus_prior_cat=isoler_trafic_inconnu(joint_fv_mmm_e2)\n",
    "    #passer les trafic du MMM\n",
    "    trafc_rens=trafic_mmm(trafic_inconnus_prior_cat,mmm_simple)\n",
    "    #calcul des trafics\n",
    "    try :\n",
    "        trafc_fin=calcul_trafic_rhv_depuisMMM(trafc_rens, gdf_rhv_cpt_perm_123)\n",
    "    except ZeroDivisionError : \n",
    "        dico_erreur[noeud_fv]=('ZeroDivisionError')\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>tmjo_2_sens</th>\n",
       "      <th>cat_rhv</th>\n",
       "      <th>nb_nan</th>\n",
       "      <th>estimable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>(12353.0, 12353.0, 12353.0, 12353.0, 12353.0, ...</td>\n",
       "      <td>(2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)</td>\n",
       "      <td>{12353.0: 9, -99.0: 2, 18574.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>38</td>\n",
       "      <td>(6014.0, 13082.0, -99.0, 1927.0)</td>\n",
       "      <td>(2, 2, 3, 3)</td>\n",
       "      <td>{6014.0: 1, 13082.0: 1, -99.0: 1, 1927.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>112</td>\n",
       "      <td>(9810.0, -1443.0, -99.0)</td>\n",
       "      <td>(2, 3, 2)</td>\n",
       "      <td>{9810.0: 1, -1443.0: 1, -99.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>130</td>\n",
       "      <td>(-99.0, -99.0, 23510.0, 54546.0)</td>\n",
       "      <td>(1, 1, 1, 1)</td>\n",
       "      <td>{-99.0: 2, 23510.0: 1, 54546.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>168</td>\n",
       "      <td>(3892.0, -99.0, 5358.0, 2617.0)</td>\n",
       "      <td>(3, 3, 3, 3)</td>\n",
       "      <td>{3892.0: 1, -99.0: 1, 5358.0: 1, 2617.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>194</td>\n",
       "      <td>(1878.0, 1878.0, 1878.0, 1878.0, 1878.0, 1613....</td>\n",
       "      <td>(3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3)</td>\n",
       "      <td>{1878.0: 9, 1613.0: 1, -99.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>215</td>\n",
       "      <td>(1535.0, -99.0, 5400.0, -99.0)</td>\n",
       "      <td>(3, 3, 3, 3)</td>\n",
       "      <td>{1535.0: 1, -99.0: 2, 5400.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>218</td>\n",
       "      <td>(-99.0, -99.0, 14427.0, 17985.0)</td>\n",
       "      <td>(3, 2, 2, 2)</td>\n",
       "      <td>{-99.0: 2, 14427.0: 1, 17985.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>317</td>\n",
       "      <td>(4488.0, -99.0, 4488.0, 4488.0, 4488.0, 4488.0...</td>\n",
       "      <td>(2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)</td>\n",
       "      <td>{4488.0: 12, -99.0: 1, 4412.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>403</td>\n",
       "      <td>(-99.0, -99.0, 7570.0, 7162.0)</td>\n",
       "      <td>(3, 2, 2, 3)</td>\n",
       "      <td>{-99.0: 2, 7570.0: 1, 7162.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7875</th>\n",
       "      <td>9351</td>\n",
       "      <td>(15302.0, 14320.0, 10896.0, -99.0)</td>\n",
       "      <td>(2, 2, 2, 2)</td>\n",
       "      <td>{15302.0: 1, 14320.0: 1, 10896.0: 1, -99.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7916</th>\n",
       "      <td>9405</td>\n",
       "      <td>(10505.0, -99.0, 2904.0)</td>\n",
       "      <td>(2, 2, 62)</td>\n",
       "      <td>{10505.0: 1, -99.0: 1, 2904.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7941</th>\n",
       "      <td>9439</td>\n",
       "      <td>(-99.0, 3942.0, -99.0, 10199.0)</td>\n",
       "      <td>(2, 3, 2, 2)</td>\n",
       "      <td>{-99.0: 2, 3942.0: 1, 10199.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7942</th>\n",
       "      <td>9440</td>\n",
       "      <td>(-99.0, 12724.0, 8154.0, -99.0)</td>\n",
       "      <td>(2, 2, 2, 2)</td>\n",
       "      <td>{-99.0: 2, 12724.0: 1, 8154.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8140</th>\n",
       "      <td>9675</td>\n",
       "      <td>(-99.0, 7344.0, -99.0, 12000.0)</td>\n",
       "      <td>(1, 2, 2, 1)</td>\n",
       "      <td>{-99.0: 2, 7344.0: 1, 12000.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8466</th>\n",
       "      <td>10076</td>\n",
       "      <td>(-99.0, 20.0, 2424.0)</td>\n",
       "      <td>(3, 3, 3)</td>\n",
       "      <td>{-99.0: 1, 20.0: 1, 2424.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8709</th>\n",
       "      <td>10341</td>\n",
       "      <td>(18007.0, -99.0, 8425.0)</td>\n",
       "      <td>(1, 1, 2)</td>\n",
       "      <td>{18007.0: 1, -99.0: 1, 8425.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8923</th>\n",
       "      <td>10582</td>\n",
       "      <td>(6992.0, -99.0, -99.0, 8607.0)</td>\n",
       "      <td>(2, 3, 3, 2)</td>\n",
       "      <td>{6992.0: 1, -99.0: 2, 8607.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8943</th>\n",
       "      <td>10607</td>\n",
       "      <td>(8808.0, 17139.0, -99.0, 17139.0)</td>\n",
       "      <td>(1, 1, 1, 1)</td>\n",
       "      <td>{8808.0: 1, 17139.0: 2, -99.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9373</th>\n",
       "      <td>11110</td>\n",
       "      <td>(21020.0, 9208.0, -99.0)</td>\n",
       "      <td>(2, 2, 2)</td>\n",
       "      <td>{21020.0: 1, 9208.0: 1, -99.0: 1}</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>176 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      noeud                                        tmjo_2_sens  \\\n",
       "11       14  (12353.0, 12353.0, 12353.0, 12353.0, 12353.0, ...   \n",
       "32       38                   (6014.0, 13082.0, -99.0, 1927.0)   \n",
       "91      112                           (9810.0, -1443.0, -99.0)   \n",
       "107     130                   (-99.0, -99.0, 23510.0, 54546.0)   \n",
       "142     168                    (3892.0, -99.0, 5358.0, 2617.0)   \n",
       "164     194  (1878.0, 1878.0, 1878.0, 1878.0, 1878.0, 1613....   \n",
       "184     215                     (1535.0, -99.0, 5400.0, -99.0)   \n",
       "187     218                   (-99.0, -99.0, 14427.0, 17985.0)   \n",
       "277     317  (4488.0, -99.0, 4488.0, 4488.0, 4488.0, 4488.0...   \n",
       "355     403                     (-99.0, -99.0, 7570.0, 7162.0)   \n",
       "...     ...                                                ...   \n",
       "7875   9351                 (15302.0, 14320.0, 10896.0, -99.0)   \n",
       "7916   9405                           (10505.0, -99.0, 2904.0)   \n",
       "7941   9439                    (-99.0, 3942.0, -99.0, 10199.0)   \n",
       "7942   9440                    (-99.0, 12724.0, 8154.0, -99.0)   \n",
       "8140   9675                    (-99.0, 7344.0, -99.0, 12000.0)   \n",
       "8466  10076                              (-99.0, 20.0, 2424.0)   \n",
       "8709  10341                           (18007.0, -99.0, 8425.0)   \n",
       "8923  10582                     (6992.0, -99.0, -99.0, 8607.0)   \n",
       "8943  10607                  (8808.0, 17139.0, -99.0, 17139.0)   \n",
       "9373  11110                           (21020.0, 9208.0, -99.0)   \n",
       "\n",
       "                                         cat_rhv  \\\n",
       "11          (2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)   \n",
       "32                                  (2, 2, 3, 3)   \n",
       "91                                     (2, 3, 2)   \n",
       "107                                 (1, 1, 1, 1)   \n",
       "142                                 (3, 3, 3, 3)   \n",
       "164            (3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3)   \n",
       "184                                 (3, 3, 3, 3)   \n",
       "187                                 (3, 2, 2, 2)   \n",
       "277   (2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2)   \n",
       "355                                 (3, 2, 2, 3)   \n",
       "...                                          ...   \n",
       "7875                                (2, 2, 2, 2)   \n",
       "7916                                  (2, 2, 62)   \n",
       "7941                                (2, 3, 2, 2)   \n",
       "7942                                (2, 2, 2, 2)   \n",
       "8140                                (1, 2, 2, 1)   \n",
       "8466                                   (3, 3, 3)   \n",
       "8709                                   (1, 1, 2)   \n",
       "8923                                (2, 3, 3, 2)   \n",
       "8943                                (1, 1, 1, 1)   \n",
       "9373                                   (2, 2, 2)   \n",
       "\n",
       "                                              nb_nan  estimable  \n",
       "11                {12353.0: 9, -99.0: 2, 18574.0: 1}       True  \n",
       "32      {6014.0: 1, 13082.0: 1, -99.0: 1, 1927.0: 1}       True  \n",
       "91                 {9810.0: 1, -1443.0: 1, -99.0: 1}       True  \n",
       "107               {-99.0: 2, 23510.0: 1, 54546.0: 1}       True  \n",
       "142      {3892.0: 1, -99.0: 1, 5358.0: 1, 2617.0: 1}       True  \n",
       "164                 {1878.0: 9, 1613.0: 1, -99.0: 1}       True  \n",
       "184                 {1535.0: 1, -99.0: 2, 5400.0: 1}       True  \n",
       "187               {-99.0: 2, 14427.0: 1, 17985.0: 1}       True  \n",
       "277                {4488.0: 12, -99.0: 1, 4412.0: 1}       True  \n",
       "355                 {-99.0: 2, 7570.0: 1, 7162.0: 1}       True  \n",
       "...                                              ...        ...  \n",
       "7875  {15302.0: 1, 14320.0: 1, 10896.0: 1, -99.0: 1}       True  \n",
       "7916               {10505.0: 1, -99.0: 1, 2904.0: 1}       True  \n",
       "7941               {-99.0: 2, 3942.0: 1, 10199.0: 1}       True  \n",
       "7942               {-99.0: 2, 12724.0: 1, 8154.0: 1}       True  \n",
       "8140               {-99.0: 2, 7344.0: 1, 12000.0: 1}       True  \n",
       "8466                  {-99.0: 1, 20.0: 1, 2424.0: 1}       True  \n",
       "8709               {18007.0: 1, -99.0: 1, 8425.0: 1}       True  \n",
       "8923                {6992.0: 1, -99.0: 2, 8607.0: 1}       True  \n",
       "8943               {8808.0: 1, 17139.0: 2, -99.0: 1}       True  \n",
       "9373               {21020.0: 1, 9208.0: 1, -99.0: 1}       True  \n",
       "\n",
       "[176 rows x 5 columns]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#se limiter aux noeuds présentant 3 lignes (pour rappel, seul une ligne présente un trafic inconnu)\n",
    "noeud_grp.loc[noeud_grp['estimable']].noeud.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>tmjo_2_sens</th>\n",
       "      <th>cat_rhv</th>\n",
       "      <th>nb_nan</th>\n",
       "      <th>estimable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [noeud, tmjo_2_sens, cat_rhv, nb_nan, estimable]\n",
       "Index: []"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noeud_grp.loc[(noeud_grp['estimable']) & \n",
    "             (noeud_grp.apply(lambda x : len(x['tmjo_2_sens']) - x['nb_nan'][-99]==1, axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mise à jour du fichier source\n",
    "gdf_rhv_extrapol.update(trafc_fin.set_index('idtronc').rename(columns={'tmjo_2_sens_extrapol':'tmjo_2_sens'})[['tmjo_2_sens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble de noeud\n",
    "gdf_rhv_extrapol=gdf_rhv_cpt_perm_123.reset_index().set_index('idtronc')\n",
    "for noeud_fv in liste_noeud_a_traiter :\n",
    "    print(noeud_fv)\n",
    "    #trouver noeud correspondant\n",
    "    try :\n",
    "        noeud_mmm_corrsp=appariement_noeud_mmm_fv.loc[appariement_noeud_mmm_fv['id_fv']==noeud_fv].id_mmm.values[0]\n",
    "    except IndexError : \n",
    "        continue        \n",
    "    #tableau de correspondance entre les voies du mmm\n",
    "    matrice_voie_mmm=rt.corresp_noeud_mmm(mmm_simple, noeud_mmm_corrsp)\n",
    "    ##tableau de correspondance entre les voies du rhv\n",
    "    matrice_voie_rhv=rt.corresp_noeud_rhv(gdf_rhv_cpt_perm_123,noeud_fv)\n",
    "    #jointure avec les voies du rhv\n",
    "    corresp_mm_rhv=matrice_voie_mmm.merge(cle_mmm_rhv[['NO','ident']], left_on='NO_x', right_on='NO').drop_duplicates().rename(\n",
    "        columns={'ident':'ident_x'}).drop('NO', axis=1).merge(cle_mmm_rhv[['NO','ident']], left_on='NO_y', right_on='NO').drop_duplicates().rename(\n",
    "        columns={'ident':'ident_y'}).drop('NO', axis=1)\n",
    "    #on ne garde que les voies d rhv présentent dans le table de correspondance\n",
    "    corresp_mm_rhv_fin=corresp_mm_rhv.loc[(corresp_mm_rhv.ident_x.isin(matrice_voie_rhv.ident_x.tolist()+matrice_voie_rhv.ident_y.tolist())) & \n",
    "                      (corresp_mm_rhv.ident_y.isin(matrice_voie_rhv.ident_x.tolist()+matrice_voie_rhv.ident_y.tolist()))]\n",
    "    #on joint les trafics\n",
    "    trafic_inconnus=corresp_mm_rhv_fin.merge(gdf_rhv_cpt_perm_123[['tmjo_2_sens', 'cat_rhv']], left_on='ident_x', right_index=True).merge(\n",
    "        gdf_rhv_cpt_perm_123[['tmjo_2_sens','cat_rhv']], left_on='ident_y', right_index=True)\n",
    "    #si les trafics inconnus sont vides on continu, on traitera apres\n",
    "    if trafic_inconnus.empty : \n",
    "        continue\n",
    "    #fonction de deterination des lignes a remplir\n",
    "    trafic_a_rens=trafic_inconnus.loc[((trafic_inconnus.tmjo_2_sens_x.isna() & trafic_inconnus.tmjo_2_sens_y.notna()) |\n",
    "    (trafic_inconnus.tmjo_2_sens_y.isna() & trafic_inconnus.tmjo_2_sens_x.notna()))].copy()\n",
    "    if trafic_a_rens.empty : \n",
    "        continue\n",
    "    #calcul des trafics\n",
    "    trafic_a_rens['ident_a_rens']=trafic_a_rens.apply(lambda x : x['ident_x'] if pd.isnull(x['tmjo_2_sens_x'])\n",
    "                                                      else x['ident_y'], axis=1)\n",
    "    trafic_a_rens['tmjo_2_sens_extrapol']=trafic_a_rens.apply(lambda x : \n",
    "        int(x['tmja_tv_y']/x['tmja_tv_x']*x['tmjo_2_sens_x']) if pd.isnull(x['tmjo_2_sens_y']) else int(x['tmja_tv_x']/x['tmja_tv_y']*x['tmjo_2_sens_y']),\n",
    "                                                      axis=1)\n",
    "    #filtre des resultats selon la cat_rhv : on prend la plus proche (a faire, pour le moment on prend celle qui est égale)\n",
    "    trafic_extrapol=trafic_a_rens.loc[trafic_a_rens['cat_rhv_x']==trafic_a_rens['cat_rhv_y']].copy()\n",
    "    #lien avec l'idtronc concerne pour affectation\n",
    "    trafic_extrapol=trafic_extrapol.merge(gdf_rhv_cpt_perm_123[['idtronc']], left_on='ident_a_rens', \n",
    "                                          right_index=True)[['ident_a_rens','tmjo_2_sens_extrapol','idtronc']]\n",
    "    #on ne gargee que le trafic max en cas de doublons\n",
    "    trafic_extrapol=trafic_extrapol.loc[trafic_extrapol['tmjo_2_sens_extrapol']==trafic_extrapol.groupby('idtronc').tmjo_2_sens_extrapol.transform(max)]\n",
    "    #mise à jour du fichier source\n",
    "\n",
    "    gdf_rhv_extrapol.update(trafic_extrapol.set_index('idtronc').rename(columns={'tmjo_2_sens_extrapol':'tmjo_2_sens'})[['tmjo_2_sens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_extrapol.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_extrapol.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "      <th>ident_x</th>\n",
       "      <th>ident_y</th>\n",
       "      <th>tmjo_2_sens_x</th>\n",
       "      <th>cat_rhv_x</th>\n",
       "      <th>tmjo_2_sens_y</th>\n",
       "      <th>cat_rhv_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6135635.0</td>\n",
       "      <td>1034980026</td>\n",
       "      <td>5948.72</td>\n",
       "      <td>1034980782</td>\n",
       "      <td>23295.99</td>\n",
       "      <td>8782</td>\n",
       "      <td>28019</td>\n",
       "      <td>7787.0</td>\n",
       "      <td>2</td>\n",
       "      <td>38095.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       noeud        NO_x  tmja_tv_x        NO_y  tmja_tv_y ident_x ident_y  \\\n",
       "1  6135635.0  1034980026    5948.72  1034980782   23295.99    8782   28019   \n",
       "\n",
       "   tmjo_2_sens_x cat_rhv_x  tmjo_2_sens_y cat_rhv_y  \n",
       "1         7787.0         2        38095.0         1  "
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trafic_inconnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>ident_x</th>\n",
       "      <th>tmjo_2_sens_x</th>\n",
       "      <th>ident_y</th>\n",
       "      <th>tmjo_2_sens_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>845</td>\n",
       "      <td>49936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2830</td>\n",
       "      <td>5133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>845</td>\n",
       "      <td>49936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50641</td>\n",
       "      <td>14216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>845</td>\n",
       "      <td>2830</td>\n",
       "      <td>5133.0</td>\n",
       "      <td>50641</td>\n",
       "      <td>14216.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noeud ident_x  tmjo_2_sens_x ident_y  tmjo_2_sens_y\n",
       "1    845   49936            NaN    2830         5133.0\n",
       "2    845   49936            NaN   50641        14216.0\n",
       "5    845    2830         5133.0   50641        14216.0"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrice_voie_rhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       noeud      NO_x  tmja_tv_x        NO_y  tmja_tv_y\n",
       "1  4722269.0  57894565    1906.02    57931055    7814.92\n",
       "2  4722269.0  57894565    1906.02  2145032626    9646.26\n",
       "5  4722269.0  57931055    7814.92  2145032626    9646.26"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrice_voie_mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "      <th>ident_x</th>\n",
       "      <th>ident_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "      <td>None</td>\n",
       "      <td>49936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "      <td>None</td>\n",
       "      <td>49936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       noeud      NO_x  tmja_tv_x        NO_y  tmja_tv_y ident_x ident_y\n",
       "0  4722269.0  57894565    1906.02    57931055    7814.92    None    None\n",
       "1  4722269.0  57894565    1906.02  2145032626    9646.26    None   49936\n",
       "2  4722269.0  57931055    7814.92  2145032626    9646.26    None   49936"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corresp_mm_rhv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filaire CBS\n",
    "En premier lieu il faut mettre à jour les attributs en renomant de façon explicite puis en faisant la somme des 2 sens de circulation pour les voies doucble sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filaire_cbs=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\CBS2017\\Filaire_global_metropole_DENSIFIER.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filaire_cbs['tmja']=filaire_cbs.apply(lambda x : (float(x['MT'].replace(',','.'))*12)+\n",
    "                                      (float(x['ME'].replace(',','.'))*4)+(float(x['MN'].replace(',','.'))*8),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filaire_cbs.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\filaire_tmja.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver les noeuds du troncon concerne\n",
    "toto=rt.Troncon(gdf_rhv_cpt_perm_123,934)\n",
    "noeud_uniq=toto.noeuds_uniques\n",
    "\n",
    "#si plus de 2 noeuds uniques, il faut regrouper\n",
    "df_noeud_uniq=graph_filaire_123_vertex.loc[(graph_filaire_123_vertex['id'].isin(noeud_uniq))]\n",
    "corresp_noeud_uniq=plus_proche_voisin(df_noeud_uniq, df_noeud_uniq, 100, 'id', 'id', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9383</td>\n",
       "      <td>4194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4194</td>\n",
       "      <td>9383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1034</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1031</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_left  id_right\n",
       "1     9383      4194\n",
       "2     4194      9383\n",
       "5     1034      1031\n",
       "6     1031      1034"
      ]
     },
     "execution_count": 1205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corresp_noeud_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver tout les noeud si 2*2 voies : \n",
    "noeud=1034\n",
    "#trouver le noeud supplementaire si besoin : \n",
    "list_tronc_noeud=gdf_rhv_cpt_perm_123.loc[(gdf_rhv_cpt_perm_123['source']==noeud) | (gdf_rhv_cpt_perm_123['target']==noeud)].copy().idtronc.tolist()\n",
    "list_noeud_sup=[]\n",
    "for t in list_tronc_noeud : \n",
    "    noeud_troncon=gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['idtronc']==t]\n",
    "    noeud_uniq=[k for k,v in Counter(noeud_troncon.source.tolist()+noeud_troncon.target.tolist()).items() if v==1]\n",
    "    if len(noeud_uniq)>2 : \n",
    "        df_noeud_uniq=graph_filaire_123_vertex.loc[(graph_filaire_123_vertex['id'].isin(noeud_uniq))]\n",
    "        corresp_noeud_uniq=plus_proche_voisin(df_noeud_uniq, df_noeud_uniq, 30, 'id', 'id', True)\n",
    "        list_noeud_sup+=corresp_noeud_uniq.loc[corresp_noeud_uniq['id_left']==noeud].id_right.tolist()\n",
    "list_noeud_sup=list(set(list_noeud_sup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_noeud_sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
