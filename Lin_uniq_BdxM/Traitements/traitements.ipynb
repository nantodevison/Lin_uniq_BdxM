{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LINEARISATION UNIQUE DES TRAFICS SUR BORDEAUX METROPOLE**\n",
    "> Traitements à mettre en oeuvre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys #c'est pas propre mais pour le moment pour importer mes modules perso dans le notebook je ne sais pas faire\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Lin_uniq_BdxM\\Lin_uniq_BdxM\\Traitements')\n",
    "import Connexion_Transfert as ct\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint, Polygon, box\n",
    "import re\n",
    "from difflib import get_close_matches,SequenceMatcher\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from Outils import creer_graph, plus_proche_voisin,nb_noeud_unique_troncon_continu\n",
    "\n",
    "from Repartition_trafic import corresp_noeud_mmm,corresp_noeud_rhv,appariement_noeud_mmm_fv,noeud_fv_ligne_ss_trafic\n",
    "from Simplifier_Rdpt import creer_dico_noeud_rdpt,simplifier_noeud_rdpt,maj_graph_rdpt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "from Base_BdTopo.Import_outils import *\n",
    "from Base_BdTopo.Rond_points import *\n",
    "from Base_BdTopo.Regroupement_correspondance import *\n",
    "from Base_BdTopo.Troncon_elementaire import *\n",
    "from Base_BdTopo.Troncon_base import *\n",
    "from Base_BdTopo.Gestion_2_chaussee import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Regrouper les troncons du filaire de voie\n",
    ">Les troncons du filaire de voie sont regroupés d'abord uniquement avec les troncons de cat_rhv égal à 1,2,3,61,62,63, pour pouvoir réaliser des troncon de trafic homogènes tous reliés entre eux.\n",
    "<br> Ensuite, un deuxième regroupement comprenant l'ensemble des troncons de cat_rhv permetd'affiner le diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1.1 Import du fichier final\n",
    "le fichier shape est vérifié à la main car l'affectation automatique n'est pas 100% fiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour info : lire le graph issu de la methode decrite dessous\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    graph_filaire_123 = gp.read_postgis('select * from linearisation_bm.graph_rhv_123', c.connexionPsy)\n",
    "    graph_filaire_123_vertex = gp.read_postgis('select id,cnt,chk,ein,eout,the_geom as geom from linearisation_bm.graph_rhv_123_vertices_pgr', c.connexionPsy)\n",
    "    graph_filaire = gp.read_postgis('select * from linearisation_bm.graph_rhv_complet', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_groupe_123=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_groupe_123_v2.shp').merge(\n",
    "    graph_filaire_123[['ident','source','target']], on='ident')\n",
    "gdf_rhv_groupe=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\rhv_grp.shp').merge(\n",
    "    graph_filaire[['ident','source','target']], on='ident')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 1.2 Méthode\n",
    " pour information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import du fichier de filaire de base\n",
    "gdf_rhv=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\Filaire_voie\\FV_TRONC_L.shp')\n",
    "# 2. mise en forme\n",
    "gdf_rhv.columns=[a.lower() for a in gdf_rhv.columns] #nom de colonne en minuscule\n",
    "gdf_rhv.rename(columns={'gid':'id','nom_voie' : 'numero'},inplace=True)\n",
    "gdf_rhv=gdf_rhv.loc[~gdf_rhv.id.isna()].copy()\n",
    "gdf_rhv['id_ign']=gdf_rhv.ident.apply(lambda x : 'TRONROUT'+str(x))\n",
    "gdf_rhv=gdf_rhv.loc[~gdf_rhv.cat_rhv.isin(['98','99'])].copy()\n",
    "gdf_rhv['nature']=gdf_rhv.apply(lambda x : 'Route à 2 chaussées' if x['rgraph_dbl']==0 else 'Route à 1 chaussée', axis=1)\n",
    "gdf_rhv['nature']=gdf_rhv.apply(lambda x : 'Bretelle' if x['cat_dig']=='8' else x['nature'], axis=1)\n",
    "gdf_rhv['numero']=gdf_rhv.apply(lambda x : 'Bretelle '+x['numero'] if x['cat_dig']=='8' else x['numero'], axis=1)\n",
    "gdf_rhv['sens']=gdf_rhv.apply(lambda x : 'Direct' if x['rgraph_dbl']==0 else 'Double', axis=1)\n",
    "gdf_rhv['codevoie_d']='NR'\n",
    "gdf_rhv['importance']=gdf_rhv['cat_dig']\n",
    "gdf_rhv['id']=gdf_rhv.id.apply(lambda x : int(x))\n",
    "#gdf_rhv.loc[gdf_rhv.nom_voie.isna()] #verif des nom_voie null : 2 lignes, cf traitements plus loins\n",
    "#filtrer les voies cylcables et autres\n",
    "gdf_rhv_filtre=gdf_rhv.loc[~gdf_rhv.cat_dig.isin(['6','7','9','10'])].copy()\n",
    "\n",
    "# 3. creer le graph en bdd\n",
    "#creer_graph(gdf_rhv_filtre.loc[gdf_rhv_filtre.cat_rhv.isin(['1','2','3','33',61','62','63'])],\n",
    "            #'local_otv',schema='linearisation_bm',table='graph_rhv_123',table_vertex='graph_rhv_123_vertices_pgr')\n",
    "creer_graph(gdf_rhv_filtre,'local_otv',schema='linearisation_bm',table='graph_rhv_complet',table_vertex='graph_rhv_complet_vertices_pgr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "# 4. initialiser les paramètre de regroupement\n",
    "df=import_donnes_base('local_otv','linearisation_bm', 'graph_rhv_complet','graph_rhv_complet_vertices_pgr')\n",
    "df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=identifier_rd_pt(df)\n",
    "df_lignes=df_avec_rd_pt.set_index('id_ign')#mettre l'id_ign en index\n",
    "bretelle=df_avec_rd_pt.loc[df_avec_rd_pt['nature']=='Bretelle'].copy()\n",
    "bretelle_tri=bretelle.loc[bretelle.length.sort_values(ascending=False).index.tolist()].id_ign.tolist()\n",
    "sans_bretelle=df_avec_rd_pt.loc[df_avec_rd_pt['nature']!='Bretelle'].copy()\n",
    "list_sans_bretelle=sans_bretelle.id_ign.tolist()\n",
    "list_tri_longueur=sans_bretelle.loc[sans_bretelle.length.sort_values(ascending=False).index.tolist()].id_ign.tolist()\n",
    "list_id_ign=bretelle_tri+list_tri_longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. appel fonction de grouepement pour creer des idtronc pour uniquement voies de 1 à 3 et d'un autre coté pour toute les voies\n",
    "df_affectation, dico_erreur, lignes_traitees, lignes_non_traitees=regrouper_troncon(\n",
    "    list_id_ign, df_avec_rd_pt, carac_rd_pt,df2_chaussees,[])\n",
    "\n",
    "# 6. mise en forme et export\n",
    "gdf_rhv_groupe=gdf_rhv_filtre.merge(df_affectation, left_on='id_ign', right_on='id', how='left')\n",
    "gdf_rhv_groupe.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\rhv_grp_v2.shp')\n",
    "#gdf_rhv_groupe_123=gdf_rhv_filtre.merge(df_affectation, left_on='id_ign', right_on='id', how='left')\n",
    "#gdf_rhv_groupe_123.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_groupe_123.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2019-12-12 11:48:54.415530 nb lignes traitees : 0\n",
      "pas de parrallele trouvee pour les troncons ['TRONROUT9919']\n"
     ]
    }
   ],
   "source": [
    "#pour test sur une ligne\n",
    "df_affectation2, dico_erreur2, lignes_traitees2, lignes_non_traitees2=regrouper_troncon(\n",
    "    ['TRONROUT9919'], df_avec_rd_pt, carac_rd_pt,df2_chaussees,[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MISE EN FORME DES COMPTAGES PONCTUELS\n",
    "Pour ces comptages l'idée c'est de réussir à regrouper les comptages par un idtronc issu de la detection des troncon elementaire. ça a du sens car pour un comptage ponctuel normalement les 2 sens de cicru sont sur un troncon unique. DE plus, les ens circu, noms de voie et autres attributs permettant de regrouper les comptages sont de qualité trop variable pour s'appuyer uniquement dessus. et enfin, peu de 2*2 vois sont meusrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des données\n",
    "cpt_pct=pd.read_excel(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\base_comptages_routiers_v4.xlsx')\n",
    "gdf_rhv_groupe['geom_src']=gdf_rhv_groupe.geometry\n",
    "cpt_pct.columns=[a.lower() for a in cpt_pct.columns]\n",
    "cpt_pct = gp.GeoDataFrame(cpt_pct, geometry=gp.points_from_xy(cpt_pct.longitude, cpt_pct.latitude))\n",
    "cpt_pct.crs = {'init' :'epsg:4326'}\n",
    "cpt_pct_l93=cpt_pct.to_crs({'init': 'epsg:2154'})\n",
    "cpt_pct_l93['x_l93']=cpt_pct_l93.geometry.apply(lambda x : x.x)\n",
    "cpt_pct_l93['y_l93']=cpt_pct_l93.geometry.apply(lambda x : x.y)\n",
    "\n",
    "#mise en forme\n",
    "cpt_pct_l93['nom_voie']=cpt_pct_l93.nom_voie.apply(lambda x : re.sub(('é|è|ê'),'e',x.lower().strip()))\n",
    "cpt_pct_l93['sens_unique']=cpt_pct_l93.sens_circulation.apply(lambda x : True if SequenceMatcher(None,' '.join(x.split(' ')[:2]).lower(),'sens unique').ratio()>0.8 else False)\n",
    "cpt_pct_l93['type_voie']=cpt_pct_l93.nom_voie.apply(lambda x : x.split(' ')[0])\n",
    "cpt_pct_l93['suffix_nom_voie']=cpt_pct_l93.nom_voie.apply(lambda x : ' '.join(x.split(' ')[1:]).lower())\n",
    "cpt_pct_l93['date_max_cptg']=cpt_pct_l93.observation.apply(lambda x : pd.to_datetime(x.split(' au ')[1],dayfirst=True))\n",
    "cpt_pct_l93['sens_circulation']=cpt_pct_l93.sens_circulation.apply(lambda x : re.sub(('é|è|ê'),'e',x.strip().lower()))\n",
    "cpt_pct_l93['observation']=cpt_pct_l93.observation.apply(lambda x : x.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster à 200m\n",
    "data_test_clust=[[x, y] for x, y in zip(cpt_pct_l93.x_l93.tolist(), cpt_pct_l93.y_l93.tolist())]\n",
    "db = DBSCAN(eps=200, min_samples=2).fit(data_test_clust)\n",
    "labels = db.labels_\n",
    "cpt_pct_l93['n_cluster']=labels\n",
    "\n",
    "#voies avec nom_proches\n",
    "cross_join_ncluster=cpt_pct_l93[['ident','nom_voie','type_voie','suffix_nom_voie', 'sens_circulation','annee','n_cluster']].merge(\n",
    "   cpt_pct_l93[['ident','nom_voie','type_voie','suffix_nom_voie', 'sens_circulation','annee','n_cluster']], on='n_cluster') #avoir toutes les relations internoms possibles\n",
    "cross_join_ncluster['comp_nom_voie']=cross_join_ncluster.apply(lambda x : SequenceMatcher(None,x['suffix_nom_voie_x'], x['suffix_nom_voie_y']).ratio(), axis=1)#affecter une note a cahque relation\n",
    "voie_nom_proches=cross_join_ncluster.loc[(cross_join_ncluster['comp_nom_voie']>0.85) & (cross_join_ncluster['type_voie_x']==cross_join_ncluster['type_voie_y'])\n",
    "                        ].sort_values(['n_cluster','ident_x'])#ne conserver qque les relations bien notees\n",
    "voie_nom_proches['id_grp_nom_voie']=voie_nom_proches.ident_x.rank(method='dense')#ajouter un id \n",
    "corresp_voies=voie_nom_proches.drop_duplicates('ident_y')\n",
    "grp_nom_voie=cpt_pct_l93.merge(corresp_voies[['ident_y','id_grp_nom_voie']].rename(columns={'ident_y':'ident'}), on='ident')#jointure sur id de depart\n",
    "\n",
    "#grouper par periode\n",
    "grp_period=grp_nom_voie.copy()\n",
    "grp_period['date_max_cptg']=grp_period.observation.apply(lambda x : pd.to_datetime(x.split(' au ')[1],dayfirst=True))\n",
    "grp_period['id_period']=grp_period.date_max_cptg.rank(method='dense')\n",
    "grp_period['indicefinal']=grp_period.n_cluster+(grp_period.id_grp_nom_voie*(1000))+(grp_period.id_period*10000)\n",
    "grp_period[[a for a in grp_period.columns if a!='date_max_cptg']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cluster_comptg_pctl_grp_period.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#regarder le nb de ligne relative à chaque comptage : siune ligne est relative à 2 comptage qui ont le mm indice final\n",
    "#i.e relatif à la mme rue, à la mm periode, sur des sens différents, on fait la somme\n",
    "#si il y a plus de 2 comptages : il faut récupérer les plus recents : si 2 on fait la somme, si 1 on prends la valeur\n",
    "\n",
    "#rappatrier le numero d'id_tronc pour chaque comptage\n",
    "#trouver la distance min à chaque objet ligne\"du rhv groupe\n",
    "grp_troncon_temp=cpt_pct_l93.copy()\n",
    "grp_troncon_temp.geometry=grp_troncon_temp.buffer(20)#passer la geom en buffer\n",
    "intersct_buff_20=gp.sjoin(grp_troncon_temp,gdf_rhv_groupe,how='left',op='intersects')\n",
    "intersct_buff_20.geometry=grp_period.geometry#repasser la geom en point\n",
    "intersct_buff_20=intersct_buff_20.merge(gdf_rhv_groupe[['ident','geometry']], left_on='ident_right', right_on='ident')\n",
    "intersct_buff_20['dist_pt_ligne']=intersct_buff_20.apply(lambda x : x['geometry_x'].distance(x['geometry_y']), axis=1) #définir la disance entre les points et ligne\n",
    "joint_dist_min=intersct_buff_20.loc[intersct_buff_20.groupby('ident_left')['dist_pt_ligne'].transform(min)==intersct_buff_20['dist_pt_ligne']][['ident_left','idtronc','numero']].rename(\n",
    "    columns={'ident_left':'ident'}).copy()#ne garder que la ligne la plus proche\n",
    "grp_troncon=grp_period.merge(joint_dist_min, on='ident',how='left')#df finale\n",
    "grp_troncon[[a for a in grp_troncon.columns if a!='date_max_cptg']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cluster_comptg_pctl_grp_troncon.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annee la plus recente\n",
    "anne_recente=grp_troncon.loc[grp_troncon.groupby('idtronc')['date_max_cptg'].transform(max)==grp_troncon['date_max_cptg']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idtronc ok (avec 1 ou 2 pt de comptage)\n",
    "idtronc_grp=anne_recente.groupby('idtronc').ident.count()\n",
    "idtroncOkTmjo=anne_recente.loc[anne_recente.idtronc.isin(idtronc_grp.loc[idtronc_grp<3].index.tolist())][['ident','idtronc','sens_circulation','tmjo_tv']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isoler les idtronc supportant + de 2 pt de comptages\n",
    "idtronc_sup2=idtronc_grp.loc[idtronc_grp>2].copy()\n",
    "#trouver les points correspondants\n",
    "pt_sup2=anne_recente.loc[anne_recente['idtronc'].isin(idtronc_sup2.index.tolist())].copy()\n",
    "#ajouter attribut qui traduit le nb de valeurs différentes de sens circulation\n",
    "def nb_sens_circu(idtronc) :\n",
    "    return len(pt_sup2.loc[pt_sup2['idtronc']==idtronc].sens_circulation.unique())  \n",
    "pt_sup2['nb_sens_circu']=pt_sup2.apply(lambda x : nb_sens_circu(x['idtronc']), axis=1)\n",
    "#pour les points a 2 sens de circu : on prend la valeur max des 2 : \n",
    "ptSup2SensCircu2=pt_sup2.loc[pt_sup2['nb_sens_circu']==2].copy()\n",
    "ptSup2SensCircu2.drop_duplicates(['nom_voie','sens_circulation','tmjo_tv','observation'],inplace=True)#qq points ont des ident différents mais sont les mêmes\n",
    "ptSup2SensCircu2.drop_duplicates(['sens_circulation','tmjo_tv','observation'],inplace=True)#qla mm que la précédente, mais je ne sais pas pourquoi l'ajout de nom_voie fait bugger le drop duplicates pour les ident  716,717,975,976\n",
    "ptSup2SensCircu2OkTmjo=ptSup2SensCircu2.groupby(['idtronc','sens_circulation'])['tmjo_tv'].max().reset_index().merge(\n",
    "    ptSup2SensCircu2[['idtronc','sens_circulation','tmjo_tv','ident']], on=['idtronc','sens_circulation','tmjo_tv'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour les autres point\n",
    "ptSup2SensCircuSup2=pt_sup2.loc[pt_sup2['nb_sens_circu']>2][['ident','idtronc','sens_circulation','tmjo_tv','indicefinal']].sort_values(['idtronc','indicefinal','sens_circulation']).copy()\n",
    "\n",
    "#filtre des points dont le sens circul est le mm ou qui sont isole\n",
    "list_ident, list_idtronc, list_indicefinal, list_senscircu=(ptSup2SensCircuSup2.ident.tolist(),ptSup2SensCircuSup2.idtronc.tolist(),\n",
    "                                                            ptSup2SensCircuSup2.indicefinal.tolist(),ptSup2SensCircuSup2.sens_circulation.tolist())\n",
    "list_new_ident=list_ident\n",
    "for i in range(len(list_ident)) : \n",
    "    if i<len(list_ident)-1 :\n",
    "        if list_idtronc[i+1]==list_idtronc[i] : \n",
    "            if list_indicefinal[i+1]==list_indicefinal[i] :\n",
    "                if SequenceMatcher(None,list_senscircu[i+1], list_senscircu[i]).ratio()>0.75 : \n",
    "                    list_new_ident[i+1]=list_ident[i]\n",
    "    else : \n",
    "        if list_idtronc[i]==list_idtronc[i-1] : \n",
    "            if list_indicefinal[i]==list_indicefinal[i-1] :\n",
    "                if SequenceMatcher(None,list_senscircu[i], list_senscircu[i-1]).ratio()>0.75 : \n",
    "                    list_new_ident[i]=list_ident[i-1]\n",
    "ptSup2SensCircuSup2['ident_final']=np.array(list_ident)\n",
    "#filtrer les points isoles\n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2.groupby(['idtronc','indicefinal']).nunique()[['ident']].reset_index()\n",
    "ptSup2SensCircuSup2=ptSup2SensCircuSup2.loc[ptSup2SensCircuSup2.indicefinal.isin(ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre['ident']>1].indicefinal.\n",
    "                                                                             tolist())].copy()\n",
    "#filtrer les points qui ont des nom des sens circu proches\n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2.loc[ptSup2SensCircuSup2['tmjo_tv']==\n",
    "                                                          ptSup2SensCircuSup2.groupby('ident_final')['tmjo_tv'].transform(max)].copy()\n",
    "#filtrer les points qui sont égaux \n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre['ident']==ptSup2SensCircuSup2_filtre['ident_final']].copy()\n",
    "#filtrer les points qui présente toujours plus de 2 identifiant (i.e pb denomination ou pb référentiel ou pb tronc_elementaire)\n",
    "grp=ptSup2SensCircuSup2_filtre.groupby('idtronc').nunique()[['ident']].reset_index()\n",
    "pt_non_affectes=grp.loc[grp['ident']>2]\n",
    "ptSup2SensCircuSup2Oktmjo=ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre.idtronc.isin(grp.loc[grp['ident']<3].idtronc.tolist())][['ident','idtronc','sens_circulation','tmjo_tv']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_finale=pd.concat([ptSup2SensCircuSup2Oktmjo,idtroncOkTmjo,ptSup2SensCircu2OkTmjo],axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qq modifs manuelle pour grouper par paire les comptages ponctuels: \n",
    "affect_finale.loc[affect_finale['ident']==606,'idtronc']=affect_finale.loc[affect_finale['ident']==605].idtronc.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>idtronc</th>\n",
       "      <th>sens_circulation</th>\n",
       "      <th>tmjo_tv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>1502</td>\n",
       "      <td>10986.0</td>\n",
       "      <td>vers l'avenue du marechal leclerc</td>\n",
       "      <td>6703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ident  idtronc                   sens_circulation  tmjo_tv\n",
       "1501   1502  10986.0  vers l'avenue du marechal leclerc     6703"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "affect_finale.loc[affect_finale['ident']==1502]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MISE EN FORME DES COMPTAGES PERMANENTS\n",
    "Pour ces compteurs on va regrouper en séparant les siredo des boucles liées au système gertrude. Pour les siredo pas de pb, pour les autre sc'est plus compliqué car les références de localiusation varient parfois. Il y a donc une phase de regroupement manuel à la fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Import du fichier final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fichier corrige_final\n",
    "cpt_perm_final=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cpt_perm_groupe.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Méthode\n",
    "Pour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer un gdf avec la fusion des points et des données de comptages non nulles\n",
    "cpt_brut=pd.read_excel(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\comptage_trafic_2017.xlsx')\n",
    "cpt_brut.columns=[a.lower() for a in cpt_brut.columns]\n",
    "cpt_brut=cpt_brut.loc[~cpt_brut.mjo_val.isna()].copy()\n",
    "pt_cpt_perm=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\PC_CAPTE_P.shp')\n",
    "pt_cpt_perm.columns=[a.lower() for a in pt_cpt_perm.columns]\n",
    "pt_cpt_perm=pt_cpt_perm.merge(cpt_brut, on='ident')\n",
    "pt_cpt_perm['nom_voie']=pt_cpt_perm.nom_voie.apply(lambda x : re.sub(('é|è|ê'),'e',x.lower().replace('  ',' ')))#mise en forme nom de voie\n",
    "\n",
    "#regrouper les comptages\n",
    "#un compteur peut caracteriser 1 ligne, ou 2 compteur 1 ligne ou 2 compteurs 2 lignes, dc les compteurs ne vont pas toujours par deux\n",
    "\n",
    "#regrouper geographiquement\n",
    "pt_cpt_perm['x_l93']=pt_cpt_perm.geometry.apply(lambda x : x.x)\n",
    "pt_cpt_perm['y_l93']=pt_cpt_perm.geometry.apply(lambda x : x.y)\n",
    "data_test_clust=[[x, y] for x, y in zip(pt_cpt_perm.x_l93.tolist(), pt_cpt_perm.y_l93.tolist())]\n",
    "db = DBSCAN(eps=200, min_samples=2).fit(data_test_clust)\n",
    "labels = db.labels_\n",
    "pt_cpt_perm['n_cluster']=labels\n",
    "\n",
    "#correspondance nom_rue\n",
    "#analyser les noms de commune : \n",
    "#grp_nom_voie.nom_voie.apply(lambda x : x.split()[0]).unique()#liste des permiers mots\n",
    "#grp_nom_voie.loc[grp_nom_voie.nom_voie.apply(lambda x : x.split()[0]=='carbon')].nom_voie.unique() #test des valeusr\n",
    "dico_commune={'le bouscat','le haillan','bordeaux','pessac','talence', 'merignac','st medard','carbon blanc',\n",
    "       'begles', 'eysines','villenave d\\'ornon','bruges','gradignan','bassens','cenon','floirac','pessa','lormont', 'artigues'}\n",
    "dico_type_voie={'allee' : ['all.','allee', 'allees'],'avenue': ['av', 'av.', 'avenue'], 'boulevard' : ['blvd'],\n",
    "               'bretelle' : 'bretelle', 'cours' : ['crs','cours'], 'cote' : ['côte'], 'place':['pl'], 'route':['route', 'rte'],'rue':['rue'],\n",
    "               'voie':['voie']}\n",
    "def decoupe_nom_voie(nom_voie) : \n",
    "    for c in dico_commune : \n",
    "        if nom_voie[:len(c)]==c :\n",
    "            commune=nom_voie[:len(c)]\n",
    "            nom_voie=nom_voie[len(c):].strip()\n",
    "    for k, v in dico_type_voie.items() : \n",
    "        if any([a for a in nom_voie.split() if a in v])  : \n",
    "            caractere=[a for a in nom_voie.split() if a in v][0]\n",
    "            type_voie=k\n",
    "            nom_voie=' '.join([n for n in nom_voie.split() if n!=caractere])\n",
    "            break\n",
    "    else : type_voie=None\n",
    "    for test in ['avant','apres'] :\n",
    "        if test in nom_voie : \n",
    "            return nom_voie.split(test)[0].strip(), test,nom_voie.split(test)[1].strip(), commune, type_voie\n",
    "    else : return nom_voie, nom_voie,nom_voie,commune,type_voie\n",
    "\n",
    "pt_cpt_perm['nom_rue']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[0])\n",
    "pt_cpt_perm['localisant']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[1])\n",
    "pt_cpt_perm['rue_refer']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[2])\n",
    "pt_cpt_perm['commune']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[3])\n",
    "pt_cpt_perm['type_voie']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[4])\n",
    "pt_cpt_perm.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\pt_cpt_perm.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>#### 2.2.1 SIREDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siredo=pt_cpt_perm.loc[pt_cpt_perm['type']=='SIREDO'].copy()#isoler siredo\n",
    "cross_join=siredo.assign(key=1).merge(siredo.assign(key=1), on='key').drop('key',axis=1)#calcul des distances\n",
    "cross_join['distance']=cross_join.apply(lambda x : x['geometry_x'].distance(x['geometry_y']),axis=1)\n",
    "cross_join_filtre=cross_join[[c for c in cross_join.columns if c[-2:]=='_x']+['distance','ident_y']].rename(columns=\n",
    "              {c : c[:-2] for c in cross_join.columns if c[-2:]=='_x'}).copy()#filtre des attribut\n",
    "cross_join_filtre=cross_join_filtre.loc[(cross_join_filtre['ident']!=cross_join_filtre['ident_y'])].copy()\n",
    "siredo_proches=cross_join_filtre.loc[(cross_join_filtre.groupby('ident')['distance'].transform(min)==\n",
    "                                      cross_join_filtre['distance'])].copy()#plus proche voisin\n",
    "siredo_proches['id_grpsiredo']=siredo_proches.reset_index().index\n",
    "id_siredo_proches=(pd.concat([siredo_proches[['ident','ident_y', 'id_grpsiredo']],\n",
    "           siredo_proches[['ident_y','ident', 'id_grpsiredo']].rename(columns={\n",
    "    'ident_y':'ident', 'ident':'ident_y'})],sort=False).sort_values('ident').\n",
    "    reset_index().drop_duplicates('index').drop_duplicates(['ident','ident_y']).drop('index',axis=1))\n",
    "cle_assoc_siredo=pd.concat([id_siredo_proches[['ident','id_grpsiredo']],id_siredo_proches[['ident_y','id_grpsiredo']].rename(columns={\n",
    "    'ident_y':'ident'})]).sort_values('id_grpsiredo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>#### 2.2.2 Gertrude\n",
    "> au final ce n'est pas très efficace, et un renseignement des regroupement a la ain aurait été plus rapide. il sera important si de nouvelles stations sont posées de fournir de suite un identifiant de regroupement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gertrude=pt_cpt_perm.loc[pt_cpt_perm['type']=='BOUCLE'].copy()#isoler les boucles\n",
    "gertrude['x']=gertrude.geometry.apply(lambda x : x.x)\n",
    "gertrude['y']=gertrude.geometry.apply(lambda x : x.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si c'est sur le mm idtronc on regroupe\n",
    "\n",
    "#rappatrier le numero d'id_tronc pour chaque comptage\n",
    "#trouver la distance min à chaque objet ligne\"du rhv groupe\n",
    "grp_troncon_temp=gertrude.copy()\n",
    "grp_troncon_temp.geometry=grp_troncon_temp.buffer(20)#passer la geom en buffer\n",
    "intersct_buff_20=gp.sjoin(grp_troncon_temp,gdf_rhv_groupe,how='left',op='intersects')\n",
    "intersct_buff_20.geometry=gertrude.geometry#repasser la geom en point\n",
    "intersct_buff_20=intersct_buff_20.merge(gdf_rhv_groupe[['ident','geometry']], left_on='ident_right', right_on='ident')\n",
    "intersct_buff_20['dist_pt_ligne']=intersct_buff_20.apply(lambda x : x['geometry_x'].distance(x['geometry_y']), axis=1) #définir la disance entre les points et ligne\n",
    "joint_dist_min=intersct_buff_20.loc[intersct_buff_20.groupby('ident_left')['dist_pt_ligne'].transform(min)==intersct_buff_20['dist_pt_ligne']][['ident_left','idtronc','numero']].rename(\n",
    "    columns={'ident_left':'ident'}).copy()#ne garder que la ligne la plus proche\n",
    "grp_troncon=gertrude.merge(joint_dist_min, on='ident',how='left')#df finale\n",
    "\n",
    "#regrouper\n",
    "grp_idtronc=grp_troncon.groupby('idtronc')['ident'].agg(lambda x : tuple(x))\n",
    "grp_idtronc=grp_idtronc.loc[grp_idtronc.apply(lambda x : len(x)>1)].reset_index().copy()\n",
    "grp_idtronc['id_grp']=grp_idtronc.ident.rank()\n",
    "#mettre en forme\n",
    "grp_idtronc=pd.DataFrame([(a,g) for i,g in zip(grp_idtronc.ident.tolist(),grp_idtronc.id_grp.tolist()) for a in i], columns=['ident','id_grp'])\n",
    "grp_final=grp_idtronc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trouver les points qui restent et préparer un regrouepement par commune, nom de rue, distance et se,s\n",
    "\n",
    "#points qui restent\n",
    "gertrude_etape2=gertrude.loc[~gertrude['ident'].isin(grp_idtronc.ident.tolist())].copy()\n",
    "#comparer les noms\n",
    "cross_join=gertrude_etape2[['gid','geometry','ident','nom_voie', 'sens_cir','commune', 'nom_rue', 'localisant','rue_refer','type_voie']].assign(key=1).merge(\n",
    "    gertrude_etape2[['gid','geometry','ident','nom_voie', 'sens_cir','commune', 'nom_rue', 'localisant','rue_refer','type_voie']].assign(key=1), on='key').drop('key',axis=1)#calcul des distances\n",
    "cross_join['distance']=cross_join.apply(lambda x : x['geometry_x'].distance(x['geometry_y']),axis=1)\n",
    "cross_join['comp_nom_rue']=cross_join.apply(lambda x : SequenceMatcher(None,x['nom_rue_x'], x['nom_rue_y']).ratio(), axis=1)#affecter une note a cahque relation\n",
    "\n",
    "#vérifier que les sens de cicru sont cohérents et ne garder que les lignes avec un sens ok, un nom de voie proche et une distance proche\n",
    "gertrude_join=cross_join.loc[(cross_join['comp_nom_rue']>0.6) & (cross_join['distance']<1000) & (cross_join['commune_x']==cross_join['commune_y']) & \n",
    "                             (cross_join['type_voie_x']==cross_join['type_voie_y'])].copy()\n",
    "#on ajoute un attribut de verif du sens\n",
    "def verif_sens(s1,s2) : \n",
    "    for sens in [['Sens Sortant', 'Sens Entrant'],['Sens N > S', 'Sens S > N'], ['Sens Nord > Sud', 'Sens Sud > Nord'],['Nord > Sud', 'Sud > Nord'],\n",
    "                ['Boulevards Nord > Sud', 'Boulevards Sud > Nord']] :\n",
    "        if s1 in sens and s2 in sens : \n",
    "            if s1!=s2 : \n",
    "                return True\n",
    "        else : \n",
    "            continue\n",
    "    return False\n",
    "gertrude_join['valid_sens']=gertrude_join.apply(lambda x : verif_sens(x['sens_cir_x'],x['sens_cir_y']),axis=1)  \n",
    "gertrude_join['id_grp']=gertrude_join.sort_values('ident_x').ident_x.rank(method=\"min\")\n",
    "gertrude_grp=gertrude_join.loc[gertrude_join['valid_sens']][['ident_x','ident_y','distance', 'comp_nom_rue', 'valid_sens','id_grp']].sort_values('ident_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenir une liste des ident qui se regroupent\n",
    "list_id_grp=pd.concat([gertrude_grp[['ident_x','id_grp']],gertrude_grp[['ident_y','id_grp']].rename(columns={'ident_y' : 'ident_x'})]).drop_duplicates([\n",
    "    'ident_x','id_grp']).groupby('id_grp').ident_x.agg(lambda x : set(sorted(tuple(x)))).tolist()\n",
    "dico={}\n",
    "for e,c in enumerate(gertrude_grp.ident_x.tolist()) : \n",
    "    for l in list_id_grp : \n",
    "        if c in l :\n",
    "            #print('c dans l','c : ',c,'l :',l)\n",
    "            if c not in dico.keys() :\n",
    "                dico[c]=list(l)\n",
    "            else : \n",
    "                dico[c]+=list(l)\n",
    "list_id_grp_temp=set([ v for v in {k : tuple(set(sorted(dico[k]))) for k in dico.keys()}.values()])\n",
    "list_id_grp_final=set(v for v in {c :b for c in gertrude_grp.ident_x.tolist() \n",
    " for b in [a for a in list_id_grp_temp if c in a] \n",
    " if len(b) == max([len(c) for c in [a for a in list_id_grp_temp if c in a]])}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affecter un identifiant\n",
    "def groupe_pt_gertrude_etape2(df_local) : \n",
    "    \"\"\"\n",
    "    regrouper les points à partir d'une liste d'ident selon l'ordre d'apparition \n",
    "    \"\"\"\n",
    "    limite_box=MultiPoint([(a.x,a.y) for a in df_local.itertuples()]).bounds\n",
    "    plg_long, plg_larg=limite_box[2]-limite_box[0],limite_box[3]-limite_box[1]\n",
    "    if max(plg_long,plg_larg)==plg_long : \n",
    "        df_local['next_sens']=df_local.sort_values('x').sens_cir.shift(-1,fill_value='NC')\n",
    "        df_local['prev_sens']=df_local.sort_values('x').sens_cir.shift(1,fill_value='NC')\n",
    "    else : \n",
    "        df_local['next_sens']=df_local.sort_values('y').sens_cir.shift(-1,fill_value='NC')\n",
    "        df_local['prev_sens']=df_local.sort_values('y').sens_cir.shift(1,fill_value='NC')\n",
    "    df_local['sens_comp']=df_local.apply(lambda x : x.prev_sens if x.next_sens=='NC' else x.next_sens,axis=1)\n",
    "    df_local['verif_sens']=df_local.apply(lambda x : verif_sens(x['sens_cir'],x['sens_comp']),axis=1)\n",
    "    df_local['id_grp']=-99\n",
    "    i=0\n",
    "    while i < len(df_local)-1:\n",
    "        if df_local.iloc[i].verif_sens : \n",
    "            df_local.loc[df_local.index.isin(df_local.iloc[i:i+2].index.tolist()),'id_grp']=i\n",
    "            i+=2\n",
    "        else : \n",
    "            i+=1\n",
    "    return\n",
    "\n",
    "# à partior de la liste des ident groupé : \n",
    "grp_final2=grp_final.copy()\n",
    "# on selectionne ces points dans la df\n",
    "increment=grp_final2.id_grp.max()\n",
    "for l in list_id_grp_final : \n",
    "    #if 'Z8CT13' not in l : continue\n",
    "    df_local=gertrude_etape2.loc[gertrude_etape2.ident.isin(l)].copy()\n",
    "    if len(df_local)==1:\n",
    "        continue\n",
    "    if len(df_local)==2 : \n",
    "        if (gertrude_join.loc[(gertrude_join.ident_x.isin(l)) & (gertrude_join['ident_x'] != gertrude_join['ident_y'])].valid_sens).all() : \n",
    "            increment+=1\n",
    "            df_local['id_grp']=increment\n",
    "            grp_final2=pd.concat([grp_final2,df_local[['ident','id_grp']]],sort=False)\n",
    "    else :\n",
    "        groupe_pt_gertrude_etape2(df_local)\n",
    "        increment+=grp_final2.id_grp.max()\n",
    "        df_local.loc[df_local['id_grp']!=-99,'id_grp']=df_local['id_grp']+increment\n",
    "        grp_final2=pd.concat([df_local[['ident','id_grp']],grp_final2],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rappatrier l'id_grp sur les données de base et export pour modif manuelle\n",
    "gertrude.merge(grp_final2, on='ident',how='left').to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude.shp')\n",
    "\n",
    "#import des modifs manuelles et mise en forme finale\n",
    "gertrude_manu=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude.shp')\n",
    "gertrude_manu.loc[(gertrude_manu.id_grp.isna()) | (gertrude_manu.id_grp==-99),'id_grp']=gertrude_manu.loc[(gertrude_manu.id_grp.isna()) | (gertrude_manu.id_grp==-99),'id_grp'].reset_index().index + gertrude_manu.id_grp.max()\n",
    "gertrude_manu.loc[gertrude_manu.type_grp.isnull(),'type_grp']='auto'\n",
    "#export pour dernière modif manuelle\n",
    "gertrude_manu.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude_final.shp')\n",
    "\n",
    "#fichier final\n",
    "gertrude_fin=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude_final.shp')\n",
    "#concat avec les siredo\n",
    "cle_assoc_siredo['id_grpsiredo']=cle_assoc_siredo['id_grpsiredo']+gertrude_fin.id_grp.max()\n",
    "cpt_perm_final=pd.concat([gertrude_fin,siredo.merge(cle_assoc_siredo, on='ident').rename(columns={'id_grpsiredo':'id_grp'})],axis=0, sort=False)\n",
    "cpt_perm_final.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cpt_perm_groupe.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Affecter du trafic au filaire de voie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### On commence par affecter le trafic aux troncons des catégories 1,2,3 à partirdes comptages permanents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#trouver la ligne la plus proche\n",
    "\n",
    "lgn_proche_perm=cpt_perm_final.merge(plus_proche_voisin(cpt_perm_final,gdf_rhv_groupe,10,'ident','ident'),left_on='ident', right_on='ident_left',how='left').merge(\n",
    "    gdf_rhv_groupe[['ident','cat_rhv','rgraph_dbl','idtronc']], left_on='ident_right', right_on='ident', how='left').rename(columns={'ident_right':'ident_lgn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer le trafic total par point de comptage permanent\n",
    "def calcul_tmjo_2sens_perm(tmjo, rgraph_dbl, nb_cpt, df,id_grp,nom_attr_trafic,nom_attr_id_grp,id_cpt,nom_attr_id_cpt) :\n",
    "    \"\"\"calculer le tmja 2 sens en fonction du nb de compteur et du sens unique ou non\"\"\"\n",
    "    if nb_cpt==2 : \n",
    "        return df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].sum(), df.loc[df[nom_attr_id_grp]==id_grp].groupby(nom_attr_id_grp)[nom_attr_id_cpt].agg(\n",
    "            lambda x : tuple(x)).values[0]\n",
    "    else : \n",
    "        if rgraph_dbl==1 : \n",
    "            return tmjo*2, (id_cpt,)\n",
    "        else : \n",
    "            return tmjo, (id_cpt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_attr_sup=lgn_proche_perm.merge(lgn_proche_perm.groupby('id_grp')['gid'].nunique(), left_on='id_grp',\n",
    "                    right_index=True).rename(columns={'gid_y':'nb_cpt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_attr_sup['tmjo_2_sens']=lgn_proche_attr_sup.apply(lambda x : calcul_tmjo_2sens_perm(x['mjo_val'],x['rgraph_dbl'],\n",
    "                                                                                          x['nb_cpt'], lgn_proche_attr_sup,x['id_grp'],\n",
    "                                                                                          'mjo_val','id_grp',x['ident_x'],'ident_x')[0],axis=1)\n",
    "lgn_proche_attr_sup['id_cpt_2_sens']=lgn_proche_attr_sup.apply(lambda x : calcul_tmjo_2sens_perm(x['mjo_val'],x['rgraph_dbl'],\n",
    "                                                                                          x['nb_cpt'], lgn_proche_attr_sup,x['id_grp'],\n",
    "                                                                                          'mjo_val','id_grp',x['ident_x'],'ident_x')[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour les lignes de cat rhv 1 ou 2 ou 3 : basculer ce trafic vers l'ensemble des lignes du même idtronc issu du cat_rhv_123\n",
    "mjo_tronc_cat123=lgn_proche_attr_sup.loc[lgn_proche_attr_sup['cat_rhv'].isin(['1','2','3','61','62','63'])][['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].merge(\n",
    "    gdf_rhv_groupe_123[['ident','idtronc']].rename(columns={'ident':'ident_lgn'}),how='left')#[['idtronc','tmjo_2_sens']]\n",
    "mjo_tronc_cat123=mjo_tronc_cat123.drop_duplicates(['idtronc','tmjo_2_sens'])\n",
    "\n",
    "#il rest des données dupliquée, notamment car certains troncons supportent plueiusr points de comptages. dans ce cas on ne garde que la valeur max (vérifié)\n",
    "traf_max=mjo_tronc_cat123.loc[mjo_tronc_cat123.duplicated('idtronc',keep=False)].groupby('idtronc').tmjo_2_sens.max().reset_index().merge(\n",
    "mjo_tronc_cat123, on='idtronc')\n",
    "traf_max=traf_max.loc[traf_max['tmjo_2_sens_x']==traf_max['tmjo_2_sens_y']].drop('tmjo_2_sens_y',axis=1).rename(columns={'tmjo_2_sens_x':'tmjo_2_sens'}).set_index('idtronc').copy()\n",
    "mjo_tronc_cat123.set_index('idtronc',inplace=True)\n",
    "mjo_tronc_cat123.update(traf_max.reset_index().set_index('idtronc'))\n",
    "mjo_tronc_cat123.reset_index(inplace=True)\n",
    "mjo_tronc_cat123_v2=mjo_tronc_cat123.drop_duplicates(['idtronc','tmjo_2_sens'])\n",
    "#jointure entre l'idtronc issu des cat 1,2,3 et les lignes qui y sont affectées puis jointure avec la df de base des lignes\n",
    "gdf_rhv_cpt_perm_123=gdf_rhv_groupe_123.merge(mjo_tronc_cat123_v2, on='idtronc', how='left')\n",
    "gdf_rhv_cpt_perm_123.loc[~gdf_rhv_cpt_perm_123.tmjo_2_sens.isna(),'type_cpt']='permanent'\n",
    "gdf_rhv_cpt_perm_123.drop('ident_lgn',axis=1, inplace=True)\n",
    "#export\n",
    "gdf_rhv_cpt_perm_123[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens', 'type_cpt']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_v0.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ensuite les comptages ponctuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#à partir de affect final, chercher les points qui supporte pas déjà un tafic d'un comptage permanent (attention, on se base sur les troncon elementaires toute\n",
    "#catégorie rhv)\n",
    "cpt_ponct_ok=cpt_pct_l93.merge(affect_finale.loc[~affect_finale.idtronc.isin(lgn_proche_perm.idtronc.tolist())][['ident','idtronc']], on='ident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#trouver la ligne la plus proche\n",
    "lgn_proche_ponct=cpt_ponct_ok.merge(plus_proche_voisin(cpt_ponct_ok,gdf_rhv_groupe,10,'ident','ident'),left_on='ident', right_on='ident_left',how='left').merge(\n",
    "    gdf_rhv_groupe[['ident','cat_rhv','rgraph_dbl','idtronc']], left_on='ident_right', right_on='ident', how='left').rename(\n",
    "    columns={'ident_right':'ident_lgn','idtronc_x':'idtronc_tt_rhv','ident_x':'ident'}).drop(['ident_left','ident_y','idtronc_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer le trafic total par point de comptage permanent\n",
    "def calcul_tmjo_2sens_ponct(tmjo, rgraph_dbl, nb_cpt,sens_uniq, df,id_grp,nom_attr_trafic,nom_attr_id_grp,id_cpt,nom_attr_id_cpt) :\n",
    "    \"\"\"calculer le tmja 2 sens en fonction du nb de compteur et du sens unique ou non\"\"\"\n",
    "    if nb_cpt==2 : \n",
    "        if (df.loc[df[nom_attr_id_grp]==id_grp].sens_unique==True).all() : #si sur unmm idtronc les 2 copt sont en sens uniq, on garde le max\n",
    "            return (df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].max(),df.loc[(df[nom_attr_id_grp]==id_grp) & (df[nom_attr_trafic]==\n",
    "                        df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].max())][nom_attr_id_cpt].values[0],)\n",
    "        else :\n",
    "            return df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].sum(), df.loc[df[nom_attr_id_grp]==id_grp].groupby(nom_attr_id_grp)[nom_attr_id_cpt].agg(\n",
    "                lambda x : tuple(x)).values[0]\n",
    "    else : \n",
    "        if rgraph_dbl==1 : \n",
    "            return tmjo*2, (id_cpt,)\n",
    "        else : \n",
    "            return tmjo, (id_cpt,)\n",
    "        \n",
    "#calcul du tmjo_2_sens\n",
    "lgn_proche_ponct_attr_sup=lgn_proche_ponct.merge(lgn_proche_ponct.groupby('idtronc_tt_rhv')['ident'].nunique(), on='idtronc_tt_rhv').rename(\n",
    "    columns={'ident_y':'nb_cpt','ident_x':'ident'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_ponct_attr_sup['tmjo_2_sens']=lgn_proche_ponct_attr_sup.apply(lambda x : \n",
    "                        calcul_tmjo_2sens_ponct(x['tmjo_tv'],x['rgraph_dbl'],x['nb_cpt'],x['sens_unique'], lgn_proche_ponct_attr_sup,\n",
    "                                          x['idtronc_tt_rhv'],'tmjo_tv','idtronc_tt_rhv',x['ident'],'ident')[0],axis=1)\n",
    "lgn_proche_ponct_attr_sup['id_cpt_2_sens']=lgn_proche_ponct_attr_sup.apply(lambda x : \n",
    "                        calcul_tmjo_2sens_ponct(x['tmjo_tv'],x['rgraph_dbl'],x['nb_cpt'],x['sens_unique'], lgn_proche_ponct_attr_sup,\n",
    "                                          x['idtronc_tt_rhv'],'tmjo_tv','idtronc_tt_rhv',x['ident'],'ident')[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verif doublon de compteur --si besoin\n",
    "lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup.duplicated('ident')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 cas possibles : soit le comptage ponctuel est sur un troncon dejà concerné issu des troncon elementaire categorie 1,2,3 ou non\n",
    "\n",
    "#séparer les points restant dans les 2 catégories : \n",
    "ponct_sur_perm_123=lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident_lgn'].isin(\n",
    "    gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['type_cpt']=='permanent'].ident.tolist())].copy()\n",
    "ponct_libre_123=lgn_proche_ponct_attr_sup.loc[~lgn_proche_ponct_attr_sup['ident_lgn'].isin(\n",
    "    gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['type_cpt']=='permanent'].ident.tolist())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rappatriement des compteurs sur les idtronc des cat 1,2,3 : il y a des doublons car les troncons sont long, donc on separe, et on va traiter les ponct_libr sans doublons\n",
    "ponct_libre_123_tot=gdf_rhv_groupe_123[['ident','idtronc']].merge(ponct_libre_123[['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].rename(columns={'ident_lgn':'ident'}), \n",
    "    on='ident')[['idtronc','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates(['idtronc','tmjo_2_sens','id_cpt_2_sens'])\n",
    "list_pct_libre_dbl=ponct_libre_123_tot.loc[ponct_libre_123_tot.duplicated('idtronc',keep=False)].sort_values('idtronc').idtronc.unique()\n",
    "#affectation des ponctuels libre sans doublons\n",
    "pct_libre_123_ss_dbl=ponct_libre_123_tot.loc[~ponct_libre_123_tot['idtronc'].isin(list_pct_libre_dbl)].copy()\n",
    "pct_libre_123_ss_dbl['type_cpt']='ponctuel'\n",
    "gdf_rhv_cpt_perm_123.set_index('idtronc', inplace=True)\n",
    "gdf_rhv_cpt_perm_123.update(pct_libre_123_ss_dbl.set_index('idtronc'))\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl=gdf_rhv_groupe_123.merge(pct_libre_123_ss_dbl, on='idtronc')\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl.loc[~gdf_rhv_groupe_123_pctLibreSsDbl.tmjo_2_sens.isna(),'type_cpt']='ponctuel'\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctSsDblv0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affectation des ponctuel libre avec doublons\n",
    "#l'idee c'est pour un idtronc des cat 1,2,3 on a la liste des ident, on affecte sur les ident des idtronc elementaire \n",
    "#sur les lignes, puis on propage sur les tronc de la cat 1,2,3\n",
    "\n",
    "pct_libre_123_avec_dbl=ponct_libre_123_tot.loc[ponct_libre_123_tot['idtronc'].isin(list_pct_libre_dbl)].copy()\n",
    "gdf_rhv_cpt_perm_123=gdf_rhv_cpt_perm_123.reset_index().set_index('ident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idtronc_test in sorted(pct_libre_123_avec_dbl.idtronc.unique()) : \n",
    "    print(idtronc_test)\n",
    "    #récupérer l'idtronc toute cat_rhv des compteurs avec doublons et faire la jointure du trafic sur les lignes concernees\n",
    "    pct_libre_123_avec_dbl_decompose=lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident'].isin(\n",
    "        [a for b in pct_libre_123_avec_dbl.loc[pct_libre_123_avec_dbl['idtronc']==idtronc_test].\n",
    "         id_cpt_2_sens.tolist() for a in b])][['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']].copy()\n",
    "\n",
    "    #initialisation des paramètres : \n",
    "    ligne_ok=gdf_rhv_groupe.merge(pct_libre_123_avec_dbl_decompose.rename(columns={'idtronc_tt_rhv':'idtronc'}), on='idtronc').merge(\n",
    "        graph_filaire_123[['ident','source','target']], on='ident').drop(['source_x', 'target_x'],axis=1).rename(\n",
    "        columns={'source_y':'source', 'target_y':'target'})#lignes avec trafic issues des troncons elemntaires et on garde les ousrces et target liées uniquement aux cat 1,2,3\n",
    "    lgn_id_tronc_123_vide=gdf_rhv_groupe_123.loc[(gdf_rhv_groupe_123['idtronc']==idtronc_test) & \n",
    "                                                 (~gdf_rhv_groupe_123.ident.isin(ligne_ok.ident.tolist()))] #l'ensemble des lignes a renseigner\n",
    "    while ~lgn_id_tronc_123_vide.empty :\n",
    "        list_noeud_trafic=ligne_ok.source.tolist()+ligne_ok.target.tolist() #les noeuds avec du trafic\n",
    "        list_noeud_trafic_null=lgn_id_tronc_123_vide.source.tolist()+lgn_id_tronc_123_vide.target.tolist() #l'inverse\n",
    "        lgn_a_renseigner=lgn_id_tronc_123_vide.loc[((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic_null))) | \n",
    "                ((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic_null)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic)))].copy()\n",
    "        if lgn_a_renseigner.empty : break #si pas de ligne a renseigner on sort\n",
    "        ligne_ok['noeud_partage']=ligne_ok.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic_null else x['target'], axis=1) #pour faire une jointure entre les lignes a trafic et les autres\n",
    "        lgn_a_renseigner['noeud_partage']=lgn_a_renseigner.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic else x['target'], axis=1)\n",
    "        lgn_a_renseigner=lgn_a_renseigner.merge(ligne_ok[['noeud_partage','tmjo_2_sens','id_cpt_2_sens']], on='noeud_partage') #recupere le tmj\n",
    "        ligne_ok=pd.concat([ligne_ok,lgn_a_renseigner], axis=0, sort=False)#on ajoute les lignes avec du trafic aux autres\n",
    "        lgn_id_tronc_123_vide=lgn_id_tronc_123_vide.loc[~lgn_id_tronc_123_vide.ident.isin(ligne_ok.ident.tolist())].copy() #on met à jour pour la boucle\n",
    "    ligne_ok.drop_duplicates('ident',inplace=True)# les ronds points crees des doublons\n",
    "    ligne_ok['type_cpt']='ponctuel'\n",
    "    #mettre à jour la donnees de base\n",
    "    gdf_rhv_cpt_perm_123.update(ligne_ok[['ident','tmjo_2_sens','id_cpt_2_sens','type_cpt']].set_index('ident'))\n",
    "#export\n",
    "#gdf_rhv_groupe_123_pctLibreAvecDbl.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctAvecDblv0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affectation des ponctuels sur permanents : de la mm façon que les ponctuels en doublons\n",
    "\n",
    "#trouver la liste des idtronc cat_rhv = 1,2,3 avec ponct sur perm\n",
    "idtronc123_pct_sur_perm=gdf_rhv_groupe_123[['ident','idtronc']].merge(ponct_sur_perm_123[['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].rename(columns={'ident_lgn':'ident'}), \n",
    "    on='ident')[['idtronc','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates()\n",
    "\n",
    "for idtronc_test in idtronc123_pct_sur_perm.idtronc.unique() : \n",
    "    print(idtronc_test)\n",
    "    #trouver les id tronc elementaire des comptages ponct et perm concerne\n",
    "    ligne_ok= gdf_rhv_groupe.merge(pd.concat([lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident'].isin(\n",
    "        [a for b in idtronc123_pct_sur_perm.loc[idtronc123_pct_sur_perm['idtronc']==idtronc_test].id_cpt_2_sens.tolist()\n",
    "         for a in b])][['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates(['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']),\n",
    "    gdf_rhv_groupe_123.loc[gdf_rhv_groupe_123['idtronc']==idtronc_test].merge(\n",
    "        lgn_proche_attr_sup, left_on='ident', right_on='ident_lgn')[['idtronc_y','tmjo_2_sens','id_cpt_2_sens']].rename(\n",
    "        columns={'idtronc_y' : 'idtronc_tt_rhv'}).drop_duplicates()],axis=0, sort=False), left_on='idtronc', right_on='idtronc_tt_rhv')\n",
    "\n",
    "    lgn_id_tronc_123_vide=gdf_rhv_groupe_123.loc[(gdf_rhv_groupe_123['idtronc']==idtronc_test) & \n",
    "                                                     (~gdf_rhv_groupe_123.ident.isin(ligne_ok.ident.tolist()))]\n",
    "\n",
    "    while ~lgn_id_tronc_123_vide.empty :\n",
    "        list_noeud_trafic=ligne_ok.source.tolist()+ligne_ok.target.tolist() #les noeuds avec du trafic\n",
    "        list_noeud_trafic_null=lgn_id_tronc_123_vide.source.tolist()+lgn_id_tronc_123_vide.target.tolist() #l'inverse\n",
    "        lgn_a_renseigner=lgn_id_tronc_123_vide.loc[((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic_null))) | \n",
    "                ((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic_null)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic)))].copy()\n",
    "        if lgn_a_renseigner.empty : break #si pas de ligne a renseigner on sort\n",
    "        ligne_ok['noeud_partage']=ligne_ok.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic_null else x['target'], axis=1) #pour faire une jointure entre les lignes a trafic et les autres\n",
    "        lgn_a_renseigner['noeud_partage']=lgn_a_renseigner.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic else x['target'], axis=1)\n",
    "        lgn_a_renseigner=lgn_a_renseigner.merge(ligne_ok[['noeud_partage','tmjo_2_sens','id_cpt_2_sens']], on='noeud_partage') #recupere le tmj\n",
    "        ligne_ok=pd.concat([ligne_ok,lgn_a_renseigner], axis=0, sort=False)#on ajoute les lignes avec du trafic aux autres\n",
    "        lgn_id_tronc_123_vide=lgn_id_tronc_123_vide.loc[~lgn_id_tronc_123_vide.ident.isin(ligne_ok.ident.tolist())].copy() #on met à jour pour la boucle\n",
    "    ligne_ok.drop_duplicates('ident',inplace=True)# les ronds points crees des doublons\n",
    "    ligne_ok['type_cpt']='ponctuel'\n",
    "    #mettre à jour la donnees de base\n",
    "    gdf_rhv_cpt_perm_123.update(ligne_ok[['ident','tmjo_2_sens','id_cpt_2_sens','type_cpt']].set_index('ident'))\n",
    "\n",
    "#gdf_rhv_groupe_123_pctLibreAvecDbl.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctAvecDblv01.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_cpt_perm_123['id_cpt_2_sens'].fillna('NC', inplace=True)\n",
    "gdf_rhv_cpt_perm_123['id_cpt_2_sens']=gdf_rhv_cpt_perm_123.apply(lambda x : ', '.join([str(a) for a in x['id_cpt_2_sens']] \n",
    "                if not isinstance(x['id_cpt_2_sens'],np.int64) else str(x['id_cpt_2_sens']) ), axis=1)\n",
    "gdf_rhv_cpt_perm_123.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_v1.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# On peut essayer de trouver les lignes qui intersectent d'autres avec lignes avec du trafci pour les completer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#trouver les noeuds du graph pour lesquels les lignes qui arrivent\n",
    "\n",
    "#simplifier la topologie en affectant une seule valuer de noeud aux voie arrivant sur un rond point\n",
    "#trouver les noeud faisant partie d'un rond point \n",
    "\n",
    "df=import_donnes_base('local_otv','linearisation_bm', 'graph_rhv_123','graph_rhv_123_vertices_pgr')\n",
    "df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=identifier_rd_pt(df)\n",
    "\n",
    "lgn_rdpt=df_avec_rd_pt.loc[~df_avec_rd_pt.id_rdpt.isna()].copy()\n",
    "dico_noeud=rt.creer_dico_noeud_rdpt(lgn_rdpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# àpartir du dico on va remplacer les valeurs de source ou target par la valeur de clé du dico. \n",
    "gdf_rhv_rdpt_simple=gdf_rhv_cpt_perm_123.loc[~gdf_rhv_cpt_perm_123.index.isin(lgn_rdpt.ident.to_list())].copy()\n",
    "#remplacement des osurces et targets : \n",
    "simplifier_noeud_rdpt(gdf_rhv_rdpt_simple, dico_noeud)\n",
    "#puis recalculer le count du nb de ligne par noeud (en otant les lignes qui font les rdpoints)\n",
    "cnt_maj=maj_graph_rdpt(gdf_rhv_rdpt_simple)\n",
    "\n",
    "#trouver les noeuds fv concernes par des lignes à mettre à jour\n",
    "noeud_grp,df_noeud_tot=noeud_fv_ligne_ss_trafic(gdf_rhv_rdpt_simple,'max 1 NaN')\n",
    "\n",
    "#se limiter aux noeuds présentant 3 lignes (pour rappel, seul une ligne présente un trafic inconnu)\n",
    "liste_noeud=noeud_grp.loc[(noeud_grp['estimable']) & \n",
    "             (noeud_grp.apply(lambda x : len(x['tmjo_2_sens'])==3, axis=1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.calculer le trafic sur la voie manquante (A FAIRE)\n",
    "\n",
    "def calcul_trafic_manquant(num_noeud, df, df_tot, nom_idtroncon, graph) : \n",
    "    \"\"\"\n",
    "    detreminer du trafic sur unnoeud a 3 voies avec 2 trafic connu\n",
    "    in : \n",
    "        num_noeud : identifiant du noeud\n",
    "        df : df des lignes liées au noeud\n",
    "        df_tot : df globale du reseau routier sans simplification des rd points. doit contenir un attribut de groupement par troncon\n",
    "        nom_idtroncon : string : nom de l'attribut de groupement par troncon\n",
    "    \"\"\"\n",
    "    \n",
    "        \n",
    "    def nb_sens(rgraph_dbl,nb_noeud_unique,nb_noeud,nb_lgn_tch_noeud_unique,nb_ligne) : \n",
    "        \"\"\"\n",
    "        savoir si un troncon est en double sens ou non\n",
    "        \"\"\"\n",
    "        if rgraph_dbl==0 :\n",
    "            if nb_noeud_unique > 2 :\n",
    "                if nb_lgn_tch_noeud_unique > 2 : \n",
    "                    if nb_ligne==nb_noeud-1 : \n",
    "                        return 0\n",
    "                    elif nb_ligne==nb_noeud-2 : \n",
    "                        return 1\n",
    "                elif nb_lgn_tch_noeud_unique == 2 :\n",
    "                    return 1\n",
    "            else : return 0\n",
    "        else: return 1\n",
    "        \n",
    "    df_calcul=df.copy()\n",
    "    \n",
    "    #verification de l'attribut rggraph_dbl\n",
    "    #ça peut etre vraiment un voie à sens unique et chaussées unique, ou alors c'est un cas genre les boulevard, qui est note sens unique mais est en realite une voie double sens\n",
    "    #on va donc regarder le nombre de noeud unique pour savoir si le troncon contient des lignes parralleles ou non. si c le cas alors c un troncon double sens\n",
    "    df_calcul['list_noeud_unique']=df_calcul.apply(lambda x : rt.Troncon(df_tot, x['idtronc']).noeuds_uniques, axis=1) #liste des noeuds en fin de troncon\n",
    "    df_calcul['nb_noeud_unique']=df_calcul.apply(lambda x : rt.Troncon(df_tot, x['idtronc']).nb_noeuds_uniques, axis=1) #nb de noeud en fin de troncon\n",
    "    df_calcul['nb_noeud']=df_calcul.apply(lambda x : rt.Troncon(df_tot, x['idtronc']).nb_noeuds, axis=1) #nb de noeud en fin de troncon\n",
    "    df_calcul['nb_lgn_tch_noeud_unique']=df_calcul.apply(lambda x : rt.Troncon(df_tot, x['idtronc']).nb_lgn_tch_noeud_unique, axis=1) \n",
    "    df_calcul['nb_lgn']=df_calcul.apply(lambda x : rt.Troncon(df_tot, x['idtronc']).nb_lgn, axis=1)\n",
    "    #ajout d'un attribut supplémentaire\n",
    "    df_calcul['rgraph_dbl_2']=df_calcul.apply(lambda x : nb_sens(x['rgraph_dbl'],x['nb_noeud_unique'],x['nb_noeud'],x['nb_lgn_tch_noeud_unique'],x['nb_lgn']), axis=1)\n",
    "    \n",
    "    \n",
    "    #verification que le troncon a mettre à jour ne croise bien que 2 autres troncons pour les voies à 2 chaussées \n",
    "    if (df_calcul.loc[df_calcul['maj']].rgraph_dbl==0).all() and (df_calcul.loc[df_calcul['maj']].rgraph_dbl_2==1).all() : \n",
    "        #trouver le troncon à mettre à jour :\n",
    "        troncon=rt.Troncon(gdf_rhv_rdpt_simple,df_noeud.loc[df_noeud['maj']].idtronc.values[0])\n",
    "        #sutilisation de la cle de correspondance entre les neouds\n",
    "        corresp_noeud_uniq=troncon.groupe_noeud_route_2_chaussees(graph,100)\n",
    "        noeud_parrallele=corresp_noeud_uniq.loc[corresp_noeud_uniq['id_left']==num_noeud].id_right.values[0]\n",
    "        #trouver les troncon qui intersectent ce troncon sur le debut ou la fin et qui n'ont pas été fléchés au début\n",
    "        df_troncons_sup=df_tot.loc[((df_tot['source']==noeud_parrallele) | (df_tot['target']==noeud_parrallele)) & \n",
    "                               (~df_tot.idtronc.isin(df_calcul.idtronc.tolist()))]\n",
    "        if not df_troncons_sup.empty : \n",
    "            raise PasDeTraficError(troncon.id)\n",
    "    \n",
    "    \n",
    "    #print(df_calcul)\n",
    "    df_calcul_trafic_exist=df_calcul.loc[df_calcul['tmjo_2_sens']!=-99]\n",
    "    df_calcul_trafic_null=df_calcul.loc[df_calcul['tmjo_2_sens']==-99]\n",
    "    \n",
    "    #calcul selon les cas de sens unique ou non\n",
    "    if (df_calcul_trafic_exist.rgraph_dbl_2==0).all() : \n",
    "        if len(df_calcul_trafic_exist.type_noeud.unique())==1 : \n",
    "            return df_calcul_trafic_exist.tmjo_2_sens.sum()\n",
    "        else : \n",
    "            if (df_calcul_trafic_null.type_noeud=='a').all() :\n",
    "                return (df_calcul_trafic_exist.loc[df_calcul_trafic_exist['type_noeud']=='d'].tmjo_2_sens.values[0] - \n",
    "                        df_calcul_trafic_exist.loc[df_calcul_trafic_exist['type_noeud']=='a'].tmjo_2_sens.values[0])\n",
    "            else :\n",
    "                return (df_calcul_trafic_exist.loc[df_calcul_trafic_exist['type_noeud']=='a'].tmjo_2_sens.values[0] - \n",
    "                 df_calcul_trafic_exist.loc[df_calcul_trafic_exist['type_noeud']=='d'].tmjo_2_sens.values[0])\n",
    "    elif (df_calcul_trafic_exist.rgraph_dbl_2!=0).all() :\n",
    "        return df_calcul_trafic_exist.tmjo_2_sens.max()-df_calcul_trafic_exist.tmjo_2_sens.min()\n",
    "    else : \n",
    "        if (df_calcul_trafic_null.rgraph_dbl_2==1).all() : \n",
    "            return df_calcul_trafic_exist.loc[df_calcul_trafic_exist['rgraph_dbl']==1].tmjo_2_sens.values[0]\n",
    "\n",
    "class PasDeTraficError(Exception):\n",
    "    \"\"\"\n",
    "    Exception levée si la ligne n'est pas affectée à du trafic\n",
    "    \"\"\"\n",
    "    def __init__(self, idtroncon):\n",
    "        Exception.__init__(self,f'pas de trafic sur le troncon : {idtroncon}')\n",
    "        self.erreur_type='PasDeTraficError'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8235\n"
     ]
    }
   ],
   "source": [
    "gdf_rhv_extrapol_1=gdf_rhv_cpt_perm_123.reset_index().set_index('idtronc') #initialiser pour resultats    \n",
    "list_noeud_pb={}\n",
    "#for noeud in liste_noeud.noeud.tolist() :\n",
    "noeud=8235\n",
    "print(noeud)\n",
    "df_temp=gdf_rhv_rdpt_simple.loc[(gdf_rhv_rdpt_simple['source']==noeud) | (gdf_rhv_rdpt_simple['target']==noeud)].copy()\n",
    "df_temp['type_noeud']=df_temp.apply(lambda x : 'd' if x['source']==noeud else 'a', axis=1 )\n",
    "df_noeud=df_noeud_tot.loc[df_noeud_tot['noeud']==noeud].merge(df_temp[['id_ign','type_noeud', 'idtronc']], on='id_ign')\n",
    "df_noeud['maj']=df_noeud.apply(lambda x : True if x['tmjo_2_sens']==-99 else False, axis=1)\n",
    "try : \n",
    "    df_noeud.loc[df_noeud['tmjo_2_sens']==-99,'tmjo_2_sens']=df_noeud.apply(lambda x : calcul_trafic_manquant(noeud,df_noeud,gdf_rhv_cpt_perm_123,'idtronc',graph_filaire_123_vertex), axis=1)\n",
    "except PasDeTraficError :\n",
    "    list_noeud_pb[noeud]='PasDeTraficError'\n",
    "except IndexError : \n",
    "    list_noeud_pb[noeud]='IndexError'\n",
    "if df_noeud.tmjo_2_sens.isnull().any() : \n",
    "    list_noeud_pb[noeud]='autre Pb trafic'\n",
    "gdf_rhv_extrapol_1.update(df_noeud.loc[df_noeud['maj']][['idtronc','tmjo_2_sens']].set_index('idtronc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_ign</th>\n",
       "      <th>noeud</th>\n",
       "      <th>tmjo_2_sens</th>\n",
       "      <th>cat_rhv</th>\n",
       "      <th>rgraph_dbl</th>\n",
       "      <th>type_noeud</th>\n",
       "      <th>idtronc</th>\n",
       "      <th>maj</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRONROUT41444</td>\n",
       "      <td>8235</td>\n",
       "      <td>1833.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>d</td>\n",
       "      <td>6013.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRONROUT41296</td>\n",
       "      <td>8235</td>\n",
       "      <td>7402.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>d</td>\n",
       "      <td>4760.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRONROUT7036</td>\n",
       "      <td>8235</td>\n",
       "      <td>7402.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>a</td>\n",
       "      <td>7883.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id_ign  noeud  tmjo_2_sens cat_rhv  rgraph_dbl type_noeud  idtronc  \\\n",
       "0  TRONROUT41444   8235       1833.0       3         0.0          d   6013.0   \n",
       "1  TRONROUT41296   8235       7402.0       3         1.0          d   4760.0   \n",
       "2   TRONROUT7036   8235       7402.0       3         1.0          a   7883.0   \n",
       "\n",
       "     maj  \n",
       "0  False  \n",
       "1  False  \n",
       "2   True  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_noeud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_noeud_pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_extrapol_1.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens','source','target']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_extrapol.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MMM\n",
    "En premier lieu il faut mettre à jour les attributs en renomant de façon explicite puis en faisant la somme des 2 sens de circulation pour les voies doucble sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer les données et convertir les noms de champs\n",
    "fichier_src=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\MMM\\2017_Matin_PR\\2017_HPM_link.SHP')\n",
    "#définir les colonnes définitives (PLUS TARD ON FERA UN FILTRE EN AMONT)\n",
    "liste_col_def=['NO', 'FROMNODENO', 'TONODENO', 'TYPENO', 'TSYSSET', 'LENGTH', 'NUMLANES', 'CAPPRT', 'V0_TV', 'VOLVEHPR~1', \n",
    "'Q_HC_PL', 'Q_HC_TV', 'Q_HC_VL', 'Q_HPM_PL', 'Q_HPM_TV', 'Q_HPM_VL', 'Q_HPS_PL', 'Q_HPS_TV', 'Q_HPS_VL', 'Q_Jour_PL', 'Q_Jour_TV',\n",
    "'Q_Jour_VL','coef_HC_PL','coef_HC_TV','coef_HC_VL','Ocup_VL_PL','V0_PL','V0_VL','VCharg_PL','VCharg_VL','VOLPCUP~22',\n",
    "'SHAREHGV', 'VOLVEH_~23', 'VOLVEH_~24', 'VOLCAPR~25', 'VEHHOUR~26', 'SPEEDLIMIT',\n",
    "'R_NO', 'R_FROMNODENO', 'R_TONODENO', 'R_TYPENO', 'R_TSYSSET','R_LENGTH', 'R_NUMLANES', 'R_CAPPRT', 'r_V0_TV', 'R_VOLVEHPR~1', \n",
    "'R_Q_HC_PL', 'R_Q_HC_TV', 'R_Q_HC_VL', 'R_Q_HPM_PL', 'R_Q_HPM_TV', 'R_Q_HPM_VL', \n",
    "'R_Q_HPS_PL', 'R_Q_HPS_TV', 'R_Q_HPS_VL', 'R_Q_Jour_PL', 'R_Q_Jour_TV', 'R_Q_Jour_VL', 'R_coef_HC_PL', 'R_coef_HC_TV', 'R_coef_HC_VL', 'R_Ocup_VL_PL', 'R_V0_PL', \n",
    "'R_V0_VL', 'R_VCharg_PL', 'R_VCharg_VL', 'R_VOLPCUP~22', 'R_SHAREHGV', 'R_VOLVEH_~23', 'R_VOLVEH_~24', 'R_VOLCAPR~25', 'R_VEHHOUR~26', 'R_SPEEDLIMIT']\n",
    "#creer un dico de renomage et renommer\n",
    "dico_renomage={a: b for a, b in zip(fichier_src.columns,liste_col_def)}\n",
    "fichier_src.rename(columns=dico_renomage, inplace=True)\n",
    "#ne conserver dans un premier temps que les attributs relatifs au trafic sur la journée\n",
    "fichier_src_simpl=fichier_src[['geometry','NO', 'FROMNODENO', 'TONODENO', 'TYPENO', 'TSYSSET', 'LENGTH', 'NUMLANES', 'CAPPRT', 'V0_TV', \n",
    "             'VOLVEHPR~1']+[a for a in fichier_src.columns if 'Jour' in a ]].copy()\n",
    "#caculer le tmja_TV\n",
    "fichier_src_simpl['tmja_tv']=fichier_src_simpl.Q_Jour_TV+fichier_src_simpl.R_Q_Jour_TV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NO</th>\n",
       "      <th>FROMNODENO</th>\n",
       "      <th>TONODENO</th>\n",
       "      <th>TYPENO</th>\n",
       "      <th>TSYSSET</th>\n",
       "      <th>LENGTH</th>\n",
       "      <th>NUMLANES</th>\n",
       "      <th>CAPPRT</th>\n",
       "      <th>V0_TV</th>\n",
       "      <th>VOLVEHPR~1</th>\n",
       "      <th>Q_Jour_PL</th>\n",
       "      <th>Q_Jour_TV</th>\n",
       "      <th>Q_Jour_VL</th>\n",
       "      <th>R_Q_Jour_PL</th>\n",
       "      <th>R_Q_Jour_TV</th>\n",
       "      <th>R_Q_Jour_VL</th>\n",
       "      <th>tmja_tv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000062</td>\n",
       "      <td>2000054</td>\n",
       "      <td>5868961</td>\n",
       "      <td>82</td>\n",
       "      <td>A,B,BHNS,P,R,V</td>\n",
       "      <td>0.114km</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>40km/h</td>\n",
       "      <td>314</td>\n",
       "      <td>221.68</td>\n",
       "      <td>4422.38</td>\n",
       "      <td>4200.70</td>\n",
       "      <td>294.99</td>\n",
       "      <td>5945.08</td>\n",
       "      <td>5650.08</td>\n",
       "      <td>10367.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000063</td>\n",
       "      <td>2000054</td>\n",
       "      <td>6389027</td>\n",
       "      <td>82</td>\n",
       "      <td>A,B,BHNS,P,R,V</td>\n",
       "      <td>0.099km</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>40km/h</td>\n",
       "      <td>467</td>\n",
       "      <td>294.99</td>\n",
       "      <td>5945.08</td>\n",
       "      <td>5650.08</td>\n",
       "      <td>221.68</td>\n",
       "      <td>4422.38</td>\n",
       "      <td>4200.70</td>\n",
       "      <td>10367.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000064</td>\n",
       "      <td>3527</td>\n",
       "      <td>2000055</td>\n",
       "      <td>77</td>\n",
       "      <td>A,B,BHNS,P,R,V</td>\n",
       "      <td>0.160km</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>40km/h</td>\n",
       "      <td>269</td>\n",
       "      <td>582.93</td>\n",
       "      <td>9835.65</td>\n",
       "      <td>9190.25</td>\n",
       "      <td>424.10</td>\n",
       "      <td>6218.11</td>\n",
       "      <td>5755.06</td>\n",
       "      <td>16053.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000065</td>\n",
       "      <td>2000055</td>\n",
       "      <td>4852848</td>\n",
       "      <td>77</td>\n",
       "      <td>A,B,BHNS,P,R,V</td>\n",
       "      <td>0.037km</td>\n",
       "      <td>1</td>\n",
       "      <td>750</td>\n",
       "      <td>40km/h</td>\n",
       "      <td>269</td>\n",
       "      <td>582.93</td>\n",
       "      <td>9835.65</td>\n",
       "      <td>9190.25</td>\n",
       "      <td>424.10</td>\n",
       "      <td>6218.11</td>\n",
       "      <td>5755.06</td>\n",
       "      <td>16053.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000068</td>\n",
       "      <td>5743212</td>\n",
       "      <td>20000010</td>\n",
       "      <td>82</td>\n",
       "      <td>A,B,BHNS,P,R,V</td>\n",
       "      <td>0.074km</td>\n",
       "      <td>1</td>\n",
       "      <td>600</td>\n",
       "      <td>40km/h</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        NO  FROMNODENO  TONODENO TYPENO         TSYSSET   LENGTH  NUMLANES  \\\n",
       "0  2000062     2000054   5868961     82  A,B,BHNS,P,R,V  0.114km         1   \n",
       "1  2000063     2000054   6389027     82  A,B,BHNS,P,R,V  0.099km         1   \n",
       "2  2000064        3527   2000055     77  A,B,BHNS,P,R,V  0.160km         1   \n",
       "3  2000065     2000055   4852848     77  A,B,BHNS,P,R,V  0.037km         1   \n",
       "4  2000068     5743212  20000010     82  A,B,BHNS,P,R,V  0.074km         1   \n",
       "\n",
       "   CAPPRT   V0_TV  VOLVEHPR~1  Q_Jour_PL  Q_Jour_TV  Q_Jour_VL  R_Q_Jour_PL  \\\n",
       "0     600  40km/h         314     221.68    4422.38    4200.70       294.99   \n",
       "1     600  40km/h         467     294.99    5945.08    5650.08       221.68   \n",
       "2     750  40km/h         269     582.93    9835.65    9190.25       424.10   \n",
       "3     750  40km/h         269     582.93    9835.65    9190.25       424.10   \n",
       "4     600  40km/h           0       0.00       0.00       0.00         0.00   \n",
       "\n",
       "   R_Q_Jour_TV  R_Q_Jour_VL   tmja_tv  \n",
       "0      5945.08      5650.08  10367.46  \n",
       "1      4422.38      4200.70  10367.46  \n",
       "2      6218.11      5755.06  16053.76  \n",
       "3      6218.11      5755.06  16053.76  \n",
       "4         0.00         0.00      0.00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fichier_src_simpl.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fichier_src_simpl.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\MMM_simplifie.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer graph et importer \n",
    "creer_graph(fichier_src_simpl, 'local_otv',id_name='NO', schema='public', table='mmm_graph', table_vertex='mmm_graph_vertices_pgr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer le graph\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    graph_mmm_filaire = gp.read_postgis('select * from linearisation_bm.mmm_graph', c.connexionPsy)\n",
    "    graph_mmm_filaire_vertex = gp.read_postgis('select * from linearisation_bm.mmm_graph_123_vertices_pgr', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer le fichier simplifie\n",
    "mmm_simple=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\MMM_simplifie.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## définir des relations entre les voies, à chaque point qui intersecte plus de 2 voie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [],
   "source": [
    "#les noued s sup à 2 voies\n",
    "noeud_sup2Voies=graph_mmm_filaire_vertex.loc[graph_mmm_filaire_vertex['cnt']>2].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver la correspondance entre les noeuds MMM et FV\n",
    "#depuis le MMM on cherche le noeud FV le plus proche et inversement,puis on fusionne\n",
    "appariement_noeud_mmm_fv=rt.appariement_noeud_mmm_fv(noeud_sup2Voies, graph_filaire_123_vertex, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver la correspondanceentre les lignes\n",
    "#importer les données de correspondance\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    cle_mmm_rhv = gp.read_postgis('select * from linearisation_bm.appariement_etape1_2', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver les noeuds fv concernes par des lignes à mettre à jour\n",
    "noeud_grp=noeud_fv_ligne_ss_trafic(gdf_rhv_cpt_perm_123,'min 1 ok')[0]\n",
    "#ne conserver que le noeuds relatif à une certaine catégorie\n",
    "liste_noeud_a_traiter=noeud_grp.loc[(noeud_grp['estimable']) & (noeud_grp.apply(lambda x : '1' in x['cat_rhv'],axis=1))].noeud.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 791,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_extrapol=gdf_rhv_cpt_perm_123.reset_index().set_index('idtronc')\n",
    "#pour un noeud\n",
    "noeud_fv=783\n",
    "#voies rhv concernees par le noeud\n",
    "matrice_voie_rhv=rt.corresp_noeud_rhv(gdf_rhv_cpt_perm_123,noeud_fv)\n",
    "#jointure via cle de passage avec voies du MMM pour la partie entrante. Si une des voies a pas de jointure  : pb ; si plusiuers voies MMM à joindre : pb\n",
    "joint_fv_mmm_e1=matrice_voie_rhv.merge(cle_mmm_rhv[['NO','ident']], left_on='ident_x', right_on='ident').drop_duplicates().rename(\n",
    "        columns={'NO':'NO_x'}).drop('ident', axis=1)\n",
    "if joint_fv_mmm_e1.NO_x.isna().any() : \n",
    "    print('pb')\n",
    "joint_fv_mmm_e2=joint_fv_mmm_e1.merge(cle_mmm_rhv[['NO','ident']], left_on='ident_y', right_on='ident').drop_duplicates().rename(\n",
    "        columns={'NO':'NO_y'}).drop('ident', axis=1)\n",
    "if joint_fv_mmm_e2.NO_y.isna().any() : \n",
    "    print('pb')\n",
    "#on ne garde que les voies qui ont un des deux tmjo avec un trafic inconnu\n",
    "trafic_inconnus=joint_fv_mmm_e2.loc[joint_fv_mmm_e2.tmjo_2_sens_x.isna() | joint_fv_mmm_e2.tmjo_2_sens_y.isna()].copy()\n",
    "#on ne garde que les ligne de catégorie la plus proche\n",
    "trafic_inconnus['diff_cat']=trafic_inconnus.apply(lambda x : abs((int(x['cat_rhv_x'])-int(x['cat_rhv_y']))), axis=1)\n",
    "trafic_inconnus_prior_cat=trafic_inconnus.loc[trafic_inconnus['diff_cat']==trafic_inconnus.groupby('ident_x').diff_cat.transform(min)]\n",
    "#passer les trafic du MMM\n",
    "traf_mmm=trafic_inconnus_prior_cat.merge(mmm_simple[['NO','tmja_tv']], left_on='NO_x', right_on='NO').drop('NO', axis=1).merge(\n",
    "    mmm_simple[['NO','tmja_tv']], left_on='NO_y', right_on='NO').drop('NO', axis=1)\n",
    "traf_mmm['traf_max']=traf_mmm.apply(lambda x:max(x['tmja_tv_x'],x['tmja_tv_y']), axis=1)\n",
    "#calcul des trafics\n",
    "trafc_rens=traf_mmm.copy()\n",
    "trafc_rens['ident_a_rens']=trafc_rens.apply(lambda x : x['ident_x'] if pd.isnull(x['tmjo_2_sens_x'])\n",
    "                                                  else x['ident_y'], axis=1)\n",
    "trafc_rens['tmjo_2_sens_extrapol']=trafc_rens.apply(lambda x : \n",
    "    int(x['tmja_tv_y']/x['tmja_tv_x']*x['tmjo_2_sens_x']) if pd.isnull(x['tmjo_2_sens_y']) else int(x['tmja_tv_x']/x['tmja_tv_y']*x['tmjo_2_sens_y']),\n",
    "                                                  axis=1)\n",
    "#si plusieurs resultas possibles pour unident on garde le max\n",
    "trafc_fin=trafc_rens.loc[trafc_rens['traf_max']==trafc_rens.groupby('ident_a_rens').traf_max.transform(max)].merge(\n",
    "    gdf_rhv_cpt_perm_123[['idtronc']], left_on='ident_a_rens',right_index=True)[['ident_a_rens','tmjo_2_sens_extrapol','idtronc']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mise à jour du fichier source\n",
    "gdf_rhv_extrapol.update(trafc_fin.set_index('idtronc').rename(columns={'tmjo_2_sens_extrapol':'tmjo_2_sens'})[['tmjo_2_sens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident_a_rens</th>\n",
       "      <th>tmjo_2_sens_extrapol</th>\n",
       "      <th>idtronc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2647</td>\n",
       "      <td>11232</td>\n",
       "      <td>7616.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident_a_rens  tmjo_2_sens_extrapol  idtronc\n",
       "0         2647                 11232   7616.0"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trafc_fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensemble de noeud\n",
    "gdf_rhv_extrapol=gdf_rhv_cpt_perm_123.reset_index().set_index('idtronc')\n",
    "for noeud_fv in liste_noeud_a_traiter :\n",
    "    print(noeud_fv)\n",
    "    #trouver noeud correspondant\n",
    "    try :\n",
    "        noeud_mmm_corrsp=appariement_noeud_mmm_fv.loc[appariement_noeud_mmm_fv['id_fv']==noeud_fv].id_mmm.values[0]\n",
    "    except IndexError : \n",
    "        continue        \n",
    "    #tableau de correspondance entre les voies du mmm\n",
    "    matrice_voie_mmm=rt.corresp_noeud_mmm(mmm_simple, noeud_mmm_corrsp)\n",
    "    ##tableau de correspondance entre les voies du rhv\n",
    "    matrice_voie_rhv=rt.corresp_noeud_rhv(gdf_rhv_cpt_perm_123,noeud_fv)\n",
    "    #jointure avec les voies du rhv\n",
    "    corresp_mm_rhv=matrice_voie_mmm.merge(cle_mmm_rhv[['NO','ident']], left_on='NO_x', right_on='NO').drop_duplicates().rename(\n",
    "        columns={'ident':'ident_x'}).drop('NO', axis=1).merge(cle_mmm_rhv[['NO','ident']], left_on='NO_y', right_on='NO').drop_duplicates().rename(\n",
    "        columns={'ident':'ident_y'}).drop('NO', axis=1)\n",
    "    #on ne garde que les voies d rhv présentent dans le table de correspondance\n",
    "    corresp_mm_rhv_fin=corresp_mm_rhv.loc[(corresp_mm_rhv.ident_x.isin(matrice_voie_rhv.ident_x.tolist()+matrice_voie_rhv.ident_y.tolist())) & \n",
    "                      (corresp_mm_rhv.ident_y.isin(matrice_voie_rhv.ident_x.tolist()+matrice_voie_rhv.ident_y.tolist()))]\n",
    "    #on joint les trafics\n",
    "    trafic_inconnus=corresp_mm_rhv_fin.merge(gdf_rhv_cpt_perm_123[['tmjo_2_sens', 'cat_rhv']], left_on='ident_x', right_index=True).merge(\n",
    "        gdf_rhv_cpt_perm_123[['tmjo_2_sens','cat_rhv']], left_on='ident_y', right_index=True)\n",
    "    #si les trafics inconnus sont vides on continu, on traitera apres\n",
    "    if trafic_inconnus.empty : \n",
    "        continue\n",
    "    #fonction de deterination des lignes a remplir\n",
    "    trafic_a_rens=trafic_inconnus.loc[((trafic_inconnus.tmjo_2_sens_x.isna() & trafic_inconnus.tmjo_2_sens_y.notna()) |\n",
    "    (trafic_inconnus.tmjo_2_sens_y.isna() & trafic_inconnus.tmjo_2_sens_x.notna()))].copy()\n",
    "    if trafic_a_rens.empty : \n",
    "        continue\n",
    "    #calcul des trafics\n",
    "    trafic_a_rens['ident_a_rens']=trafic_a_rens.apply(lambda x : x['ident_x'] if pd.isnull(x['tmjo_2_sens_x'])\n",
    "                                                      else x['ident_y'], axis=1)\n",
    "    trafic_a_rens['tmjo_2_sens_extrapol']=trafic_a_rens.apply(lambda x : \n",
    "        int(x['tmja_tv_y']/x['tmja_tv_x']*x['tmjo_2_sens_x']) if pd.isnull(x['tmjo_2_sens_y']) else int(x['tmja_tv_x']/x['tmja_tv_y']*x['tmjo_2_sens_y']),\n",
    "                                                      axis=1)\n",
    "    #filtre des resultats selon la cat_rhv : on prend la plus proche (a faire, pour le moment on prend celle qui est égale)\n",
    "    trafic_extrapol=trafic_a_rens.loc[trafic_a_rens['cat_rhv_x']==trafic_a_rens['cat_rhv_y']].copy()\n",
    "    #lien avec l'idtronc concerne pour affectation\n",
    "    trafic_extrapol=trafic_extrapol.merge(gdf_rhv_cpt_perm_123[['idtronc']], left_on='ident_a_rens', \n",
    "                                          right_index=True)[['ident_a_rens','tmjo_2_sens_extrapol','idtronc']]\n",
    "    #on ne gargee que le trafic max en cas de doublons\n",
    "    trafic_extrapol=trafic_extrapol.loc[trafic_extrapol['tmjo_2_sens_extrapol']==trafic_extrapol.groupby('idtronc').tmjo_2_sens_extrapol.transform(max)]\n",
    "    #mise à jour du fichier source\n",
    "\n",
    "    gdf_rhv_extrapol.update(trafic_extrapol.set_index('idtronc').rename(columns={'tmjo_2_sens_extrapol':'tmjo_2_sens'})[['tmjo_2_sens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_extrapol.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_extrapol.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 695,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "      <th>ident_x</th>\n",
       "      <th>ident_y</th>\n",
       "      <th>tmjo_2_sens_x</th>\n",
       "      <th>cat_rhv_x</th>\n",
       "      <th>tmjo_2_sens_y</th>\n",
       "      <th>cat_rhv_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6135635.0</td>\n",
       "      <td>1034980026</td>\n",
       "      <td>5948.72</td>\n",
       "      <td>1034980782</td>\n",
       "      <td>23295.99</td>\n",
       "      <td>8782</td>\n",
       "      <td>28019</td>\n",
       "      <td>7787.0</td>\n",
       "      <td>2</td>\n",
       "      <td>38095.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       noeud        NO_x  tmja_tv_x        NO_y  tmja_tv_y ident_x ident_y  \\\n",
       "1  6135635.0  1034980026    5948.72  1034980782   23295.99    8782   28019   \n",
       "\n",
       "   tmjo_2_sens_x cat_rhv_x  tmjo_2_sens_y cat_rhv_y  \n",
       "1         7787.0         2        38095.0         1  "
      ]
     },
     "execution_count": 695,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trafic_inconnus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>ident_x</th>\n",
       "      <th>tmjo_2_sens_x</th>\n",
       "      <th>ident_y</th>\n",
       "      <th>tmjo_2_sens_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>845</td>\n",
       "      <td>49936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2830</td>\n",
       "      <td>5133.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>845</td>\n",
       "      <td>49936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50641</td>\n",
       "      <td>14216.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>845</td>\n",
       "      <td>2830</td>\n",
       "      <td>5133.0</td>\n",
       "      <td>50641</td>\n",
       "      <td>14216.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   noeud ident_x  tmjo_2_sens_x ident_y  tmjo_2_sens_y\n",
       "1    845   49936            NaN    2830         5133.0\n",
       "2    845   49936            NaN   50641        14216.0\n",
       "5    845    2830         5133.0   50641        14216.0"
      ]
     },
     "execution_count": 686,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrice_voie_rhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       noeud      NO_x  tmja_tv_x        NO_y  tmja_tv_y\n",
       "1  4722269.0  57894565    1906.02    57931055    7814.92\n",
       "2  4722269.0  57894565    1906.02  2145032626    9646.26\n",
       "5  4722269.0  57931055    7814.92  2145032626    9646.26"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrice_voie_mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noeud</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "      <th>ident_x</th>\n",
       "      <th>ident_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57894565</td>\n",
       "      <td>1906.02</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "      <td>None</td>\n",
       "      <td>49936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4722269.0</td>\n",
       "      <td>57931055</td>\n",
       "      <td>7814.92</td>\n",
       "      <td>2145032626</td>\n",
       "      <td>9646.26</td>\n",
       "      <td>None</td>\n",
       "      <td>49936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       noeud      NO_x  tmja_tv_x        NO_y  tmja_tv_y ident_x ident_y\n",
       "0  4722269.0  57894565    1906.02    57931055    7814.92    None    None\n",
       "1  4722269.0  57894565    1906.02  2145032626    9646.26    None   49936\n",
       "2  4722269.0  57931055    7814.92  2145032626    9646.26    None   49936"
      ]
     },
     "execution_count": 690,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corresp_mm_rhv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filaire CBS\n",
    "En premier lieu il faut mettre à jour les attributs en renomant de façon explicite puis en faisant la somme des 2 sens de circulation pour les voies doucble sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filaire_cbs=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\CBS2017\\Filaire_global_metropole_DENSIFIER.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filaire_cbs['tmja']=filaire_cbs.apply(lambda x : (float(x['MT'].replace(',','.'))*12)+\n",
    "                                      (float(x['ME'].replace(',','.'))*4)+(float(x['MN'].replace(',','.'))*8),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filaire_cbs.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\filaire_tmja.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver les noeuds du troncon concerne\n",
    "toto=rt.Troncon(gdf_rhv_cpt_perm_123,934)\n",
    "noeud_uniq=toto.noeuds_uniques\n",
    "\n",
    "#si plus de 2 noeuds uniques, il faut regrouper\n",
    "df_noeud_uniq=graph_filaire_123_vertex.loc[(graph_filaire_123_vertex['id'].isin(noeud_uniq))]\n",
    "corresp_noeud_uniq=plus_proche_voisin(df_noeud_uniq, df_noeud_uniq, 100, 'id', 'id', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_left</th>\n",
       "      <th>id_right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9383</td>\n",
       "      <td>4194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4194</td>\n",
       "      <td>9383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1034</td>\n",
       "      <td>1031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1031</td>\n",
       "      <td>1034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id_left  id_right\n",
       "1     9383      4194\n",
       "2     4194      9383\n",
       "5     1034      1031\n",
       "6     1031      1034"
      ]
     },
     "execution_count": 1205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corresp_noeud_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trouver tout les noeud si 2*2 voies : \n",
    "noeud=1034\n",
    "#trouver le noeud supplementaire si besoin : \n",
    "list_tronc_noeud=gdf_rhv_cpt_perm_123.loc[(gdf_rhv_cpt_perm_123['source']==noeud) | (gdf_rhv_cpt_perm_123['target']==noeud)].copy().idtronc.tolist()\n",
    "list_noeud_sup=[]\n",
    "for t in list_tronc_noeud : \n",
    "    noeud_troncon=gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['idtronc']==t]\n",
    "    noeud_uniq=[k for k,v in Counter(noeud_troncon.source.tolist()+noeud_troncon.target.tolist()).items() if v==1]\n",
    "    if len(noeud_uniq)>2 : \n",
    "        df_noeud_uniq=graph_filaire_123_vertex.loc[(graph_filaire_123_vertex['id'].isin(noeud_uniq))]\n",
    "        corresp_noeud_uniq=plus_proche_voisin(df_noeud_uniq, df_noeud_uniq, 30, 'id', 'id', True)\n",
    "        list_noeud_sup+=corresp_noeud_uniq.loc[corresp_noeud_uniq['id_left']==noeud].id_right.tolist()\n",
    "list_noeud_sup=list(set(list_noeud_sup))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_noeud_sup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
