{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LINEARISATION UNIQUE DES TRAFICS SUR BORDEAUX METROPOLE**\n",
    "> Traitements à mettre en oeuvre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys #c'est pas propre mais pour le moment pour importer mes modules perso dans le notebook je ne sais pas faire\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Outils\\Outils\\Martin_Perso')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\otv\\otv\\Transfert_Donnees')\n",
    "sys.path.append(r'C:\\Users\\martin.schoreisz\\git\\Lin_uniq_BdxM\\Lin_uniq_BdxM\\Traitements')\n",
    "import Connexion_Transfert as ct\n",
    "import geopandas as gp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.geometry import MultiPoint, Polygon, box\n",
    "import re\n",
    "from difflib import get_close_matches,SequenceMatcher\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "from Outils import creer_graph, plus_proche_voisin,nb_noeud_unique_troncon_continu,verif_index\n",
    "\n",
    "from Appariement import (corresp_noeud_mmm,corresp_noeud_rhv,appariement_noeud_mmm_fv,estim_mmm_jointure_voies,isoler_trafic_inconnu,\n",
    "                         trafic_mmm,calcul_trafic_rhv_depuisMMM,PasCorrespondanceError)\n",
    "from Repartition_trafic import (noeud_fv_ligne_ss_trafic,df_noeud_troncon,verif_double_sens,verif_carrefour_2_chaussees,\n",
    "                                separer_troncon_a_estimer,calcul_trafic_manquant_3troncons,PasDeTraficError,maj_carrefour_3_troncons,trouver_noeud_3tronc_1NaN)\n",
    "from Simplifier_Rdpt import creer_dico_noeud_rdpt,simplifier_noeud_rdpt,maj_graph_rdpt\n",
    "import Estim_trafic as et\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "from Base_BdTopo.Import_outils import *\n",
    "from Base_BdTopo.Rond_points import *\n",
    "from Base_BdTopo.Regroupement_correspondance import *\n",
    "from Base_BdTopo.Troncon_elementaire import *\n",
    "from Base_BdTopo.Troncon_base import *\n",
    "from Base_BdTopo.Gestion_2_chaussee import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Regrouper les troncons du filaire de voie\n",
    ">Les troncons du filaire de voie sont regroupés d'abord uniquement avec les troncons de cat_rhv égal à 1,2,3,61,62,63, pour pouvoir réaliser des troncon de trafic homogènes tous reliés entre eux.\n",
    "<br> Ensuite, un deuxième regroupement comprenant l'ensemble des troncons de cat_rhv permetd'affiner le diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 1.1 Import du fichier final\n",
    "le fichier shape est vérifié à la main car l'affectation automatique n'est pas 100% fiable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour info : lire le graph issu de la methode decrite dessous\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    graph_filaire_123 = gp.read_postgis('select * from linearisation_bm.graph_rhv_123', c.connexionPsy)\n",
    "    graph_filaire_123_vertex = gp.read_postgis('select id,cnt,chk,ein,eout,the_geom as geom from linearisation_bm.graph_rhv_123_vertices_pgr', c.connexionPsy)\n",
    "    graph_filaire = gp.read_postgis('select * from linearisation_bm.graph_rhv_complet', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_groupe_123=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_groupe_123_v2.shp').merge(\n",
    "    graph_filaire_123[['ident','source','target']], on='ident', how='left')\n",
    "gdf_rhv_groupe=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\rhv_grp.shp').merge(\n",
    "    graph_filaire[['ident','source','target']], on='ident', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">### 1.2 Méthode\n",
    " pour information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. import du fichier de filaire de base\n",
    "gdf_rhv=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\Filaire_voie\\FV_TRONC_L.shp')\n",
    "# 2. mise en forme\n",
    "gdf_rhv.columns=[a.lower() for a in gdf_rhv.columns] #nom de colonne en minuscule\n",
    "gdf_rhv.rename(columns={'gid':'id','nom_voie' : 'numero'},inplace=True)\n",
    "gdf_rhv=gdf_rhv.loc[~gdf_rhv.id.isna()].copy()\n",
    "gdf_rhv['id_ign']=gdf_rhv.ident.apply(lambda x : 'TRONROUT'+str(x))\n",
    "gdf_rhv=gdf_rhv.loc[~gdf_rhv.cat_rhv.isin(['98','99'])].copy()\n",
    "gdf_rhv['nature']=gdf_rhv.apply(lambda x : 'Route à 2 chaussées' if x['rgraph_dbl']==0 else 'Route à 1 chaussée', axis=1)\n",
    "gdf_rhv['nature']=gdf_rhv.apply(lambda x : 'Bretelle' if x['cat_dig']=='8' else x['nature'], axis=1)\n",
    "gdf_rhv['numero']=gdf_rhv.apply(lambda x : 'Bretelle '+x['numero'] if x['cat_dig']=='8' else x['numero'], axis=1)\n",
    "gdf_rhv['sens']=gdf_rhv.apply(lambda x : 'Direct' if x['rgraph_dbl']==0 else 'Double', axis=1)\n",
    "gdf_rhv['codevoie_d']='NR'\n",
    "gdf_rhv['importance']=gdf_rhv['cat_dig']\n",
    "gdf_rhv['id']=gdf_rhv.id.apply(lambda x : int(x))\n",
    "#gdf_rhv.loc[gdf_rhv.nom_voie.isna()] #verif des nom_voie null : 2 lignes, cf traitements plus loins\n",
    "#filtrer les voies cylcables et autres\n",
    "gdf_rhv_filtre=gdf_rhv.loc[~gdf_rhv.cat_dig.isin(['6','7','9','10'])].copy()\n",
    "\n",
    "# 3. creer le graph en bdd\n",
    "#creer_graph(gdf_rhv_filtre.loc[gdf_rhv_filtre.cat_rhv.isin(['1','2','3','33','61','62','63'])],\n",
    "            #'local_otv',schema='linearisation_bm',table='graph_rhv_123',table_vertex='graph_rhv_123_vertices_pgr')\n",
    "creer_graph(gdf_rhv_filtre,'local_otv',schema='linearisation_bm',table='graph_rhv_complet',table_vertex='graph_rhv_complet_vertices_pgr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "# 4. initialiser les paramètre de regroupement\n",
    "df=import_donnes_base('local_otv','linearisation_bm', 'graph_rhv_complet','graph_rhv_complet_vertices_pgr')\n",
    "df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=identifier_rd_pt(df)\n",
    "df_lignes=df_avec_rd_pt.set_index('id_ign')#mettre l'id_ign en index\n",
    "bretelle=df_avec_rd_pt.loc[df_avec_rd_pt['nature']=='Bretelle'].copy()\n",
    "bretelle_tri=bretelle.loc[bretelle.length.sort_values(ascending=False).index.tolist()].id_ign.tolist()\n",
    "sans_bretelle=df_avec_rd_pt.loc[df_avec_rd_pt['nature']!='Bretelle'].copy()\n",
    "list_sans_bretelle=sans_bretelle.id_ign.tolist()\n",
    "list_tri_longueur=sans_bretelle.loc[sans_bretelle.length.sort_values(ascending=False).index.tolist()].id_ign.tolist()\n",
    "list_id_ign=bretelle_tri+list_tri_longueur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. appel fonction de grouepement pour creer des idtronc pour uniquement voies de 1 à 3 et d'un autre coté pour toute les voies\n",
    "df_affectation, dico_erreur, lignes_traitees, lignes_non_traitees=regrouper_troncon(\n",
    "    list_id_ign, df_avec_rd_pt, carac_rd_pt,df2_chaussees,[])\n",
    "\n",
    "# 6. mise en forme et export\n",
    "gdf_rhv_groupe=gdf_rhv_filtre.merge(df_affectation, left_on='id_ign', right_on='id', how='left')\n",
    "gdf_rhv_groupe.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\rhv_grp_v2.shp')\n",
    "#gdf_rhv_groupe_123=gdf_rhv_filtre.merge(df_affectation, left_on='id_ign', right_on='id', how='left')\n",
    "#gdf_rhv_groupe_123.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_groupe_123.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2019-12-12 11:48:54.415530 nb lignes traitees : 0\n",
      "pas de parrallele trouvee pour les troncons ['TRONROUT9919']\n"
     ]
    }
   ],
   "source": [
    "#pour test sur une ligne\n",
    "df_affectation2, dico_erreur2, lignes_traitees2, lignes_non_traitees2=regrouper_troncon(\n",
    "    ['TRONROUT9919'], df_avec_rd_pt, carac_rd_pt,df2_chaussees,[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MISE EN FORME DES COMPTAGES PONCTUELS\n",
    "Pour ces comptages l'idée c'est de réussir à regrouper les comptages par un idtronc issu de la detection des troncon elementaire. ça a du sens car pour un comptage ponctuel normalement les 2 sens de cicru sont sur un troncon unique. DE plus, les ens circu, noms de voie et autres attributs permettant de regrouper les comptages sont de qualité trop variable pour s'appuyer uniquement dessus. et enfin, peu de 2*2 vois sont meusrés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import des données\n",
    "cpt_pct=pd.read_excel(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\base_comptages_routiers_v4.xlsx')\n",
    "gdf_rhv_groupe['geom_src']=gdf_rhv_groupe.geometry\n",
    "cpt_pct.columns=[a.lower() for a in cpt_pct.columns]\n",
    "cpt_pct = gp.GeoDataFrame(cpt_pct, geometry=gp.points_from_xy(cpt_pct.longitude, cpt_pct.latitude))\n",
    "cpt_pct.crs = {'init' :'epsg:4326'}\n",
    "cpt_pct_l93=cpt_pct.to_crs({'init': 'epsg:2154'})\n",
    "cpt_pct_l93['x_l93']=cpt_pct_l93.geometry.apply(lambda x : x.x)\n",
    "cpt_pct_l93['y_l93']=cpt_pct_l93.geometry.apply(lambda x : x.y)\n",
    "\n",
    "#mise en forme\n",
    "cpt_pct_l93['nom_voie']=cpt_pct_l93.nom_voie.apply(lambda x : re.sub(('é|è|ê'),'e',x.lower().strip()))\n",
    "cpt_pct_l93['sens_unique']=cpt_pct_l93.sens_circulation.apply(lambda x : True if SequenceMatcher(None,' '.join(x.split(' ')[:2]).lower(),'sens unique').ratio()>0.8 else False)\n",
    "cpt_pct_l93['type_voie']=cpt_pct_l93.nom_voie.apply(lambda x : x.split(' ')[0])\n",
    "cpt_pct_l93['suffix_nom_voie']=cpt_pct_l93.nom_voie.apply(lambda x : ' '.join(x.split(' ')[1:]).lower())\n",
    "cpt_pct_l93['date_max_cptg']=cpt_pct_l93.observation.apply(lambda x : pd.to_datetime(x.split(' au ')[1],dayfirst=True))\n",
    "cpt_pct_l93['sens_circulation']=cpt_pct_l93.sens_circulation.apply(lambda x : re.sub(('é|è|ê'),'e',x.strip().lower()))\n",
    "cpt_pct_l93['observation']=cpt_pct_l93.observation.apply(lambda x : x.strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster à 200m\n",
    "data_test_clust=[[x, y] for x, y in zip(cpt_pct_l93.x_l93.tolist(), cpt_pct_l93.y_l93.tolist())]\n",
    "db = DBSCAN(eps=200, min_samples=2).fit(data_test_clust)\n",
    "labels = db.labels_\n",
    "cpt_pct_l93['n_cluster']=labels\n",
    "\n",
    "#voies avec nom_proches\n",
    "cross_join_ncluster=cpt_pct_l93[['ident','nom_voie','type_voie','suffix_nom_voie', 'sens_circulation','annee','n_cluster']].merge(\n",
    "   cpt_pct_l93[['ident','nom_voie','type_voie','suffix_nom_voie', 'sens_circulation','annee','n_cluster']], on='n_cluster') #avoir toutes les relations internoms possibles\n",
    "cross_join_ncluster['comp_nom_voie']=cross_join_ncluster.apply(lambda x : SequenceMatcher(None,x['suffix_nom_voie_x'], x['suffix_nom_voie_y']).ratio(), axis=1)#affecter une note a cahque relation\n",
    "voie_nom_proches=cross_join_ncluster.loc[(cross_join_ncluster['comp_nom_voie']>0.85) & (cross_join_ncluster['type_voie_x']==cross_join_ncluster['type_voie_y'])\n",
    "                        ].sort_values(['n_cluster','ident_x'])#ne conserver qque les relations bien notees\n",
    "voie_nom_proches['id_grp_nom_voie']=voie_nom_proches.ident_x.rank(method='dense')#ajouter un id \n",
    "corresp_voies=voie_nom_proches.drop_duplicates('ident_y')\n",
    "grp_nom_voie=cpt_pct_l93.merge(corresp_voies[['ident_y','id_grp_nom_voie']].rename(columns={'ident_y':'ident'}), on='ident')#jointure sur id de depart\n",
    "\n",
    "#grouper par periode\n",
    "grp_period=grp_nom_voie.copy()\n",
    "grp_period['date_max_cptg']=grp_period.observation.apply(lambda x : pd.to_datetime(x.split(' au ')[1],dayfirst=True))\n",
    "grp_period['id_period']=grp_period.date_max_cptg.rank(method='dense')\n",
    "grp_period['indicefinal']=grp_period.n_cluster+(grp_period.id_grp_nom_voie*(1000))+(grp_period.id_period*10000)\n",
    "grp_period[[a for a in grp_period.columns if a!='date_max_cptg']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cluster_comptg_pctl_grp_period.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#regarder le nb de ligne relative à chaque comptage : siune ligne est relative à 2 comptage qui ont le mm indice final\n",
    "#i.e relatif à la mme rue, à la mm periode, sur des sens différents, on fait la somme\n",
    "#si il y a plus de 2 comptages : il faut récupérer les plus recents : si 2 on fait la somme, si 1 on prends la valeur\n",
    "\n",
    "#rappatrier le numero d'id_tronc pour chaque comptage\n",
    "#trouver la distance min à chaque objet ligne\"du rhv groupe\n",
    "grp_troncon_temp=cpt_pct_l93.copy()\n",
    "grp_troncon_temp.geometry=grp_troncon_temp.buffer(20)#passer la geom en buffer\n",
    "intersct_buff_20=gp.sjoin(grp_troncon_temp,gdf_rhv_groupe,how='left',op='intersects')\n",
    "intersct_buff_20.geometry=grp_period.geometry#repasser la geom en point\n",
    "intersct_buff_20=intersct_buff_20.merge(gdf_rhv_groupe[['ident','geometry']], left_on='ident_right', right_on='ident')\n",
    "intersct_buff_20['dist_pt_ligne']=intersct_buff_20.apply(lambda x : x['geometry_x'].distance(x['geometry_y']), axis=1) #définir la disance entre les points et ligne\n",
    "joint_dist_min=intersct_buff_20.loc[intersct_buff_20.groupby('ident_left')['dist_pt_ligne'].transform(min)==intersct_buff_20['dist_pt_ligne']][['ident_left','idtronc','numero']].rename(\n",
    "    columns={'ident_left':'ident'}).copy()#ne garder que la ligne la plus proche\n",
    "grp_troncon=grp_period.merge(joint_dist_min, on='ident',how='left')#df finale\n",
    "grp_troncon[[a for a in grp_troncon.columns if a!='date_max_cptg']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cluster_comptg_pctl_grp_troncon.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annee la plus recente\n",
    "anne_recente=grp_troncon.loc[grp_troncon.groupby('idtronc')['date_max_cptg'].transform(max)==grp_troncon['date_max_cptg']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idtronc ok (avec 1 ou 2 pt de comptage)\n",
    "idtronc_grp=anne_recente.groupby('idtronc').ident.count()\n",
    "idtroncOkTmjo=anne_recente.loc[anne_recente.idtronc.isin(idtronc_grp.loc[idtronc_grp<3].index.tolist())][['ident','idtronc','sens_circulation','tmjo_tv']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isoler les idtronc supportant + de 2 pt de comptages\n",
    "idtronc_sup2=idtronc_grp.loc[idtronc_grp>2].copy()\n",
    "#trouver les points correspondants\n",
    "pt_sup2=anne_recente.loc[anne_recente['idtronc'].isin(idtronc_sup2.index.tolist())].copy()\n",
    "#ajouter attribut qui traduit le nb de valeurs différentes de sens circulation\n",
    "def nb_sens_circu(idtronc) :\n",
    "    return len(pt_sup2.loc[pt_sup2['idtronc']==idtronc].sens_circulation.unique())  \n",
    "pt_sup2['nb_sens_circu']=pt_sup2.apply(lambda x : nb_sens_circu(x['idtronc']), axis=1)\n",
    "#pour les points a 2 sens de circu : on prend la valeur max des 2 : \n",
    "ptSup2SensCircu2=pt_sup2.loc[pt_sup2['nb_sens_circu']==2].copy()\n",
    "ptSup2SensCircu2.drop_duplicates(['nom_voie','sens_circulation','tmjo_tv','observation'],inplace=True)#qq points ont des ident différents mais sont les mêmes\n",
    "ptSup2SensCircu2.drop_duplicates(['sens_circulation','tmjo_tv','observation'],inplace=True)#qla mm que la précédente, mais je ne sais pas pourquoi l'ajout de nom_voie fait bugger le drop duplicates pour les ident  716,717,975,976\n",
    "ptSup2SensCircu2OkTmjo=ptSup2SensCircu2.groupby(['idtronc','sens_circulation'])['tmjo_tv'].max().reset_index().merge(\n",
    "    ptSup2SensCircu2[['idtronc','sens_circulation','tmjo_tv','ident']], on=['idtronc','sens_circulation','tmjo_tv'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour les autres point\n",
    "ptSup2SensCircuSup2=pt_sup2.loc[pt_sup2['nb_sens_circu']>2][['ident','idtronc','sens_circulation','tmjo_tv','indicefinal']].sort_values(['idtronc','indicefinal','sens_circulation']).copy()\n",
    "\n",
    "#filtre des points dont le sens circul est le mm ou qui sont isole\n",
    "list_ident, list_idtronc, list_indicefinal, list_senscircu=(ptSup2SensCircuSup2.ident.tolist(),ptSup2SensCircuSup2.idtronc.tolist(),\n",
    "                                                            ptSup2SensCircuSup2.indicefinal.tolist(),ptSup2SensCircuSup2.sens_circulation.tolist())\n",
    "list_new_ident=list_ident\n",
    "for i in range(len(list_ident)) : \n",
    "    if i<len(list_ident)-1 :\n",
    "        if list_idtronc[i+1]==list_idtronc[i] : \n",
    "            if list_indicefinal[i+1]==list_indicefinal[i] :\n",
    "                if SequenceMatcher(None,list_senscircu[i+1], list_senscircu[i]).ratio()>0.75 : \n",
    "                    list_new_ident[i+1]=list_ident[i]\n",
    "    else : \n",
    "        if list_idtronc[i]==list_idtronc[i-1] : \n",
    "            if list_indicefinal[i]==list_indicefinal[i-1] :\n",
    "                if SequenceMatcher(None,list_senscircu[i], list_senscircu[i-1]).ratio()>0.75 : \n",
    "                    list_new_ident[i]=list_ident[i-1]\n",
    "ptSup2SensCircuSup2['ident_final']=np.array(list_ident)\n",
    "#filtrer les points isoles\n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2.groupby(['idtronc','indicefinal']).nunique()[['ident']].reset_index()\n",
    "ptSup2SensCircuSup2=ptSup2SensCircuSup2.loc[ptSup2SensCircuSup2.indicefinal.isin(ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre['ident']>1].indicefinal.\n",
    "                                                                             tolist())].copy()\n",
    "#filtrer les points qui ont des nom des sens circu proches\n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2.loc[ptSup2SensCircuSup2['tmjo_tv']==\n",
    "                                                          ptSup2SensCircuSup2.groupby('ident_final')['tmjo_tv'].transform(max)].copy()\n",
    "#filtrer les points qui sont égaux \n",
    "ptSup2SensCircuSup2_filtre=ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre['ident']==ptSup2SensCircuSup2_filtre['ident_final']].copy()\n",
    "#filtrer les points qui présente toujours plus de 2 identifiant (i.e pb denomination ou pb référentiel ou pb tronc_elementaire)\n",
    "grp=ptSup2SensCircuSup2_filtre.groupby('idtronc').nunique()[['ident']].reset_index()\n",
    "pt_non_affectes=grp.loc[grp['ident']>2]\n",
    "ptSup2SensCircuSup2Oktmjo=ptSup2SensCircuSup2_filtre.loc[ptSup2SensCircuSup2_filtre.idtronc.isin(grp.loc[grp['ident']<3].idtronc.tolist())][['ident','idtronc','sens_circulation','tmjo_tv']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "affect_finale=pd.concat([ptSup2SensCircuSup2Oktmjo,idtroncOkTmjo,ptSup2SensCircu2OkTmjo],axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qq modifs manuelle pour grouper par paire les comptages ponctuels: \n",
    "affect_finale.loc[affect_finale['ident']==606,'idtronc']=affect_finale.loc[affect_finale['ident']==605].idtronc.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MISE EN FORME DES COMPTAGES PERMANENTS\n",
    "Pour ces compteurs on va regrouper en séparant les siredo des boucles liées au système gertrude. Pour les siredo pas de pb, pour les autre sc'est plus compliqué car les références de localiusation varient parfois. Il y a donc une phase de regroupement manuel à la fin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Import du fichier final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fichier corrige_final\n",
    "cpt_perm_final=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cpt_perm_groupe.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### 2.1 Méthode\n",
    "Pour info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer un gdf avec la fusion des points et des données de comptages non nulles\n",
    "cpt_brut=pd.read_excel(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\comptage_trafic_2017.xlsx')\n",
    "cpt_brut.columns=[a.lower() for a in cpt_brut.columns]\n",
    "cpt_brut=cpt_brut.loc[~cpt_brut.mjo_val.isna()].copy()\n",
    "pt_cpt_perm=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_source\\Bdx-Metro\\comptages\\PC_CAPTE_P.shp')\n",
    "pt_cpt_perm.columns=[a.lower() for a in pt_cpt_perm.columns]\n",
    "pt_cpt_perm=pt_cpt_perm.merge(cpt_brut, on='ident')\n",
    "pt_cpt_perm['nom_voie']=pt_cpt_perm.nom_voie.apply(lambda x : re.sub(('é|è|ê'),'e',x.lower().replace('  ',' ')))#mise en forme nom de voie\n",
    "\n",
    "#regrouper les comptages\n",
    "#un compteur peut caracteriser 1 ligne, ou 2 compteur 1 ligne ou 2 compteurs 2 lignes, dc les compteurs ne vont pas toujours par deux\n",
    "\n",
    "#regrouper geographiquement\n",
    "pt_cpt_perm['x_l93']=pt_cpt_perm.geometry.apply(lambda x : x.x)\n",
    "pt_cpt_perm['y_l93']=pt_cpt_perm.geometry.apply(lambda x : x.y)\n",
    "data_test_clust=[[x, y] for x, y in zip(pt_cpt_perm.x_l93.tolist(), pt_cpt_perm.y_l93.tolist())]\n",
    "db = DBSCAN(eps=200, min_samples=2).fit(data_test_clust)\n",
    "labels = db.labels_\n",
    "pt_cpt_perm['n_cluster']=labels\n",
    "\n",
    "#correspondance nom_rue\n",
    "#analyser les noms de commune : \n",
    "#grp_nom_voie.nom_voie.apply(lambda x : x.split()[0]).unique()#liste des permiers mots\n",
    "#grp_nom_voie.loc[grp_nom_voie.nom_voie.apply(lambda x : x.split()[0]=='carbon')].nom_voie.unique() #test des valeusr\n",
    "dico_commune={'le bouscat','le haillan','bordeaux','pessac','talence', 'merignac','st medard','carbon blanc',\n",
    "       'begles', 'eysines','villenave d\\'ornon','bruges','gradignan','bassens','cenon','floirac','pessa','lormont', 'artigues'}\n",
    "dico_type_voie={'allee' : ['all.','allee', 'allees'],'avenue': ['av', 'av.', 'avenue'], 'boulevard' : ['blvd'],\n",
    "               'bretelle' : 'bretelle', 'cours' : ['crs','cours'], 'cote' : ['côte'], 'place':['pl'], 'route':['route', 'rte'],'rue':['rue'],\n",
    "               'voie':['voie']}\n",
    "def decoupe_nom_voie(nom_voie) : \n",
    "    for c in dico_commune : \n",
    "        if nom_voie[:len(c)]==c :\n",
    "            commune=nom_voie[:len(c)]\n",
    "            nom_voie=nom_voie[len(c):].strip()\n",
    "    for k, v in dico_type_voie.items() : \n",
    "        if any([a for a in nom_voie.split() if a in v])  : \n",
    "            caractere=[a for a in nom_voie.split() if a in v][0]\n",
    "            type_voie=k\n",
    "            nom_voie=' '.join([n for n in nom_voie.split() if n!=caractere])\n",
    "            break\n",
    "    else : type_voie=None\n",
    "    for test in ['avant','apres'] :\n",
    "        if test in nom_voie : \n",
    "            return nom_voie.split(test)[0].strip(), test,nom_voie.split(test)[1].strip(), commune, type_voie\n",
    "    else : return nom_voie, nom_voie,nom_voie,commune,type_voie\n",
    "\n",
    "pt_cpt_perm['nom_rue']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[0])\n",
    "pt_cpt_perm['localisant']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[1])\n",
    "pt_cpt_perm['rue_refer']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[2])\n",
    "pt_cpt_perm['commune']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[3])\n",
    "pt_cpt_perm['type_voie']=pt_cpt_perm.nom_voie.apply(lambda x : decoupe_nom_voie(x)[4])\n",
    "pt_cpt_perm.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\pt_cpt_perm.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>#### 2.2.1 SIREDO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siredo=pt_cpt_perm.loc[pt_cpt_perm['type']=='SIREDO'].copy()#isoler siredo\n",
    "cross_join=siredo.assign(key=1).merge(siredo.assign(key=1), on='key').drop('key',axis=1)#calcul des distances\n",
    "cross_join['distance']=cross_join.apply(lambda x : x['geometry_x'].distance(x['geometry_y']),axis=1)\n",
    "cross_join_filtre=cross_join[[c for c in cross_join.columns if c[-2:]=='_x']+['distance','ident_y']].rename(columns=\n",
    "              {c : c[:-2] for c in cross_join.columns if c[-2:]=='_x'}).copy()#filtre des attribut\n",
    "cross_join_filtre=cross_join_filtre.loc[(cross_join_filtre['ident']!=cross_join_filtre['ident_y'])].copy()\n",
    "siredo_proches=cross_join_filtre.loc[(cross_join_filtre.groupby('ident')['distance'].transform(min)==\n",
    "                                      cross_join_filtre['distance'])].copy()#plus proche voisin\n",
    "siredo_proches['id_grpsiredo']=siredo_proches.reset_index().index\n",
    "id_siredo_proches=(pd.concat([siredo_proches[['ident','ident_y', 'id_grpsiredo']],\n",
    "           siredo_proches[['ident_y','ident', 'id_grpsiredo']].rename(columns={\n",
    "    'ident_y':'ident', 'ident':'ident_y'})],sort=False).sort_values('ident').\n",
    "    reset_index().drop_duplicates('index').drop_duplicates(['ident','ident_y']).drop('index',axis=1))\n",
    "cle_assoc_siredo=pd.concat([id_siredo_proches[['ident','id_grpsiredo']],id_siredo_proches[['ident_y','id_grpsiredo']].rename(columns={\n",
    "    'ident_y':'ident'})]).sort_values('id_grpsiredo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>#### 2.2.2 Gertrude\n",
    "> au final ce n'est pas très efficace, et un renseignement des regroupement a la ain aurait été plus rapide. il sera important si de nouvelles stations sont posées de fournir de suite un identifiant de regroupement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gertrude=pt_cpt_perm.loc[pt_cpt_perm['type']=='BOUCLE'].copy()#isoler les boucles\n",
    "gertrude['x']=gertrude.geometry.apply(lambda x : x.x)\n",
    "gertrude['y']=gertrude.geometry.apply(lambda x : x.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si c'est sur le mm idtronc on regroupe\n",
    "\n",
    "#rappatrier le numero d'id_tronc pour chaque comptage\n",
    "#trouver la distance min à chaque objet ligne\"du rhv groupe\n",
    "grp_troncon_temp=gertrude.copy()\n",
    "grp_troncon_temp.geometry=grp_troncon_temp.buffer(20)#passer la geom en buffer\n",
    "intersct_buff_20=gp.sjoin(grp_troncon_temp,gdf_rhv_groupe,how='left',op='intersects')\n",
    "intersct_buff_20.geometry=gertrude.geometry#repasser la geom en point\n",
    "intersct_buff_20=intersct_buff_20.merge(gdf_rhv_groupe[['ident','geometry']], left_on='ident_right', right_on='ident')\n",
    "intersct_buff_20['dist_pt_ligne']=intersct_buff_20.apply(lambda x : x['geometry_x'].distance(x['geometry_y']), axis=1) #définir la disance entre les points et ligne\n",
    "joint_dist_min=intersct_buff_20.loc[intersct_buff_20.groupby('ident_left')['dist_pt_ligne'].transform(min)==intersct_buff_20['dist_pt_ligne']][['ident_left','idtronc','numero']].rename(\n",
    "    columns={'ident_left':'ident'}).copy()#ne garder que la ligne la plus proche\n",
    "grp_troncon=gertrude.merge(joint_dist_min, on='ident',how='left')#df finale\n",
    "\n",
    "#regrouper\n",
    "grp_idtronc=grp_troncon.groupby('idtronc')['ident'].agg(lambda x : tuple(x))\n",
    "grp_idtronc=grp_idtronc.loc[grp_idtronc.apply(lambda x : len(x)>1)].reset_index().copy()\n",
    "grp_idtronc['id_grp']=grp_idtronc.ident.rank()\n",
    "#mettre en forme\n",
    "grp_idtronc=pd.DataFrame([(a,g) for i,g in zip(grp_idtronc.ident.tolist(),grp_idtronc.id_grp.tolist()) for a in i], columns=['ident','id_grp'])\n",
    "grp_final=grp_idtronc.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trouver les points qui restent et préparer un regrouepement par commune, nom de rue, distance et se,s\n",
    "\n",
    "#points qui restent\n",
    "gertrude_etape2=gertrude.loc[~gertrude['ident'].isin(grp_idtronc.ident.tolist())].copy()\n",
    "#comparer les noms\n",
    "cross_join=gertrude_etape2[['gid','geometry','ident','nom_voie', 'sens_cir','commune', 'nom_rue', 'localisant','rue_refer','type_voie']].assign(key=1).merge(\n",
    "    gertrude_etape2[['gid','geometry','ident','nom_voie', 'sens_cir','commune', 'nom_rue', 'localisant','rue_refer','type_voie']].assign(key=1), on='key').drop('key',axis=1)#calcul des distances\n",
    "cross_join['distance']=cross_join.apply(lambda x : x['geometry_x'].distance(x['geometry_y']),axis=1)\n",
    "cross_join['comp_nom_rue']=cross_join.apply(lambda x : SequenceMatcher(None,x['nom_rue_x'], x['nom_rue_y']).ratio(), axis=1)#affecter une note a cahque relation\n",
    "\n",
    "#vérifier que les sens de cicru sont cohérents et ne garder que les lignes avec un sens ok, un nom de voie proche et une distance proche\n",
    "gertrude_join=cross_join.loc[(cross_join['comp_nom_rue']>0.6) & (cross_join['distance']<1000) & (cross_join['commune_x']==cross_join['commune_y']) & \n",
    "                             (cross_join['type_voie_x']==cross_join['type_voie_y'])].copy()\n",
    "#on ajoute un attribut de verif du sens\n",
    "def verif_sens(s1,s2) : \n",
    "    for sens in [['Sens Sortant', 'Sens Entrant'],['Sens N > S', 'Sens S > N'], ['Sens Nord > Sud', 'Sens Sud > Nord'],['Nord > Sud', 'Sud > Nord'],\n",
    "                ['Boulevards Nord > Sud', 'Boulevards Sud > Nord']] :\n",
    "        if s1 in sens and s2 in sens : \n",
    "            if s1!=s2 : \n",
    "                return True\n",
    "        else : \n",
    "            continue\n",
    "    return False\n",
    "gertrude_join['valid_sens']=gertrude_join.apply(lambda x : verif_sens(x['sens_cir_x'],x['sens_cir_y']),axis=1)  \n",
    "gertrude_join['id_grp']=gertrude_join.sort_values('ident_x').ident_x.rank(method=\"min\")\n",
    "gertrude_grp=gertrude_join.loc[gertrude_join['valid_sens']][['ident_x','ident_y','distance', 'comp_nom_rue', 'valid_sens','id_grp']].sort_values('ident_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "#obtenir une liste des ident qui se regroupent\n",
    "list_id_grp=pd.concat([gertrude_grp[['ident_x','id_grp']],gertrude_grp[['ident_y','id_grp']].rename(columns={'ident_y' : 'ident_x'})]).drop_duplicates([\n",
    "    'ident_x','id_grp']).groupby('id_grp').ident_x.agg(lambda x : set(sorted(tuple(x)))).tolist()\n",
    "dico={}\n",
    "for e,c in enumerate(gertrude_grp.ident_x.tolist()) : \n",
    "    for l in list_id_grp : \n",
    "        if c in l :\n",
    "            #print('c dans l','c : ',c,'l :',l)\n",
    "            if c not in dico.keys() :\n",
    "                dico[c]=list(l)\n",
    "            else : \n",
    "                dico[c]+=list(l)\n",
    "list_id_grp_temp=set([ v for v in {k : tuple(set(sorted(dico[k]))) for k in dico.keys()}.values()])\n",
    "list_id_grp_final=set(v for v in {c :b for c in gertrude_grp.ident_x.tolist() \n",
    " for b in [a for a in list_id_grp_temp if c in a] \n",
    " if len(b) == max([len(c) for c in [a for a in list_id_grp_temp if c in a]])}.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affecter un identifiant\n",
    "def groupe_pt_gertrude_etape2(df_local) : \n",
    "    \"\"\"\n",
    "    regrouper les points à partir d'une liste d'ident selon l'ordre d'apparition \n",
    "    \"\"\"\n",
    "    limite_box=MultiPoint([(a.x,a.y) for a in df_local.itertuples()]).bounds\n",
    "    plg_long, plg_larg=limite_box[2]-limite_box[0],limite_box[3]-limite_box[1]\n",
    "    if max(plg_long,plg_larg)==plg_long : \n",
    "        df_local['next_sens']=df_local.sort_values('x').sens_cir.shift(-1,fill_value='NC')\n",
    "        df_local['prev_sens']=df_local.sort_values('x').sens_cir.shift(1,fill_value='NC')\n",
    "    else : \n",
    "        df_local['next_sens']=df_local.sort_values('y').sens_cir.shift(-1,fill_value='NC')\n",
    "        df_local['prev_sens']=df_local.sort_values('y').sens_cir.shift(1,fill_value='NC')\n",
    "    df_local['sens_comp']=df_local.apply(lambda x : x.prev_sens if x.next_sens=='NC' else x.next_sens,axis=1)\n",
    "    df_local['verif_sens']=df_local.apply(lambda x : verif_sens(x['sens_cir'],x['sens_comp']),axis=1)\n",
    "    df_local['id_grp']=-99\n",
    "    i=0\n",
    "    while i < len(df_local)-1:\n",
    "        if df_local.iloc[i].verif_sens : \n",
    "            df_local.loc[df_local.index.isin(df_local.iloc[i:i+2].index.tolist()),'id_grp']=i\n",
    "            i+=2\n",
    "        else : \n",
    "            i+=1\n",
    "    return\n",
    "\n",
    "# à partior de la liste des ident groupé : \n",
    "grp_final2=grp_final.copy()\n",
    "# on selectionne ces points dans la df\n",
    "increment=grp_final2.id_grp.max()\n",
    "for l in list_id_grp_final : \n",
    "    #if 'Z8CT13' not in l : continue\n",
    "    df_local=gertrude_etape2.loc[gertrude_etape2.ident.isin(l)].copy()\n",
    "    if len(df_local)==1:\n",
    "        continue\n",
    "    if len(df_local)==2 : \n",
    "        if (gertrude_join.loc[(gertrude_join.ident_x.isin(l)) & (gertrude_join['ident_x'] != gertrude_join['ident_y'])].valid_sens).all() : \n",
    "            increment+=1\n",
    "            df_local['id_grp']=increment\n",
    "            grp_final2=pd.concat([grp_final2,df_local[['ident','id_grp']]],sort=False)\n",
    "    else :\n",
    "        groupe_pt_gertrude_etape2(df_local)\n",
    "        increment+=grp_final2.id_grp.max()\n",
    "        df_local.loc[df_local['id_grp']!=-99,'id_grp']=df_local['id_grp']+increment\n",
    "        grp_final2=pd.concat([df_local[['ident','id_grp']],grp_final2],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rappatrier l'id_grp sur les données de base et export pour modif manuelle\n",
    "gertrude.merge(grp_final2, on='ident',how='left').to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude.shp')\n",
    "\n",
    "#import des modifs manuelles et mise en forme finale\n",
    "gertrude_manu=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude.shp')\n",
    "gertrude_manu.loc[(gertrude_manu.id_grp.isna()) | (gertrude_manu.id_grp==-99),'id_grp']=gertrude_manu.loc[(gertrude_manu.id_grp.isna()) | (gertrude_manu.id_grp==-99),'id_grp'].reset_index().index + gertrude_manu.id_grp.max()\n",
    "gertrude_manu.loc[gertrude_manu.type_grp.isnull(),'type_grp']='auto'\n",
    "#export pour dernière modif manuelle\n",
    "gertrude_manu.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude_final.shp')\n",
    "\n",
    "#fichier final\n",
    "gertrude_fin=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gertrude_final.shp')\n",
    "#concat avec les siredo\n",
    "cle_assoc_siredo['id_grpsiredo']=cle_assoc_siredo['id_grpsiredo']+gertrude_fin.id_grp.max()\n",
    "cpt_perm_final=pd.concat([gertrude_fin,siredo.merge(cle_assoc_siredo, on='ident').rename(columns={'id_grpsiredo':'id_grp'})],axis=0, sort=False)\n",
    "cpt_perm_final.to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\cpt_perm_groupe.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Affecter du trafic au filaire de voie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### On commence par affecter le trafic aux troncons des catégories 1,2,3 à partirdes comptages permanents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#trouver la ligne la plus proche\n",
    "\n",
    "lgn_proche_perm=cpt_perm_final.merge(plus_proche_voisin(cpt_perm_final,gdf_rhv_groupe,10,'ident','ident'),left_on='ident', right_on='ident_left',how='left').merge(\n",
    "    gdf_rhv_groupe[['ident','cat_rhv','rgraph_dbl','idtronc']], left_on='ident_right', right_on='ident', how='left').rename(columns={'ident_right':'ident_lgn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer le trafic total par point de comptage permanent\n",
    "dico_verif_nb_sens={'Z27CT9':2,'Z27CT6':2,'Z31CT7':2,'Z31CT8':2,'Z25CT2':2,'Z23CT1':2,'Z25CT6':2,'Z14CT12':2,'Z14CT7':2,'Z14CT10':2,\n",
    "                   'Z14CT17':2,'Z14CT16':2,'Z1CT14':2,'Z1CT16':2,'Z1CT1':2,'Z1CT3' : 2,'Z1CT13':2,'Z5CT2':2,'Z29CT11':2,'Z8CT11':2,'Z8CT4':2,\n",
    "                   'Z8CT5':2,'Z8CT8':2,'Z22CT8':2,'Z29CT1':2,'Z30CT9':2,'Z13CT2':2,'Z9CT2':2,'Z11CT14':2,'Z11CT10':2,'Z26CT1':2,'Z11CT16':2,'Z17CT14':2,\n",
    "                   'Z17CT3': 2,'Z17CT9':2,'Z17CT4':2,'Z16CT14':2,'Z9CT15' :2}\n",
    "def calcul_tmjo_2sens_perm(tmjo, rgraph_dbl, nb_cpt, df,id_grp,nom_attr_trafic,nom_attr_id_grp,id_cpt,nom_attr_id_cpt,) :\n",
    "    \"\"\"calculer le tmja 2 sens en fonction du nb de compteur et du sens unique ou non\"\"\"\n",
    "    if nb_cpt==2 : \n",
    "        return df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].sum(), df.loc[df[nom_attr_id_grp]==id_grp].groupby(nom_attr_id_grp)[nom_attr_id_cpt].agg(\n",
    "            lambda x : tuple(x)).values[0]\n",
    "    else : \n",
    "        if rgraph_dbl==1 : \n",
    "            return tmjo*2, (id_cpt,)\n",
    "        else :\n",
    "            if id_cpt in dico_verif_nb_sens.keys() : \n",
    "                return tmjo*2, (id_cpt,)\n",
    "            return tmjo, (id_cpt,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_attr_sup=lgn_proche_perm.merge(lgn_proche_perm.groupby('id_grp')['gid'].nunique(), left_on='id_grp',\n",
    "                    right_index=True).rename(columns={'gid_y':'nb_cpt'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_attr_sup['tmjo_2_sens']=lgn_proche_attr_sup.apply(lambda x : calcul_tmjo_2sens_perm(x['mjo_val'],x['rgraph_dbl'],\n",
    "                                                                                          x['nb_cpt'], lgn_proche_attr_sup,x['id_grp'],\n",
    "                                                                                          'mjo_val','id_grp',x['ident_x'],'ident_x')[0],axis=1)\n",
    "lgn_proche_attr_sup['id_cpt_2_sens']=lgn_proche_attr_sup.apply(lambda x : calcul_tmjo_2sens_perm(x['mjo_val'],x['rgraph_dbl'],\n",
    "                                                                                          x['nb_cpt'], lgn_proche_attr_sup,x['id_grp'],\n",
    "                                                                                          'mjo_val','id_grp',x['ident_x'],'ident_x')[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pour les lignes de cat rhv 1 ou 2 ou 3 : basculer ce trafic vers l'ensemble des lignes du même idtronc issu du cat_rhv_123\n",
    "mjo_tronc_cat123=lgn_proche_attr_sup.loc[lgn_proche_attr_sup['cat_rhv'].isin(['1','2','3','61','62','63'])][['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].merge(\n",
    "    gdf_rhv_groupe_123[['ident','idtronc']].rename(columns={'ident':'ident_lgn'}),how='left')#[['idtronc','tmjo_2_sens']]\n",
    "mjo_tronc_cat123=mjo_tronc_cat123.drop_duplicates(['idtronc','tmjo_2_sens'])\n",
    "\n",
    "#il rest des données dupliquée, notamment car certains troncons supportent plueiusr points de comptages. dans ce cas on ne garde que la valeur max (vérifié)\n",
    "traf_max=mjo_tronc_cat123.loc[mjo_tronc_cat123.duplicated('idtronc',keep=False)].groupby('idtronc').tmjo_2_sens.max().reset_index().merge(\n",
    "mjo_tronc_cat123, on='idtronc')\n",
    "traf_max=traf_max.loc[traf_max['tmjo_2_sens_x']==traf_max['tmjo_2_sens_y']].drop('tmjo_2_sens_y',axis=1).rename(columns={'tmjo_2_sens_x':'tmjo_2_sens'}).set_index('idtronc').copy()\n",
    "mjo_tronc_cat123.set_index('idtronc',inplace=True)\n",
    "mjo_tronc_cat123.update(traf_max.reset_index().set_index('idtronc'))\n",
    "mjo_tronc_cat123.reset_index(inplace=True)\n",
    "mjo_tronc_cat123_v2=mjo_tronc_cat123.drop_duplicates(['idtronc','tmjo_2_sens'])\n",
    "#jointure entre l'idtronc issu des cat 1,2,3 et les lignes qui y sont affectées puis jointure avec la df de base des lignes\n",
    "gdf_rhv_cpt_perm_123=gdf_rhv_groupe_123.merge(mjo_tronc_cat123_v2, on='idtronc', how='left')\n",
    "gdf_rhv_cpt_perm_123.loc[~gdf_rhv_cpt_perm_123.tmjo_2_sens.isna(),'type_cpt']='permanent'\n",
    "gdf_rhv_cpt_perm_123.drop('ident_lgn',axis=1, inplace=True)\n",
    "#export\n",
    "gdf_rhv_cpt_perm_123[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens', 'type_cpt']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_v0.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### ensuite les comptages ponctuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#à partir de affect final, chercher les points qui supporte pas déjà un tafic d'un comptage permanent (attention, on se base sur les troncon elementaires toute\n",
    "#catégorie rhv)\n",
    "cpt_ponct_ok=cpt_pct_l93.merge(affect_finale.loc[~affect_finale.idtronc.isin(lgn_proche_perm.idtronc.tolist())][['ident','idtronc']], on='ident')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#trouver la ligne la plus proche\n",
    "lgn_proche_ponct=cpt_ponct_ok.merge(plus_proche_voisin(cpt_ponct_ok,gdf_rhv_groupe,10,'ident','ident'),left_on='ident', right_on='ident_left',how='left').merge(\n",
    "    gdf_rhv_groupe[['ident','cat_rhv','rgraph_dbl','idtronc']], left_on='ident_right', right_on='ident', how='left').rename(\n",
    "    columns={'ident_right':'ident_lgn','idtronc_x':'idtronc_tt_rhv','ident_x':'ident'}).drop(['ident_left','ident_y','idtronc_y'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculer le trafic total par point de comptage permanent\n",
    "def calcul_tmjo_2sens_ponct(tmjo, rgraph_dbl, nb_cpt,sens_uniq, df,id_grp,nom_attr_trafic,nom_attr_id_grp,id_cpt,nom_attr_id_cpt) :\n",
    "    \"\"\"calculer le tmja 2 sens en fonction du nb de compteur et du sens unique ou non\"\"\"\n",
    "    if nb_cpt==2 : \n",
    "        if (df.loc[df[nom_attr_id_grp]==id_grp].sens_unique==True).all() : #si sur unmm idtronc les 2 copt sont en sens uniq, on garde le max\n",
    "            return (df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].max(),df.loc[(df[nom_attr_id_grp]==id_grp) & (df[nom_attr_trafic]==\n",
    "                        df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].max())][nom_attr_id_cpt].values[0],)\n",
    "        else :\n",
    "            return df.loc[df[nom_attr_id_grp]==id_grp][nom_attr_trafic].sum(), df.loc[df[nom_attr_id_grp]==id_grp].groupby(nom_attr_id_grp)[nom_attr_id_cpt].agg(\n",
    "                lambda x : tuple(x)).values[0]\n",
    "    else : \n",
    "        if rgraph_dbl==1 : \n",
    "            return tmjo*2, (id_cpt,)\n",
    "        else : \n",
    "            return tmjo, (id_cpt,)\n",
    "        \n",
    "#calcul du tmjo_2_sens\n",
    "lgn_proche_ponct_attr_sup=lgn_proche_ponct.merge(lgn_proche_ponct.groupby('idtronc_tt_rhv')['ident'].nunique(), on='idtronc_tt_rhv').rename(\n",
    "    columns={'ident_y':'nb_cpt','ident_x':'ident'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgn_proche_ponct_attr_sup['tmjo_2_sens']=lgn_proche_ponct_attr_sup.apply(lambda x : \n",
    "                        calcul_tmjo_2sens_ponct(x['tmjo_tv'],x['rgraph_dbl'],x['nb_cpt'],x['sens_unique'], lgn_proche_ponct_attr_sup,\n",
    "                                          x['idtronc_tt_rhv'],'tmjo_tv','idtronc_tt_rhv',x['ident'],'ident')[0],axis=1)\n",
    "lgn_proche_ponct_attr_sup['id_cpt_2_sens']=lgn_proche_ponct_attr_sup.apply(lambda x : \n",
    "                        calcul_tmjo_2sens_ponct(x['tmjo_tv'],x['rgraph_dbl'],x['nb_cpt'],x['sens_unique'], lgn_proche_ponct_attr_sup,\n",
    "                                          x['idtronc_tt_rhv'],'tmjo_tv','idtronc_tt_rhv',x['ident'],'ident')[1],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verif doublon de compteur --si besoin\n",
    "lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup.duplicated('ident')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 cas possibles : soit le comptage ponctuel est sur un troncon dejà concerné issu des troncon elementaire categorie 1,2,3 ou non\n",
    "\n",
    "#séparer les points restant dans les 2 catégories : \n",
    "ponct_sur_perm_123=lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident_lgn'].isin(\n",
    "    gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['type_cpt']=='permanent'].ident.tolist())].copy()\n",
    "ponct_libre_123=lgn_proche_ponct_attr_sup.loc[~lgn_proche_ponct_attr_sup['ident_lgn'].isin(\n",
    "    gdf_rhv_cpt_perm_123.loc[gdf_rhv_cpt_perm_123['type_cpt']=='permanent'].ident.tolist())].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rappatriement des compteurs sur les idtronc des cat 1,2,3 : il y a des doublons car les troncons sont long, donc on separe, et on va traiter les ponct_libr sans doublons\n",
    "ponct_libre_123_tot=gdf_rhv_groupe_123[['ident','idtronc']].merge(ponct_libre_123[['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].rename(columns={'ident_lgn':'ident'}), \n",
    "    on='ident')[['idtronc','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates(['idtronc','tmjo_2_sens','id_cpt_2_sens'])\n",
    "list_pct_libre_dbl=ponct_libre_123_tot.loc[ponct_libre_123_tot.duplicated('idtronc',keep=False)].sort_values('idtronc').idtronc.unique()\n",
    "#affectation des ponctuels libre sans doublons\n",
    "pct_libre_123_ss_dbl=ponct_libre_123_tot.loc[~ponct_libre_123_tot['idtronc'].isin(list_pct_libre_dbl)].copy()\n",
    "pct_libre_123_ss_dbl['type_cpt']='ponctuel'\n",
    "gdf_rhv_cpt_perm_123.set_index('idtronc', inplace=True)\n",
    "gdf_rhv_cpt_perm_123.update(pct_libre_123_ss_dbl.set_index('idtronc'))\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl=gdf_rhv_groupe_123.merge(pct_libre_123_ss_dbl, on='idtronc')\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl.loc[~gdf_rhv_groupe_123_pctLibreSsDbl.tmjo_2_sens.isna(),'type_cpt']='ponctuel'\n",
    "#gdf_rhv_groupe_123_pctLibreSsDbl[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctSsDblv0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affectation des ponctuel libre avec doublons\n",
    "#l'idee c'est pour un idtronc des cat 1,2,3 on a la liste des ident, on affecte sur les ident des idtronc elementaire \n",
    "#sur les lignes, puis on propage sur les tronc de la cat 1,2,3\n",
    "\n",
    "pct_libre_123_avec_dbl=ponct_libre_123_tot.loc[ponct_libre_123_tot['idtronc'].isin(list_pct_libre_dbl)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idtronc_test in sorted(pct_libre_123_avec_dbl.idtronc.unique()) : \n",
    "    print(idtronc_test)\n",
    "    #récupérer l'idtronc toute cat_rhv des compteurs avec doublons et faire la jointure du trafic sur les lignes concernees\n",
    "    pct_libre_123_avec_dbl_decompose=lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident'].isin(\n",
    "        [a for b in pct_libre_123_avec_dbl.loc[pct_libre_123_avec_dbl['idtronc']==idtronc_test].\n",
    "         id_cpt_2_sens.tolist() for a in b])][['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']].copy()\n",
    "\n",
    "    #initialisation des paramètres : \n",
    "    ligne_ok=gdf_rhv_groupe.merge(pct_libre_123_avec_dbl_decompose.rename(columns={'idtronc_tt_rhv':'idtronc'}), on='idtronc').merge(\n",
    "        graph_filaire_123[['ident','source','target']], on='ident').drop(['source_x', 'target_x'],axis=1).rename(\n",
    "        columns={'source_y':'source', 'target_y':'target'})#lignes avec trafic issues des troncons elemntaires et on garde les ousrces et target liées uniquement aux cat 1,2,3\n",
    "    lgn_id_tronc_123_vide=gdf_rhv_groupe_123.loc[(gdf_rhv_groupe_123['idtronc']==idtronc_test) & \n",
    "                                                 (~gdf_rhv_groupe_123.ident.isin(ligne_ok.ident.tolist()))] #l'ensemble des lignes a renseigner\n",
    "    while ~lgn_id_tronc_123_vide.empty :\n",
    "        list_noeud_trafic=ligne_ok.source.tolist()+ligne_ok.target.tolist() #les noeuds avec du trafic\n",
    "        list_noeud_trafic_null=lgn_id_tronc_123_vide.source.tolist()+lgn_id_tronc_123_vide.target.tolist() #l'inverse\n",
    "        lgn_a_renseigner=lgn_id_tronc_123_vide.loc[((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic_null))) | \n",
    "                ((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic_null)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic)))].copy()\n",
    "        if lgn_a_renseigner.empty : break #si pas de ligne a renseigner on sort\n",
    "        ligne_ok['noeud_partage']=ligne_ok.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic_null else x['target'], axis=1) #pour faire une jointure entre les lignes a trafic et les autres\n",
    "        lgn_a_renseigner['noeud_partage']=lgn_a_renseigner.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic else x['target'], axis=1)\n",
    "        lgn_a_renseigner=lgn_a_renseigner.merge(ligne_ok[['noeud_partage','tmjo_2_sens','id_cpt_2_sens']], on='noeud_partage') #recupere le tmj\n",
    "        ligne_ok=pd.concat([ligne_ok,lgn_a_renseigner], axis=0, sort=False)#on ajoute les lignes avec du trafic aux autres\n",
    "        lgn_id_tronc_123_vide=lgn_id_tronc_123_vide.loc[~lgn_id_tronc_123_vide.ident.isin(ligne_ok.ident.tolist())].copy() #on met à jour pour la boucle\n",
    "    ligne_ok.drop_duplicates('ident',inplace=True)# les ronds points crees des doublons\n",
    "    ligne_ok['type_cpt']='ponctuel'\n",
    "    #mettre à jour la donnees de base\n",
    "    gdf_rhv_cpt_perm_123.update(ligne_ok[['ident','tmjo_2_sens','id_cpt_2_sens','type_cpt']].set_index('ident'))\n",
    "#export\n",
    "#gdf_rhv_groupe_123_pctLibreAvecDbl.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctAvecDblv0.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#affectation des ponctuels sur permanents : de la mm façon que les ponctuels en doublons\n",
    "\n",
    "#trouver la liste des idtronc cat_rhv = 1,2,3 avec ponct sur perm\n",
    "idtronc123_pct_sur_perm=gdf_rhv_groupe_123[['ident','idtronc']].merge(ponct_sur_perm_123[['ident_lgn','tmjo_2_sens','id_cpt_2_sens']].rename(columns={'ident_lgn':'ident'}), \n",
    "    on='ident')[['idtronc','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates()\n",
    "\n",
    "for idtronc_test in idtronc123_pct_sur_perm.idtronc.unique() : \n",
    "    print(idtronc_test)\n",
    "    #trouver les id tronc elementaire des comptages ponct et perm concerne\n",
    "    ligne_ok= gdf_rhv_groupe.merge(pd.concat([lgn_proche_ponct_attr_sup.loc[lgn_proche_ponct_attr_sup['ident'].isin(\n",
    "        [a for b in idtronc123_pct_sur_perm.loc[idtronc123_pct_sur_perm['idtronc']==idtronc_test].id_cpt_2_sens.tolist()\n",
    "         for a in b])][['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']].drop_duplicates(['idtronc_tt_rhv','tmjo_2_sens','id_cpt_2_sens']),\n",
    "    gdf_rhv_groupe_123.loc[gdf_rhv_groupe_123['idtronc']==idtronc_test].merge(\n",
    "        lgn_proche_attr_sup, left_on='ident', right_on='ident_lgn')[['idtronc_y','tmjo_2_sens','id_cpt_2_sens']].rename(\n",
    "        columns={'idtronc_y' : 'idtronc_tt_rhv'}).drop_duplicates()],axis=0, sort=False), left_on='idtronc', right_on='idtronc_tt_rhv')\n",
    "\n",
    "    lgn_id_tronc_123_vide=gdf_rhv_groupe_123.loc[(gdf_rhv_groupe_123['idtronc']==idtronc_test) & \n",
    "                                                     (~gdf_rhv_groupe_123.ident.isin(ligne_ok.ident.tolist()))]\n",
    "\n",
    "    while ~lgn_id_tronc_123_vide.empty :\n",
    "        list_noeud_trafic=ligne_ok.source.tolist()+ligne_ok.target.tolist() #les noeuds avec du trafic\n",
    "        list_noeud_trafic_null=lgn_id_tronc_123_vide.source.tolist()+lgn_id_tronc_123_vide.target.tolist() #l'inverse\n",
    "        lgn_a_renseigner=lgn_id_tronc_123_vide.loc[((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic_null))) | \n",
    "                ((lgn_id_tronc_123_vide.source.isin(list_noeud_trafic_null)) & (lgn_id_tronc_123_vide.target.isin(list_noeud_trafic)))].copy()\n",
    "        if lgn_a_renseigner.empty : break #si pas de ligne a renseigner on sort\n",
    "        ligne_ok['noeud_partage']=ligne_ok.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic_null else x['target'], axis=1) #pour faire une jointure entre les lignes a trafic et les autres\n",
    "        lgn_a_renseigner['noeud_partage']=lgn_a_renseigner.apply(lambda x : x['source'] if x['source'] in list_noeud_trafic else x['target'], axis=1)\n",
    "        lgn_a_renseigner=lgn_a_renseigner.merge(ligne_ok[['noeud_partage','tmjo_2_sens','id_cpt_2_sens']], on='noeud_partage') #recupere le tmj\n",
    "        ligne_ok=pd.concat([ligne_ok,lgn_a_renseigner], axis=0, sort=False)#on ajoute les lignes avec du trafic aux autres\n",
    "        lgn_id_tronc_123_vide=lgn_id_tronc_123_vide.loc[~lgn_id_tronc_123_vide.ident.isin(ligne_ok.ident.tolist())].copy() #on met à jour pour la boucle\n",
    "    ligne_ok.drop_duplicates('ident',inplace=True)# les ronds points crees des doublons\n",
    "    ligne_ok['type_cpt']='ponctuel'\n",
    "    #mettre à jour la donnees de base\n",
    "    gdf_rhv_cpt_perm_123.update(ligne_ok[['ident','tmjo_2_sens','id_cpt_2_sens','type_cpt']].set_index('ident'))\n",
    "\n",
    "#gdf_rhv_groupe_123_pctLibreAvecDbl.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       #'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       #'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       #'idtronc', 'geometry','tmjo_2_sens']].to_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_pctAvecDblv01.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_cpt_perm_123['id_cpt_2_sens'].fillna('NC', inplace=True)\n",
    "gdf_rhv_cpt_perm_123['id_cpt_2_sens']=gdf_rhv_cpt_perm_123.apply(lambda x : ', '.join([str(a) for a in x['id_cpt_2_sens']] \n",
    "                if not isinstance(x['id_cpt_2_sens'],np.int64) else str(x['id_cpt_2_sens']) ), axis=1)\n",
    "gdf_rhv_cpt_perm_123.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_v1.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. MMM\n",
    "En premier lieu il faut mettre à jour les attributs en renomant de façon explicite puis en faisant la somme des 2 sens de circulation pour les voies doucble sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creer graph et importer \n",
    "creer_graph(fichier_src_simpl, 'local_otv',id_name='NO', schema='public', table='mmm_graph', table_vertex='mmm_graph_vertices_pgr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer le graph\n",
    "with ct.ConnexionBdd('local_otv') as c :\n",
    "    graph_mmm_filaire = gp.read_postgis('select * from linearisation_bm.mmm_graph', c.connexionPsy)\n",
    "    graph_mmm_filaire_vertex = gp.read_postgis('select * from linearisation_bm.mmm_graph_vertices_pgr', c.connexionPsy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importer le fichier simplifie\n",
    "mmm_simple=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\MMM_simplifie.shp')\n",
    "cle_mmm_rhv=gp.read_file(r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\appariementV0.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affecter du trafic aux voies non connues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_rhv_cpt_perm_123.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "c:\\users\\martin.schoreisz\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\lib\\function_base.py:2167: RuntimeWarning: invalid value encountered in ? (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "#SIMPLIFIER LES RONDS POINTS\n",
    "\n",
    "#simplifier la topologie en affectant une seule valuer de noeud aux voie arrivant sur un rond point\n",
    "#trouver les noeud faisant partie d'un rond point \n",
    "df=import_donnes_base('local_otv','linearisation_bm', 'graph_rhv_123','graph_rhv_123_vertices_pgr')\n",
    "df2_chaussees=df.loc[df.nature.isin(['Autoroute', 'Quasi-autoroute', 'Route à 2 chaussées'])]\n",
    "df_avec_rd_pt,carac_rd_pt,lign_entrant_rdpt=identifier_rd_pt(df)\n",
    "\n",
    "lgn_rdpt=df_avec_rd_pt.loc[~df_avec_rd_pt.id_rdpt.isna()].copy()\n",
    "dico_noeud=creer_dico_noeud_rdpt(lgn_rdpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# àpartir du dico on va remplacer les valeurs de source ou target par la valeur de clé du dico. \n",
    "gdf_base=gdf_rhv_cpt_perm_123.copy()\n",
    "gdf_rhv_rdpt_simple=gdf_base.loc[~gdf_base.index.isin(lgn_rdpt.ident.to_list())].copy()\n",
    "#remplacement des sources et targets : \n",
    "simplifier_noeud_rdpt(gdf_rhv_rdpt_simple, dico_noeud)\n",
    "#puis recalculer le count du nb de ligne par noeud (en otant les lignes qui font les rdpoints)\n",
    "cnt_maj=maj_graph_rdpt(gdf_rhv_rdpt_simple)\n",
    "noeuds_estim, noeuds_grp, df_noeuds=et.noeuds_estimables(gdf_rhv_rdpt_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caracteriser les noeuds\n",
    "# 1. trouver ceux estimables\n",
    "noeuds_estim, noeuds_grp, df_noeuds=et.noeuds_estimables(gdf_rhv_rdpt_simple)\n",
    "liste_noeud_estim=noeuds_estim.noeud.tolist()\n",
    "liste_noeud_traite=[]\n",
    "dico_erreur={}\n",
    "while liste_noeud_estim : \n",
    "    # pour chaque noeuds : \n",
    "      #calcul des troncons arrivants\n",
    "    num_noeud=liste_noeud_estim[0]\n",
    "    print(num_noeud)\n",
    "    gdf_rhv_rdpt_simple=verif_index(gdf_rhv_rdpt_simple,'idtronc')#Îsi idtronc est en index on le repasse dans les colonnes\n",
    "    liste_noeud_traite.append(num_noeud)\n",
    "    df_troncon_noeud=et.carac_troncon_noeud(gdf_rhv_rdpt_simple, gdf_base, graph_filaire_123_vertex,num_noeud,df_noeuds)\n",
    "      #si tous les trafics du noeud sont déjà renseignés on passe\n",
    "    if (df_troncon_noeud.tmjo_2_sens!=-99).all() : \n",
    "        continue\n",
    "      #determiner si on est dans le cas d'une voie pouvant etre estimée par calcul (3 troncon dont 1 seul manquant) ou par analogie avec le MMM\n",
    "    if et.type_estim(df_troncon_noeud)=='calcul_3_voies' :\n",
    "        et.maj_trafic_3tronc(df_troncon_noeud)\n",
    "        df_update_traf=df_troncon_noeud[['idtronc','tmjo_2_sens']].set_index('idtronc').drop_duplicates()\n",
    "    else : #RETRAVAILLER : CA NE MARCHE PAS COMME PREVU\n",
    "        matrice_rhv=et.matrice_troncon_noeud_rhv(df_troncon_noeud,lgn_rdpt) \n",
    "        try :\n",
    "            joint_fv_mmm_e2=et.estim_mmm_jointure_voies(matrice_rhv,cle_mmm_rhv)\n",
    "        except et.PasCorrespondanceError : \n",
    "            dico_erreur[num_noeud]='PasCorrespondanceError'\n",
    "            noeuds_estim, noeuds_grp, df_noeuds=et.noeuds_estimables(gdf_rhv_rdpt_simple)\n",
    "            liste_noeud_estim=[n for n in noeuds_estim.noeud.tolist() if n not in liste_noeud_traite]\n",
    "            continue\n",
    "        try :\n",
    "            trafic_inconnus_prior_cat=et.isoler_trafic_inconnu(joint_fv_mmm_e2)\n",
    "        except et.PasDeTraficInconnuError: \n",
    "            dico_erreur[num_noeud]='PasDeTraficInconnuError' \n",
    "            noeuds_estim, noeuds_grp, df_noeuds=et.noeuds_estimables(gdf_rhv_rdpt_simple)\n",
    "            liste_noeud_estim=[n for n in noeuds_estim.noeud.tolist() if n not in liste_noeud_traite]\n",
    "            continue\n",
    "        try :\n",
    "            trafc_rens=et.trafic_mmm(trafic_inconnus_prior_cat,mmm_simple,num_noeud, graph_filaire_123_vertex,gdf_rhv_rdpt_simple,cle_mmm_rhv)\n",
    "            df_update_traf=et.calcul_trafic_rhv_depuisMMM(trafc_rens).set_index('idtronc').rename(columns={'tmjo_2_sens_extrapol':'tmjo_2_sens'})\n",
    "        except et.PasDeTraficError as e: \n",
    "            dico_erreur[num_noeud]=e \n",
    "            noeuds_estim, noeuds_grp, df_noeuds=et.noeuds_estimables(gdf_rhv_rdpt_simple)\n",
    "            liste_noeud_estim=[n for n in noeuds_estim.noeud.tolist() if n not in liste_noeud_traite]\n",
    "            continue\n",
    "      #mise à jour du fichier source\n",
    "    #try : \n",
    "    gdf_rhv_rdpt_simple=gdf_rhv_rdpt_simple.set_index('idtronc')\n",
    "    gdf_rhv_rdpt_simple.update(df_update_traf[['tmjo_2_sens']])\n",
    "    gdf_rhv_rdpt_simple.reset_index(inplace=True)\n",
    "    gdf_base=gdf_base.set_index('idtronc')\n",
    "    gdf_base.update(df_update_traf[['tmjo_2_sens']])\n",
    "    gdf_base.reset_index(inplace=True)\n",
    "    #except ValueError as e : \n",
    "        #dico_erreur[num_noeud]=e\n",
    "      #recherche noeud estimable\n",
    "        #noeuds_estim, noeuds_grp, df_noeuds=et.noeuds_estimables(gdf_rhv_rdpt_simple)\n",
    "        #liste_noeud_estim=[n for n in noeuds_estim.noeud.tolist() if n not in liste_noeud_traite]\n",
    "        #continue\n",
    "    noeuds_estim, noeuds_grp, df_noeuds=et.noeuds_estimables(gdf_rhv_rdpt_simple)\n",
    "    liste_noeud_estim=[n for n in noeuds_estim.noeud.tolist() if n not in liste_noeud_traite]\n",
    "\n",
    "gdf_base.reset_index()[['id_x', 'ident', 'domanial', 'groupe', 'cat_dig', 'cat_rhv', 'passage',\n",
    "       'rggraph_nd', 'rggraph_na', 'rgraph_dbl', 'numero', 'cdate', 'mdate',\n",
    "       'id_ign', 'nature', 'sens', 'codevoie_d', 'importance', 'id_y',\n",
    "       'idtronc', 'geometry','tmjo_2_sens','type_cpt', 'id_cpt_2_sens']].to_file(\n",
    "    r'D:\\temp\\Linearisation_BM\\C19SA0101\\C19SA0101\\Doc_travail\\Donnees_produites\\Donnees\\gdf_rhv_trafic_123_extrapol.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"gdf_rhv_rdpt_simple=gdf_rhv_rdpt_simple.set_index('idtronc')\\ngdf_rhv_rdpt_simple.update(df_update_traf[['tmjo_2_sens']])\\ngdf_rhv_rdpt_simple.reset_index(inplace=True)\\ngdf_base=gdf_base.set_index('idtronc')\\ngdf_base.update(df_update_traf[['tmjo_2_sens']])\\ngdf_base.reset_index(inplace=True)\""
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pour chaque noeuds : \n",
    "  #calcul des troncons arrivants\n",
    "num_noeud=3056\n",
    "print(num_noeud)\n",
    "df_troncon_noeud=et.carac_troncon_noeud(gdf_rhv_rdpt_simple.reset_index(), gdf_base, graph_filaire_123_vertex,num_noeud,df_noeuds)\n",
    "  #si tous les trafics du noeud sont déjà renseignés on passe\n",
    "if (df_troncon_noeud.tmjo_2_sens!=-99).all() : \n",
    "    print('on passe')\n",
    "  #determiner si on est dans le cas d'une voie pouvant etre estimée par calcul (3 troncon dont 1 seul manquant) ou par analogie avec le MMM\n",
    "if et.type_estim(df_troncon_noeud)=='calcul_3_voies' :\n",
    "    et.maj_trafic_3tronc(df_troncon_noeud)\n",
    "    df_update_traf=df_troncon_noeud[['idtronc','tmjo_2_sens']].set_index('idtronc').drop_duplicates()\n",
    "else : #RETRAVAILLER : CA NE MARCHE PAS COMME PREVU\n",
    "    matrice_rhv=et.matrice_troncon_noeud_rhv(df_troncon_noeud,lgn_rdpt) \n",
    "    joint_fv_mmm_e2=et.estim_mmm_jointure_voies(matrice_rhv,cle_mmm_rhv)\n",
    "    trafic_inconnus_prior_cat=et.isoler_trafic_inconnu(joint_fv_mmm_e2)\n",
    "    trafc_rens=et.trafic_mmm(trafic_inconnus_prior_cat,mmm_simple,num_noeud, graph_filaire_123_vertex,gdf_rhv_rdpt_simple.reset_index(),cle_mmm_rhv)\n",
    "    df_update_traf=et.calcul_trafic_rhv_depuisMMM(trafc_rens).set_index('idtronc').rename(columns={'tmjo_2_sens_extrapol':'tmjo_2_sens'})\n",
    "\"\"\"gdf_rhv_rdpt_simple=gdf_rhv_rdpt_simple.set_index('idtronc')\n",
    "gdf_rhv_rdpt_simple.update(df_update_traf[['tmjo_2_sens']])\n",
    "gdf_rhv_rdpt_simple.reset_index(inplace=True)\n",
    "gdf_base=gdf_base.set_index('idtronc')\n",
    "gdf_base.update(df_update_traf[['tmjo_2_sens']])\n",
    "gdf_base.reset_index(inplace=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident_x</th>\n",
       "      <th>ident_y</th>\n",
       "      <th>idtronc_x</th>\n",
       "      <th>idtronc_y</th>\n",
       "      <th>tmjo_2_sens_x</th>\n",
       "      <th>tmjo_2_sens_y</th>\n",
       "      <th>cat_rhv_x</th>\n",
       "      <th>cat_rhv_y</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>idtronc_a_estim</th>\n",
       "      <th>diff_cat</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "      <th>traf_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37809</td>\n",
       "      <td>37803</td>\n",
       "      <td>700.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>34330.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>724099606.0</td>\n",
       "      <td>724099596.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42586.17</td>\n",
       "      <td>44385.40</td>\n",
       "      <td>21343.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37803</td>\n",
       "      <td>37808</td>\n",
       "      <td>557.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>34330.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>724099596.0</td>\n",
       "      <td>724099602.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>0</td>\n",
       "      <td>44385.40</td>\n",
       "      <td>42586.17</td>\n",
       "      <td>21343.19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident_x ident_y  idtronc_x  idtronc_y  tmjo_2_sens_x  tmjo_2_sens_y  \\\n",
       "0   37809   37803      700.0      557.0        34330.0          -99.0   \n",
       "3   37803   37808      557.0      700.0          -99.0        34330.0   \n",
       "\n",
       "  cat_rhv_x cat_rhv_y         NO_x         NO_y  idtronc_a_estim  diff_cat  \\\n",
       "0         1         1  724099606.0  724099596.0            557.0         0   \n",
       "3         1         1  724099596.0  724099602.0            557.0         0   \n",
       "\n",
       "   tmja_tv_x  tmja_tv_y  traf_max  \n",
       "0   42586.17   44385.40  21343.19  \n",
       "3   44385.40   42586.17  21343.19  "
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trafc_rens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmjo_2_sens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idtronc</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>557.0</th>\n",
       "      <td>35780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         tmjo_2_sens\n",
       "idtronc             \n",
       "557.0          35780"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_update_traf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "noeud=3056\n",
    "def trafic_reference_mmm(tmjo_2_sens_x,tmjo_2_sens_y, df, idtronc_x,idtronc_y) : \n",
    "        \"\"\"\n",
    "        trouver le tmjo ode reference du mmm pour le calcul des trafic manquants,si plueisuers lignes possible\n",
    "        in : \n",
    "           tmjo_2_sens_x : float : trafic issu du filaire prealablement renseigne\n",
    "           tmjo_2_sens_y : float : trafic issu du filaire prealablement renseigne\n",
    "           df : df des voies a renseignees\n",
    "           idtronc : float : ifentifiant du troncon concerne\n",
    "        \"\"\"\n",
    "        if tmjo_2_sens_y == -99 : \n",
    "            df_traf_ref=df.loc[(df['idtronc_y']==idtronc_y) | (df['idtronc_x']==idtronc_y) ].copy()\n",
    "            df_traf_ref['traf_ref']=df_traf_ref.apply(lambda x : x['tmja_tv_x'] if x['tmjo_2_sens_y']==-99 else x['tmja_tv_y'],axis=1)\n",
    "            return df_traf_ref.traf_ref.max()\n",
    "        else : \n",
    "            df_traf_ref=df.loc[(df['idtronc_y']==idtronc_x) | (df['idtronc_x']==idtronc_x) ].copy()\n",
    "            df_traf_ref['traf_ref']=df_traf_ref.apply(lambda x : x['tmja_tv_x'] if x['tmjo_2_sens_y']==-99 else x['tmja_tv_y'],axis=1)\n",
    "            return df_traf_ref.traf_ref.max()\n",
    "    \n",
    "def ajout_tmja(idtronc, tmja_base, noeud) : \n",
    "    \"\"\"\n",
    "    Pour les voies du MMM qui sont représentées par 2 lignes il faut ajouter les 2 trafics pour le troncon\n",
    "    \"\"\"\n",
    "    troncon=et.Troncon(gdf_rhv_rdpt_simple,idtronc)\n",
    "    noeud_parrallele=et.trouver_noeud_parrallele(troncon,graph_filaire_123_vertex, noeud,100)\n",
    "    #trouver les lignes du troncon qui intersectent ce noeud\n",
    "    lign_fin=troncon.df_lign_fin_tronc.loc[(troncon.df_lign_fin_tronc['source']==noeud_parrallele) | (troncon.df_lign_fin_tronc['target']==noeud_parrallele)]\n",
    "    #jointure avec les données MMM\n",
    "    trafc_mmm_sup=lign_fin[['idtronc','ident']].merge(cle_mmm_rhv[['NO','ident']], on='ident').merge(mmm_simple[['NO','tmja_tv']], on='NO')\n",
    "    if trafc_mmm_sup.empty : \n",
    "        return tmja_base\n",
    "    else : \n",
    "        return tmja_base+trafc_mmm_sup.tmja_tv.sum()\n",
    "\n",
    "def somme_traf(idtronc, traf_rens) : \n",
    "    \"\"\"\n",
    "    pour les voies du MMM représenté par une ligne qui se sépare en 2 dur la fin (rd point par exemple))\n",
    "    \"\"\"\n",
    "    nb_value=list(set(traf_rens.loc[traf_rens['idtronc_x']==idtronc].tmja_tv_x.tolist()+traf_rens.loc[traf_rens['idtronc_y']==idtronc].tmja_tv_y.tolist()))\n",
    "    if len(nb_value)>=2 : \n",
    "        return (sum(nb_value))\n",
    "    else : return (nb_value[0])\n",
    "\n",
    "#test si les troncon MMM identifiés ont bien un trafic dans le fichier mmm simplifie. si pas le cas corrige le fichier simplifie\n",
    "troncon_mmm_sstraf=[a for a  in trafic_inconnus_prior_cat.NO_x.tolist()+trafic_inconnus_prior_cat.NO_y.tolist() if a not in mmm_simple.NO.tolist()]\n",
    "if troncon_mmm_sstraf : \n",
    "    raise PasDeTraficError(troncon_mmm_sstraf)\n",
    "traf_mmm=trafic_inconnus_prior_cat.merge(mmm_simple[['NO','tmja_tv']], left_on='NO_x', right_on='NO').drop('NO', axis=1).merge(\n",
    "mmm_simple[['NO','tmja_tv']], left_on='NO_y', right_on='NO').drop('NO', axis=1)\n",
    "#mise a jour des trafic mmm si 2*2voies MMM\n",
    "if traf_mmm.apply(lambda x : ','.join([str(x['idtronc_x']),str(x['idtronc_y'])]),axis=1).nunique()!=len(traf_mmm) :\n",
    "    traf_mmm['tmja_tv_x']=traf_mmm.apply(lambda x : ajout_tmja(x['idtronc_x'], x['tmja_tv_x'], noeud),axis=1)\n",
    "    traf_mmm['tmja_tv_y']=traf_mmm.apply(lambda x : ajout_tmja(x['idtronc_y'], x['tmja_tv_y'], noeud),axis=1)\n",
    "#ensuite on refait un test dans le cas par exemple des rd pt pour lesquels les voies se séparent à la fin\n",
    "traf_mmm2=traf_mmm.copy()\n",
    "traf_mmm2['tmja_tv_x']=traf_mmm2.apply(lambda x : somme_traf(x['idtronc_x'], traf_mmm),axis=1)\n",
    "traf_mmm2['tmja_tv_y']=traf_mmm2.apply(lambda x : somme_traf(x['idtronc_y'], traf_mmm),axis=1)\n",
    "\n",
    "traf_mmm2['traf_max']=traf_mmm2.apply(lambda x:trafic_reference_mmm(x['tmjo_2_sens_x'],x['tmjo_2_sens_y'],traf_mmm2,x['idtronc_x'],x['idtronc_y']), axis=1)\n",
    "traf_mmm2.drop_duplicates(['idtronc_x','idtronc_y','tmja_tv_x','tmja_tv_y','traf_max'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafc_rens2=trafc_rens.copy()\n",
    "trafc_rens2['idtronc']=trafc_rens2.apply(lambda x : x['idtronc_x'] if x['tmjo_2_sens_x']==-99\n",
    "                                                  else x['idtronc_y'], axis=1)\n",
    "trafc_rens2['ident_ref_calcul']=trafc_rens2.apply(lambda x : 'x' if x['tmjo_2_sens_y']==-99\n",
    "                                                      else 'y', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#si plusieurs resultas possibles pour unident on garde le max\n",
    "trafc_rens2=trafc_rens2.loc[trafc_rens2.apply(lambda x : x['traf_max']==x['tmja_tv_'+x['ident_ref_calcul']], axis=1)].copy()\n",
    "\n",
    "try : \n",
    "    trafc_rens2['tmjo_2_sens_extrapol']=trafc_rens2.apply(lambda x : \n",
    "        int(x['tmjo_2_sens_x']/x['tmja_tv_x']*x['tmja_tv_y']) if x['tmjo_2_sens_y']==-99 else int(x['tmjo_2_sens_y']/x['tmja_tv_y']*x['tmja_tv_x']),\n",
    "                                                      axis=1)\n",
    "    #check si une seule valeur par idtronc que l'on cherche, si pas le cas on prend la valeur calculee max\n",
    "    if not (trafc_rens2.groupby('idtronc').tmjo_2_sens_extrapol.nunique()==1).all() : \n",
    "        trafc_rens2=trafc_rens2.loc[trafc_rens.tmjo_2_sens_extrapol==trafc_rens.groupby('idtronc').tmjo_2_sens_extrapol.transform(max)].copy()   \n",
    "\n",
    "except (ZeroDivisionError, ValueError) : \n",
    "    raise PasDeTraficError (trafc_rens.idtronc.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident_x</th>\n",
       "      <th>ident_y</th>\n",
       "      <th>idtronc_x</th>\n",
       "      <th>idtronc_y</th>\n",
       "      <th>tmjo_2_sens_x</th>\n",
       "      <th>tmjo_2_sens_y</th>\n",
       "      <th>cat_rhv_x</th>\n",
       "      <th>cat_rhv_y</th>\n",
       "      <th>NO_x</th>\n",
       "      <th>NO_y</th>\n",
       "      <th>idtronc_a_estim</th>\n",
       "      <th>diff_cat</th>\n",
       "      <th>tmja_tv_x</th>\n",
       "      <th>tmja_tv_y</th>\n",
       "      <th>traf_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37809</td>\n",
       "      <td>37803</td>\n",
       "      <td>700.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>34330.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>724099606.0</td>\n",
       "      <td>724099596.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>0</td>\n",
       "      <td>42586.17</td>\n",
       "      <td>44385.40</td>\n",
       "      <td>42586.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37803</td>\n",
       "      <td>37808</td>\n",
       "      <td>557.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>-99.0</td>\n",
       "      <td>34330.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>724099596.0</td>\n",
       "      <td>724099602.0</td>\n",
       "      <td>557.0</td>\n",
       "      <td>0</td>\n",
       "      <td>44385.40</td>\n",
       "      <td>42586.17</td>\n",
       "      <td>42586.17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident_x ident_y  idtronc_x  idtronc_y  tmjo_2_sens_x  tmjo_2_sens_y  \\\n",
       "0   37809   37803      700.0      557.0        34330.0          -99.0   \n",
       "3   37803   37808      557.0      700.0          -99.0        34330.0   \n",
       "\n",
       "  cat_rhv_x cat_rhv_y         NO_x         NO_y  idtronc_a_estim  diff_cat  \\\n",
       "0         1         1  724099606.0  724099596.0            557.0         0   \n",
       "3         1         1  724099596.0  724099602.0            557.0         0   \n",
       "\n",
       "   tmja_tv_x  tmja_tv_y  traf_max  \n",
       "0   42586.17   44385.40  42586.17  \n",
       "3   44385.40   42586.17  42586.17  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traf_mmm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "274 in dico_erreur.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
